{"meta":{"title":"code","subtitle":"码总","description":"JVM相关，Spring，SpringBoot,SpringCloud,SpringMvc,Redis,Kafka,RabbitMQ,EleasticSearch","author":"Liu Dong","url":"https://liudong-code.github.io"},"pages":[{"title":"categories","date":"2020-02-20T02:52:59.000Z","updated":"2020-02-20T03:43:51.276Z","comments":true,"path":"categories/index.html","permalink":"https://liudong-code.github.io/categories/index.html","excerpt":"","text":"type: “categories” # 将页面的类型设置为 categories ，主题将自动为这个页面显示所有分类comments: true # 如果有启用多说 或者 Disqus 评论，默认页面也会带有评论。需要关闭的话，设置为 false 这里也有 “—“"}],"posts":[{"title":"JVM虚拟机","slug":"JVM虚拟机","date":"2020-03-10T02:42:35.000Z","updated":"2020-03-10T05:50:43.013Z","comments":true,"path":"2020/03/10/JVM虚拟机/","link":"","permalink":"https://liudong-code.github.io/2020/03/10/JVM%E8%99%9A%E6%8B%9F%E6%9C%BA/","excerpt":"","text":"一、概述 JVM是Java Virtual Machine（Java虚拟机）的缩写，是一种用于计算设备的规范，它是一个虚构出来的计算机，是通过在实际的计算机上仿真模拟各种计算机功能来实现的。 有两个概念和JVM息息相关并且很容易搞混，那就是JRE和JDK。 JRE(JavaRuntimeEnvironment，Java运行环境)，指的是Java平台。所有的Java 程序都要在JRE下才能运行。普通用户运行已开发好的java程序，只要安装JRE即可。 JDK(JavaDevelopmentKit)是程序开发者用来来编译、调试java程序用的开发工具包。JDK的工具也是Java程序，也需要JRE才能运行。 为了保持JDK的独立性和完整性，在JDK的安装过程中，JRE也是安装的一部分。所以，在JDK的安装目录下有一个名为jre的目录，用于存放JRE文件。而JVM是JRE的一部分。JVM有自己完善的硬件架构，如处理器、堆栈、寄存器等，还具有相应的指令系统。Java语言最重要的特点就是跨平台运行。使用JVM就是为了支持与操作系统无关，实现跨平台。 下面这张图是java程序的一个总执行流程： 二、类加载机制 从上图可以看到，我们写的源程序通过编译后生成的class文件，在运行的时候首先会通过类加载器系统。这边来简要说一下类加载机制。 上面是一个类从最初加载到最后卸载的整个流程。我们对其中几个过程进行梳理。 2.1 加载加载过程是将class 文件字节码内容加载到内存中，并将这些静态数据转换成方法区中的运行时数据结构，在堆中生成一个代表这个类的java.lang.Class对象，作为方法区类数据的访问入口。这个过程需要类加载器参与。 2.2 链接链接是将java类的二进制代码合并到JVM的运行状态之中的过程。可以再细分为如下3步： ① 验证：确保加载的类信息符合JVM规范，没有安全方面的问题。 ② 准备：正式为变量(static)分配内存并设置类变量默认值，这些内存都将在方法区中进行分配。 ③ 解析：虚拟机常量池内的符号引用替换为直接引用的过程。 2.3 初始化初始化阶段是执行类构造器()方法的过程。类构造器()方法是由编译器自动收集类中的所有变量的赋值动作和静态语句块（static块）中的语句合并产生的。 当初始化一个类的时候，如果发现其父类还没有进行过初始化、则需要先初始化其父类。虚拟机会保证一个类的()方法在多线程环境中被正确加锁同步。 当访问一个java类的静态域时，只有真正声明这个域的类才会被初始化。 下面是一个例子： 1234567891011121314151617181920212223242526272829303132333435public class Test&#123; public static void main(String[] args)&#123; System.out.println(\"运行main\"); A testa = new A(); System.out.println(\"a:\"+testa.a); testa = new A(); &#125;&#125;class A&#123; public static int a = 100; static&#123; System.out.println(\"初始化A静态块\"); a = 300； &#125; public A()&#123; System.out.println(\"创建A对象\"); &#125;&#125; 输出结果如下： 123456789运行main初始化A静态块创建A对象a:300创建A对象 从上面可以看出，如果类还没加载进来，会先加载类，并初始化static，其中static 加载顺序是在代码中从上到下的顺序来执行的。 2.3.1 类的主动引用对于类的主动引用操作，则一定会发生类的初始化。下面罗列了一些类的主动引用： ① New一个类的对象。 ② 调用类的静态成员（除了final常量）和静态方法。 ③ 使用java.lang.reflect包的方法对类进行反射调用。 ④ 当虚拟机启动，java 会先启动main方法所在的类。 ⑤ 当初始化一个类，如果其父类没有被初始化，则会先初始化他的父类。 2.3.2 类的被动引用① 当访问一个静态域时，只有真正声明这个域的类才会被初始化，通过子类引用父类的静态变量，不会导致子类初始化。 ② 通过数组定义类引用，不会触发此类的初始化。 ③ 引用常量不会触发此类的初始化。 2.4 类加载器加载类必然离不开类加载器，类加载器是将class文件字节码内容加载到内存中，并将这些静态数据转换成方法区中的运行时数据结构，在堆中生成一个代表这个类的java.lang.Class 对象，作为方法区的访问入口。 关于类加载有一个很有名的机制——双亲委托机制： （后续的博客仔细做讨论） 这个机制采用代理模式，某个特定的类加载器在接到加载类的请求时，先将代理任务传到最高一辈，加载不了再逐级往下传，直到能加载。 这个机制的作用是为了保证Java核心库的类型安全。比如说如果用户自定义了一个String类，在该机制下，类加载器还是会去加载系统自带的String类，而不是加载用户自定义的这个String类。 三、JVM运行时数据区 讲完类加载系统，来说一下JVM的运行时数据区，先看如下图： 运行的程序是内容是放在运行时数据区中的，如上图蓝色那块依次来说明一下： 3.1 堆 保存所有引用数据类型的真实信息（线程共享）。也就是说那些new出来的对象都是放在这块区域的。 3.2 虚拟机栈 基本数据、运算、指向堆内存的引用（线程私有）。 在栈里面是由一个个栈帧组成的，每个正在执行的方法对应一个栈帧。 当一个方法运行到一半需要调用另一个方法时，就创建一个新的栈帧表示新调用的方法，将原来那个方法压入栈中。 当方法运行完毕，栈帧出栈，原来方法处于栈顶接着运行。 和栈这一数据结构一样，虚拟机栈里面的栈帧遵循后进先出的原则。 常见的一个错误栈溢出 StackOverflowError 就是由于方法递归层数太多，导致栈空间满了。看如下一个例子: 123456789101112131415public class TestDemo&#123; public static void main(String args[])&#123; fun(); &#125; public static void fun()&#123; fun(); &#125;&#125; 上面fun()函数无限递归调用自己，最终会造成栈溢出： 3.3 方法区又叫静态区，跟堆一样，被所有的线程共享。 方法区包含所有的class和static变量。方法区中包含的都是在整个程序中永远唯一的元素，如class，static变量。 同时方法区里面还有一个叫常量池的地方，String的字符串等常量存储就存储在那边。 3.4 程序计数器 一个非常小的内存空间，用来保存程序执行到的位置（线程私有）。下面是一个程序计数器的演示： 123456789public class TestDemo&#123; public static void main(String args[])&#123; String str = null; str.length(); &#125;&#125; 上面程序会报空指针异常，如下图，在报的这个异常中，有一行日志 at TestDemo,main(TestDemo.java:4) 代表程序运行到TestDemo 中main()函数第四行的时候发生的错误，就是通过程序计数器来记录这个程序运行的位置的。 3.5 本地方法栈 和虚拟机栈类似，不过本地方法栈里面运行的方法不是用java写的，一般是用c或c++写的方法，也有类似栈帧的的概念。 四、内存模型和垃圾回收4.1 内存模型 JVM对于运行时对于共享数据的部分，即堆和方法区做了一个内存划分的规范。以JDK1.8为分界线，稍微有些不同，先看如下图： 两者变化不大，只是将永久带变成了元空间。 一般来说新生代和老年代对应着上一节所讲的堆部分，而永久带或者元空间对应着上一节所说方法区。 这边的内存模型和上一节里面的JVM运行时数据区，可以理解为从不同的角度阐述了同一个物理实物。 4.1.1 年轻代所有新生成的对象首先都是放在年轻代的。年轻代的目标就是尽可能快速的收集掉那些生 命周期短的对象。年轻代分三个区。一个Eden区，两个 Survivor区(一般而言)。大部分对象在Eden区中生成。当Eden区满时，还存活的对象将被复制到Survivor区（两个中的一个），当这个 Survivor区满时，此区的存活对象将被复制到另外一个Survivor区，当这个Survivor去也满了的时候，从第一个Survivor区复制过来的并且此时还存活的对象，将被复制“年老区(Tenured)”。需要注意，Survivor的两个区是对称的，没先后关系，所以同一个区中可能同时存在从Eden复制过来对象，和从前一个Survivor复制过来的对象，而复制到年老区的只有从一个Survivor去过来的对象。而且，Survivor区总有一个是空的。同时，根据程序需要，Survivor区是可以配置为多个的（多于两个），这样可以增加对象在年轻代中的存在时间，减少被放到年老代的可能。 4.1.2 老年代 在年轻代中经历了N次垃圾回收后仍然存活的对象，就会被放到年老代中。因此，可以认为年老代中存放的都是一些生命周期较长的对象。 4.1.3 永久带 用于存放静态文件，如Java类、方法等。永久带对垃圾回收没有显著影响，一般不做垃圾回收，在JVM内存中划分空间。 4.1.4 元空间 类似于永久带，不过它是直接使用物理内存而不占用JVM堆内存。 4.2 垃圾回收机制 垃圾回收是JVM中非常重要的部分，也正是由于这一机制的存在，使得Java语言不用像c++一样需要开发者自己去释放内存。而是通过垃圾回收器GC来进行内存的回收释放，下面来看下这个流程是怎么样的： GC主要处理的是年轻代与老年代的内存清理操作，元空间,永久代一般很少用GC。整个流程图如上图所示总的来说如下流程： ① 当一个新对象产生，需要内存空间，为该对象进行内存空间申请。 ② 首先判断伊甸园区是否有有内存空间，有的话直接将新对象保存在伊甸园区。 ③ 如果此时伊甸园区内存空间不足，会自动触发MinorGC，将伊甸园区不用的内存空间进行清理，清理之后判断伊甸园区内存空间是否充足，充足的话在伊甸园区分配内存空间。 ⑤ 如果此时存活区也没空间了，继续判断老年区，如果老年区空间充足，则将存活区中活跃对象保存到老年代，而后存活区有空余空间，随后伊甸园区将活跃对象保存在存活区之中，在伊甸园区为新对象开辟空间。 ⑥ 如果老年代满了，此时将产生MajorGC进行老年代内存清理，进行完全垃圾回收。 ⑦ 如果老年代执行MajorGC发现依然无法进行对象保存，此时会进行OOM异常（OutOfMemoryError）。 上面流程就是整个垃圾回收机制流程，总的来说，新创建的对象一般都会在伊甸园区生成，除非这个创建对象太大，那有可能直接在老年区生成。 4.3 垃圾回收算法4.3.1 BTP和TLAB算法 ① BTP:Bump-the-Pointer,该算法的主要特点是跟踪在Eden区保存的最后一个对象，类似栈的形式，每次创建新空间时只要判断最后保存的对象是否有足够空间，可极大提高内存分配速度。 ② TLAB：Thread-Local Allocation Buffers BTP不适合多线程，TLAB将Eden区分为多个数据块，每个数据块分别采用BTP进行分配。 这两种算法都在伊甸园区使用，他们的优点在于速度快，由于伊甸园区里面的对象往往是小往往立刻就回收的，很适合这种算法。但这种算法有一个缺点就是当对象回收后会产生许多碎片。对于这个问题就需要下面一种算法来进行弥补了。 4.3.2 复制算法这个算法在新生代的GC中使用，从根集合扫描出存活对象，并将找到的存活对象复制到一块空的空间中，然后清空伊甸园和前面一块有对象的存活区，这块清空的存活区就变成了空的空间供下次复制。这个算法的优点是能整合出大块连续的空间，缺点就是需要有一块空空间来存放复制后的对象，相对来说比较浪费空间。所以这个算法在存活区中使用，存活区也是相对来说最小的一块区域，两个存活区必定有一个区域是空的。 4.3.2 标记压缩算法改算法是老年代里面所采用的垃圾回收算法，采用的方式为从根集合开始扫描，对存活的对象进行标记，标记完毕后，回收不存活对象所占用的内存空间并且会将其他所存活对象都往左端空闲空间进行移动，并更新引用其对象的指针。这个算法不会产生碎片，也不需要存在一块空的空间，而其缺点就是速度慢。所以比较适合回收频率相对较低的老年区。","categories":[],"tags":[{"name":"JVM","slug":"JVM","permalink":"https://liudong-code.github.io/tags/JVM/"}],"keywords":[]},{"title":"MySql数据库","slug":"MySql数据库","date":"2020-03-07T03:06:15.000Z","updated":"2020-03-07T15:10:50.137Z","comments":true,"path":"2020/03/07/MySql数据库/","link":"","permalink":"https://liudong-code.github.io/2020/03/07/MySql%E6%95%B0%E6%8D%AE%E5%BA%93/","excerpt":"","text":"一、数据库隔离级别（详见七）四种隔离级别（SQL92标准）：现在来看看MySQL数据库为我们提供的四种隔离级别（由低到高）： ① Read uncommitted (读未提交)：最低级别，任何情况都无法保证。 ② Read committed (RC，读已提交)：可避免脏读的发生。 ③ Repeatable read (RR，可重复读)：可避免脏读、不可重复读的发生。 （注意事项：InnoDB的RR还可以解决幻读，主要原因是Next-Key锁，只有RR才能使用Next-Key锁） ④ Serializable (串行化)：可避免脏读、不可重复读、幻读的发生。（由MVCC降级为Locking-Base CC） 二、MYSQL有哪些存储引擎，各自优缺点 MyISAM 高速引擎，拥有较高的插入，查询速度，但不支持事务、不支持行锁、支持3种不同的存储格式。包括静态型、动态型和压缩型。 InnoDB 5.5版本后MySQL的默认数据库，支持事务和行级锁定，事务处理、回滚、崩溃修复能力和多版本并发控制的事务安全，比MyISAM处理速度稍慢、支持外键（FOREIGN KEY） ISAM MyISAM的前身，MySQL5.0以后不再默认安装 MRG_MyISAM（MERGE） 将多个表联合成一个表使用，在超大规模数据存储时很有用 Memory 内存存储引擎，拥有极高的插入，更新和查询效率。但是会占用和数据量成正比的内存空间。只在内存上保存数据，意味着数据可能会丢失 Falcon 一种新的存储引擎，支持事物处理，传言可能是InnoDB的替代者 Archive 将数据压缩后进行存储，非常适合存储大量的独立的，作为历史记录的数据，但是只能进行插入和查询操作 CSV 存储引擎是基于 CSV 格式文件存储数据(应用于跨平台的数据交换) 存储引擎的选型：InnoDB：支持事务处理，支持外键，支持崩溃修复能力和并发控制。如果需要对事务的完整性要求比较高（比如银行），要求实现并发控制（比如售票），那选择InnoDB有很大的优势。如果需要频繁的更新、删除操作的数据库，也可以选择InnoDB，因为支持事务的提交（commit）和回滚（rollback）。 MyISAM：插入数据快，空间和内存使用比较低。如果表主要是用于插入新记录和读出记录，那么选择MyISAM能实现处理高效率。如果应用的完整性、并发性要求比 较低，也可以使用。 MEMORY：所有的数据都在内存中，数据的处理速度快，但是安全性不高。如果需要很快的读写速度，对数据的安全性要求较低，不需要持久保存，可以选择MEMOEY。它对表的大小有要求，不能建立太大的表。所以，这类数据库只使用在相对较小的数据库表。 注意，同一个数据库也可以使用多种存储引擎的表。如果一个表要求比较高的事务处理，可以选择InnoDB。这个数据库中可以将查询要求比较高的表选择MyISAM存储。如果该数据库需要一个用于查询的临时表，可以选择MEMORY存储引擎。 三、高并发下，如何做到安全的修改同一行数据1、使用悲观锁悲观锁本质是当前只有一个线程执行操作，排斥外部请求的修改。遇到加锁的状态，就必须等待。结束了唤醒其他线程进行处理。虽然此方案的确解决了数据安全的问题，但是，我们的场景是“高并发”。也就是说，会很多这样的修改请求，每个请求都需要等待“锁”，某些线程可能永远都没有机会抢到这个“锁”，这种请求就会死在那里。同时，这种请求会很多，瞬间增大系统的平均响应时间，结果是可用连接数被耗尽，系统陷入异常。 2、FIFO（First Input First Output，先进先出）缓存队列思路直接将请求放入队列中，就不会导致某些请求永远获取不到锁。 3、使用乐观锁这个时候，我们就可以讨论一下“乐观锁”的思路了。乐观锁，是相对于“悲观锁”采用更为宽松的加锁机制，大都是采用带版本号（Version）更新。实现就是，这个数据所有请求都有资格去修改，但会获得一个该数据的版本号，只有版本号符合的才能更新成功，其他的返回抢购失败。这样的话，我们就不需要考虑队列的问题，不过，它会增大CPU的计算开销。但是，综合来说，这是一个比较好的解决方案。 4、Zookeeper 锁 基于异常的分布式锁（基于临时节点）创建不带序号的节点， 创建成功获得锁， 创建不成功， 会抛出异常， 监听lock 节点， 当lock 删除时，再重新去创建节点 四、SQL执行计划一、执行计划是什么？ 执行计划，简单的来说，是SQL在数据库中执行时的表现情况,通常用于SQL性能分析,优化等场景。在MySQL中使用 explain 关键字来查看 二、查看执行计划1EXPLAIN + sql语句 三、MySQL执行计划分析 sql如何使用索引 联接查询的执行顺序 查询扫描的数据行数 四、各个字段的含义ID id列中的数据为一组数字，表示执行select语句顺序 id值相同时，执行顺序由上至下 id值越大优先级越高，越先被执行 SELECT_TYPE 查询的类型，主要是用于区分普通查询、联合查询、子查询等复杂的查询。 SIMPLE：简单的select查询，查询中不包含子查询或者union PRIMARY：查询中包含任何复杂的子部分，最外层查询则被标记为primary SUBQUERY：在select 或 where列表中包含了子查询 DERIVED：在from列表中包含的子查询被标记为derived（衍生），mysql会递归执行这些子查询，把结果放在临时表里 UNION：若第二个select出现在union之后，则被标记为union；若union包含在from子句的子查询中，外层select将被标记为derived UNION RESULT：从union表获取结果的select DEPENDENT SUBQUERY: 依赖外部结果的子查询 DEPENDENT UNION: 当union作为子查询时，第二或是第二个后的查询的 select_type值 /article/details/91349161 TABLE 输出数据行所在的表的名称 如果不涉及对数据表的操作，那么这显示为null 如果显示为尖括号括起来的就表示这个是临时表，后边的N就是执行计划中的id，表示结果来自于这个查询产生。 &lt; unionM,N&gt;由ID为M,N查询union产生的结果集 &lt; derivedN&gt;/&lt; subqueryN&gt; 由Id为n的查询产生的结果 PARTITIONS 对于分区表，显示查询的分区ID 对于非分区表，显示的为 NULL TYPE 访问类型，sql查询优化中一个很重要的指标，结果值从好到坏依次是： system &gt; const &gt; eq_ref &gt; ref &gt; fulltext &gt; ref_or_null &gt; index_merge &gt; unique_subquery &gt; index_subquery &gt; range &gt; index &gt; ALL system 这是const联接类型的一个特例，当查询的表只有一行时使用 const：表示通过索引一次就找到了，const用于比较primary key 或者 unique索引。因为只需匹配一行数据，所有很快。如果将主键置于where列表中，mysql就能将该查询转换为一个const eq_ref：唯一性索引扫描，对于每个索引键，表中只有一条记录与之匹配。常见于主键 或 唯一索引扫描。 ref：非唯一性索引扫描，返回匹配某个单独值的所有行。本质是也是一种索引访问，它返回所有匹配某个单独值的行，然而他可能会找到多个符合条件的行，所以它应该属于查找和扫描的混合体 ref_or_null 类似于 ref类型的查询，但是附加了对 null 值列的查询 range：只检索给定范围的行，使用一个索引来选择行。key列显示使用了那个索引。一般就是在where语句中出现了between、&lt;、&gt;、in等的查询。这种索引列上的范围扫描比全索引扫描要好。只需要开始于某个点，结束于另一个点，不用扫描全部索引 index：Full Index Scan，index与ALL区别为index类型只遍历索引树。这通常为ALL块，应为索引文件通常比数据文件小。（Index与ALL虽然都是读全表，但index是从索引中读取，而ALL是从硬盘读取） ALL：Full Table Scan，遍历全表以找到匹配的行 index：关键字：条件是出现在索引树中的节点的。可能没有完全匹配索引 索引全表扫描，把索引从头到尾扫一遍，常见于使用索引列就可以处理不需要读取数据文件的查询、可以使用索引排序或者分组的查询。 POSSIBLE_KEYS 指出mysql能使用哪些索引来优化查询 查询列所涉及的列上的索引都会被列出来，但不一定会被使用 KEY 实际使用的索引，如果为NULL，则没有使用索引。 查询中如果使用了覆盖索引，则该索引仅出现在key列表中。 REF 如果是使用的常数等值查询，这里会显示const 如果是连接查询，被驱动表的执行计划这里会显示驱动表的关联字段 如果是条件使用了表达式或者函数，或者条件列发生了内部隐式转换，这里可能显示为func ROWS 表示msql通过索引的统计信息，估算的所需读取的行数 rows值的大小时个统计抽样的结果，并不十分的准确 extra（重要） distinct ：在select部分使用了distinct关键字 no tables used：不带from字句的查询或者From dual查询 使用not in()形式子查询或not exists运算符的连接查询，这种叫做反连接即，一般连接查询是先查询内表，再查询外表，反连接就是先查询外表，再查询内表。 using filesort（重要） ​ 排序时无法使用到索引时，就会出现这个。常见于order by和group by语句中 ​ 说明MySQL会使用一个外部的索引排序，而不是按照索引顺序进行读取。 ​ MySQL中无法利用索引完成的排序操作称为“文件排序” using index（重要） 查询时不需要回表查询，直接通过索引就可以获取查询的数据。 ​ 表示相应的SELECT查询中使用到了覆盖索引（Covering Index），避免访问表的数据行，效率不错！​ 如果同时出现Using Where ，说明索引被用来执行查找索引键值​ 如果没有同时出现Using Where ，表明索引用来读取数据而非执行查找动作。 using temporary ​ 表示使用了临时表存储中间结果。​ MySQL在对查询结果order by和group by时使用临时表​ 临时表可以是内存临时表和磁盘临时表，执行计划中看不出来，需要查看status变量，used_tmp_table，used_tmp_disk_table才能看出来。 using where（重要） 表示存储引擎返回的记录并不是所有的都满足查询条件，需要在server层进行过滤。 查询条件中分为限制条件和检查条件，5.6之前，存储引擎只能根据限制条件扫描数据并返回，然后server层根据检查条件进行过滤再返回真正符合查询的数据。5.6.x之后支持ICP特性，可以把检查条件也下推到存储引擎层，不符合检查条件和限制条件的数据，直接不读取，这样就大大减少了 五、MySQL索引a.索引是什么​ 官方介绍索引是帮助MySQL高效获取数据的数据结构。更通俗的说，数据库索引好比是一本书前面的目录，能加快数据库的查询速度。 b.索引的优势和劣势优势：可以提高数据检索的效率，降低数据库的IO成本，类似于书的目录。 – 检索 通过索引列对数据进行排序，降低数据排序的成本，降低了CPU的消耗。 –排序 被索引的列会自动进行排序，包括【单列索引】和【组合索引】，只是组合索引的排序要复杂一些。 如果按照索引列的顺序进行排序，对应order by语句来说，效率就会提高很多。 where 索引列 在存储引擎层 处理 覆盖索引 select 字段 字段是索引 劣势：索引会占据磁盘空间 索引虽然会提高查询效率，但是会降低更新表的效率。比如每次对表进行增删改操作，MySQL不仅要保存数据，还有保存或者更新对应的索引文件 c.索引的分类​ 单列索引​ 组合索引 *​ 全文索引​ 空间索引​ 位图索引 Oracle d.索引的使用创建索引单列索引之普通索引12CREATE INDEX index_name ON table(column(length)) ;ALTER TABLE table_name ADD INDEX index_name (column(length)) ; 单列索引之唯一索引12CREATE UNIQUE INDEX index_name ON table(column(length)) ;alter table table_name add unique index index_name(column); 单列索引之全文索引12CREATE FULLTEXT INDEX index_name ON table(column(length)) ;alter table table_name add fulltext index_name(column) 组合索引1ALTER TABLE article ADD INDEX index_titme_time (title(50),time(10)) ; 删除索引1DROP INDEX index_name ON table 查看索引1SHOW INDEX FROM table_name \\G e.索引原理分析索引存储结构 索引是在存储引擎中实现的，也就是说不同的存储引擎，会使用不同的索引 MyISAM和InnoDB存储引擎：只支持B+ TREE索引， 也就是说默认使用BTREE，不能够更换 MEMORY/HEAP存储引擎：支持HASH和BTREE索引 B树和B+树数据结构示例网站：https://www.cs.usfca.edu/~galles/visualization/Algorithms.html B树图示B树是为了磁盘或其它存储设备而设计的一种多叉（下面你会看到，相对于二叉，B树每个内结点有多个分支，即多叉）平衡查找树。 多叉平衡 B树的高度一般都是在2-4这个高度，树的高度直接影响IO读写的次数。 如果是三层树结构—支撑的数据可以达到20G，如果是四层树结构—支撑的数据可以达到几十T B和B+的区别​ B树和B+树的最大区别在于非叶子节点是否存储数据的问题。 ​ B树是非叶子节点和叶子节点都会存储数据。 ​ B+树只有叶子节点才会存储数据，而且存储的数据都是在一行上，而且这些数据都是有指针指向的，也就是有顺序的。 非聚集索引（MyISAM） 叶子节点下面保存的是数据的地址，再由地址去找到真正的数据 辅助索引（次要索引） 聚集索引（InnoDB） 叶子节点上 挂着所有的数据 主键索引数 主键：1、建主键2、没建主键找唯一字段 当主键自动生成伪列 当主键主键创建自增整数不要用大字符串比如 uuid 辅助索引（次要索引） 辅助索引的叶子节点下面储存的是 主键值 回表select * from t where name=’Alice’ 给name做了索引，查询全部，但是在索引树上面没有全部的数据，就照成了回表。 解决方案：用组合索引对查询的值形成覆盖。 聚集索引是MyISAM采用的索引形式，B+Tree的叶子节点存储的是数据的地址 非聚集索引是INNODB引擎采用的索引形式，B+Tree的叶子节点存储的是字段的值 f.索引的使用场景哪些情况需要创建索引1、主键自动建立唯一索引2、频繁作为查询条件的字段应该创建索引3、多表关联查询中，关联字段应该创建索引 on 两边都要创建索引4、查询中排序的字段，应该创建索引 B + tree 有顺序5、覆盖索引 好处是？ 不需要回表 组合索引6、统计或者分组字段，应该创建索引 哪些情况不需要创建索引1、表记录太少 索引是要有存储的开销2、频繁更新 索引要维护3、查询字段使用频率不高 g.组合索引由多个字段组成的索引 使用顺序就是创建的顺序 1ALTER TABLE 'table_name' ADD INDEX index_name(col1,col2,col3） 使用：遵循最左前缀原则1、前缀索引like 常量% 使用索引 like %常量 不使用索引2、最左前缀从左向右匹配直到遇到范围查询 &gt; &lt; between 索引失效 h.索引失效1.全值匹配我最爱123456789101112mysql&gt; explain select * from tuser where name='zhaoyun' and age=1 and sex='1';+----+-------------+-------+------+------------------+------------------+---------+-------------------+------+-----------------------+| id | select_type | table | type | possible_keys | key |key_len | ref | rows | Extra |+----+-------------+-------+------+------------------+------------------+---------+-------------------+------+-----------------------+| 1 | SIMPLE | tuser | ref | idx_name_age_sex | idx_name_age_sex | 312| const,const,const | 1 | Using index condition |+----+-------------+-------+------+------------------+------------------+---------+-------------------+------+-----------------------+条件与索引一一对应 2.最佳左前缀法则1组合索引 : 带头索引不能死，中间索引不能断 如果索引了多个列，要遵守最佳左前缀法则。指的是查询从索引的最左前列开始 并且不跳过索引中的列。 3.不要在索引上做计算4.范围条件右边的列失效不能继续使用索引中范围条件（bettween、&lt;、&gt;、in等）右边的列 1234567891011mysql&gt; explain select * from tuser where name='asd' and age&gt;20 and sex='1';+----+-------------+-------+-------+------------------+------------------+---------+------+------+-----------------------+| id | select_type | table | type | possible_keys | key |key_len | ref | rows | Extra |+----+-------------+-------+-------+------------------+------------------+---------+------+------+-----------------------+| 1 | SIMPLE | tuser | range | idx_name_age_sex | idx_name_age_sex | 308| NULL | 1 | Using index condition |+----+-------------+-------+-------+------------------+------------------+---------+------+------+-----------------------+ 5.尽量使用覆盖索引6.索引字段上不要使用不等索引字段上使用（！= 或者 &lt; &gt;）判断时，会导致索引失效而转向全表扫描 1234567891011mysql&gt; explain select * from tuser where loginname!='zhy';+----+-------------+-------+------+---------------+------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows| Extra |+----+-------------+-------+------+---------------+------+---------+------+------+-------------+| 1 | SIMPLE | tuser | ALL | idx_loginname | NULL | NULL | NULL | 1| Using where |+----+-------------+-------+------+---------------+------+---------+------+------+-------------+ 7.主键索引字段上不可以判断null12345678910111213141516171819202122232425主键字段上不可以使用 null索引字段上使用 is null 判断时，可使用索引mysql&gt; explain select * from tuser where name is null;+----+-------------+-------+------+------------------+------------------+---------+-------+------+-----------------------+| id | select_type | table | type | possible_keys | key |key_len | ref | rows | Extra |+----+-------------+-------+------+------------------+------------------+---------+-------+------+-----------------------+| 1 | SIMPLE | tuser | ref | idx_name_age_sex | idx_name_age_sex | 303| const | 1 | Using index condition |+----+-------------+-------+------+------------------+------------------+---------+-------+------+-----------------------+1 row in set (0.00 sec)mysql&gt; explain select * from tuser where loginname is null;+----+-------------+-------+------+---------------+---------------+---------+-------+------+-----------------------+| id | select_type | table | type | possible_keys | key | key_len |ref | rows | Extra |+----+-------------+-------+------+---------------+---------------+---------+-------+------+-----------------------+| 1 | SIMPLE | tuser | ref | idx_loginname | idx_loginname | 303 |const | 1 | Using index condition |+----+-------------+-------+------+---------------+---------------+---------+-------+------+-----------------------+ 8.索引字段使用like不以通配符开头1234567891011121314151617181920212223索引字段使用like以通配符开头（‘%字符串’）时，会导致索引失效而转向全表扫描mysql&gt; explain select * from tuser where name like 'a%';+----+-------------+-------+-------+------------------+------------------+---------+------+------+-----------------------+| id | select_type | table | type | possible_keys | key |key_len | ref | rows | Extra |+----+-------------+-------+-------+------------------+------------------+---------+------+------+-----------------------+| 1 | SIMPLE | tuser | range | idx_name_age_sex | idx_name_age_sex | 303| NULL | 1 | Using index condition |+----+-------------+-------+-------+------------------+------------------+---------+------+------+-----------------------+mysql&gt; explain select * from tuser where name like '%a';+----+-------------+-------+------+---------------+------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows| Extra |+----+-------------+-------+------+---------------+------+---------+------+------+-------------+| 1 | SIMPLE | tuser | ALL | NULL | NULL | NULL | NULL | 2| Using where |+----+-------------+-------+------+---------------+------+---------+------+------+-------------+ 9.索引字段字符串要加单引号索引字段是字符串，但查询时不加单引号，会导致索引失效而转向全表扫描 1234567891011mysql&gt; explain select * from tuser where name=123;+----+-------------+-------+------+------------------+------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref |rows | Extra |+----+-------------+-------+------+------------------+------+---------+------+------+-------------+| 1 | SIMPLE | tuser | ALL | idx_name_age_sex | NULL | NULL | NULL |2 | Using where |+----+-------------+-------+------+------------------+------+---------+------+------+-------------+ 10.索引字段不要使用or123456789101112索引字段使用 or 时，会导致索引失效而转向全表扫描mysql&gt; explain select * from tuser where name='asd' or age=23;+----+-------------+-------+------+------------------+------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref |rows | Extra |+----+-------------+-------+------+------------------+------+---------+------+------+-------------+| 1 | SIMPLE | tuser | ALL | idx_name_age_sex | NULL | NULL | NULL |2 | Using where |+----+-------------+-------+------+------------------+------+---------+------+------+-------------+ 11.口诀1234567全值匹配我最爱，最左前缀要遵守 带头大哥不能死，中间兄弟不能断 索引列上少计算，范围之后全失效 LIKE符号写最右，覆盖索引不写星 不等空值还有or，索引失效要少用 var引号不能丢，SQL高级也不难 分组之前必排序，一定要上索引啊 六、数据库MySQL锁 一.表级锁介绍由MySQL SQL layer层实现 1.MySQL的表级锁有两种：12一种是表锁。一种是元数据锁（meta data lock，MDL) 2.表锁介绍表锁有两种表现形式： 12表共享读锁（Table Read Lock）表独占写锁（Table Write Lock） 手动增加表锁 1lock table 表名称 read(write),表名称2 read(write)，其他; 删除表锁 1unlock tables; 3.元数据锁介绍MDL (metaDataLock) 元数据：表结构 在 MySQL 5.5 版本中引入了 MDL，当对一个表做增删改查操作的时候，加 MDL 读锁；当要对表做结构变更操作的时候，加 MDL 写锁。 二、行级锁行级锁介绍InnoDB存储引擎实现InnoDB的行级锁，按照锁定范围来说，分为三种： 记录锁（Record Locks）:锁定索引中一条记录。 主键指定 where id=3 间隙锁（Gap Locks）: 锁定记录前、记录中、记录后的行 RR隔离级 （可重复读） Next-Key 锁: 记录锁 + 间隙锁 行级锁分类按照功能来说，分为两种： 共享读锁（S）：​ 允许一个事务去读一行，阻止其他事务获得相同数据集的排他锁。 12SELECT * FROM table_name WHERE ... LOCK IN SHARE MODE -- 共享读锁 手动添加select * from table -- 无锁 排他写锁（X）：允许获得排他写锁的事务更新数据，阻止其他事务取得相同数据集的共享读锁（不是读）和排他写锁。1、自动加 DML对于UPDATE、DELETE和INSERT语句，InnoDB会自动给涉及数据集加排他锁（X)； 2、手动加1SELECT * FROM table_name WHERE ... FOR UPDATE InnoDB也实现了表级锁，也就是意向锁，意向锁是mysql内部使用的，不需要用户干预。 意向共享锁（IS）：事务打算给数据行加行共享锁，事务在给一个数据行加共享锁前必须先取得该表的IS锁。 意向排他锁（IX）：事务打算给数据行加行排他锁，事务在给一个数据行加排他锁前必须先取得该表的IX锁。 意向锁的主要作用是为了【全表更新数据】时的性能提升。否则在全表更新数据时，需要先检索该表是否某些记录上面有行锁。 两阶段锁（2PL） 锁操作分为两个阶段：加锁阶段与解锁阶段， 加锁阶段与解锁阶段不相交。 加锁阶段：只加锁，不放锁。 解锁阶段：只放锁，不加锁 三，行锁演示InnoDB行锁是通过给索引上的索引项加锁来实现的，因此InnoDB这种行锁实现特点意味着：只有通过索引条件检索的数据，InnoDB才使用行级锁，否则，InnoDB将使用表锁！ 行读锁session1（Navicat）、session2（mysql）Innodb_row_lock_current_waits：当前正在等待锁定的数量； Innodb_row_lock_time：从系统启动到现在锁定总时间长度； Innodb_row_lock_time_avg：每次等待所花平均时间； Innodb_row_lock_time_max：从系统启动到现在等待最常的一次所花的时间； Innodb_row_lock_waits：系统启动后到现在总共等待的次数； 12345678910111213查看行锁状态 show STATUS like 'innodb_row_lock%';1、session1: begin;--开启事务未提交select * from mylock where ID=1 lock in share mode; --手动加id=1的行读锁,使用索引2、session2：update mylock set name='y' where id=2; -- 未锁定该行可以修改3、session2：update mylock set name='y' where id=1; -- 锁定该行修改阻塞ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction-- 锁定超时4、session1: commit; --提交事务 或者 rollback 释放读锁5、session2：update mylock set name='y' where id=1; --修改成功Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0注：使用索引加行锁 ，未锁定的行可以访问 行读锁升级为表锁1234567891、session1: begin;--开启事务未提交--手动加name='c'的行读锁,未使用索引select * from mylock where name='c' lock in share mode;2、session2：update mylock set name='y' where id=2; -- 修改阻塞 未用索引行锁升级为表锁3、session1: commit; --提交事务 或者 rollback 释放读锁4、session2：update mylock set name='y' where id=2; --修改成功Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0注：未使用索引行锁升级为表锁 行写锁123456789101、session1: begin;--开启事务未提交--手动加id=1的行写锁,select * from mylock where id=1 for update;2、session2：select * from mylock where id=2 ; -- 可以访问3、session2: select * from mylock where id=1 ; -- 可以读 不加锁4、session2: select * from mylock where id=1 lock in share mode ; -- 加读锁被阻塞5、session1：commit; -- 提交事务 或者 rollback 释放写锁5、session2：执行成功主键索引产生记录锁 间隙锁12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152案例演示：mysql&gt; create table news (id int, number int,primary key (id));mysql&gt; insert into news values(1,2);......--加非唯一索引mysql&gt; alter table news add index idx_num(number);-- 非唯一索引的等值session 1:start transaction ;select * from news where number=4 for update ;session 2:start transaction ;insert into news value(2,4);#（阻塞）insert into news value(2,2);#（阻塞）insert into news value(4,4);#（阻塞）insert into news value(4,5);#（阻塞）insert into news value(7,5);#（执行成功）insert into news value(9,5);#（执行成功）insert into news value(11,5);#（执行成功）注：id和number都在间隙内则阻塞。 -- 主键索引的范围session 1:start transaction ;select * from news where id&gt;1 and id &lt; 4 for update;session 2:start transaction ;insert into news value(2,4);#（阻塞）insert into news value(2,2);#（阻塞）insert into news value(4,4);#（阻塞）insert into news value(4,5);#（阻塞）insert into news value(7,5);#（执行成功）insert into news value(9,5);#（执行成功）insert into news value(11,5);#（执行成功）````注：id和number都在间隙内则阻塞。 --无穷大session 1:start transaction ;select * from news where number=13 for update ;update news set xx where number=13 ;session 2:start transaction ;insert into news value(11,5);#(执行成功)insert into news value(12,11);#(执行成功)insert into news value(14,11);#(阻塞)insert into news value(15,12);#(阻塞)检索条件number=13,向左取得最靠近的值11作为左区间，向右由于没有记录因此取得无穷大作为右区间，因此，session 1的间隙锁的范围（11，无穷大）注：非主键索引产生间隙锁，主键范围产生间隙锁 四，死锁两个 session 互相等等待对方的资源释放之后，才能释放自己的资源,造成了死锁 123456789101、session1: begin;--开启事务未提交--手动加行写锁 id=1 ，使用索引update mylock set name='m' where id=1;2、session2：begin;--开启事务未提交--手动加行写锁 id=2 ，使用索引update mylock set name='m' where id=2;3、session1: update mylock set name='nn' where id=2; -- 加写锁被阻塞4、session2：update mylock set name='nn' where id=1; -- 加写锁会死锁，不允许操作ERROR 1213 (40001): Deadlock found when trying to get lock; try restartingtransaction 七、MySQL事务事务介绍在MySQL中的事务是由存储引擎实现的，而且支持事务的存储引擎不多，我们主要讲解InnoDB存储引擎中的事务。 事务处理可以用来维护数据库的完整性，保证成批的 SQL 语句要么全部执行，要么全部不执行。 事务用来管理DDL、DML、DCL 操作，比如 insert,update,delete 语句，默认是自动提交的。 事务开启BEGIN 或START TRANSACTION；显式地开启一个事务； COMMIT 也可以使用COMMIT WORK ，不过二者是等价的。COMMIT会提交事务，并使已对数据库进行的所有修改称为永久性的； ROLLBACK 有可以使用ROLLBACK WORK，不过二者是等价的。回滚会结束用户的事务，并撤销正在进行的所有未提交的修改； 事务四大特性(ACID)Atomicity（原子性）：构成事务的的所有操作必须是一个逻辑单元，要么全部执行，要么全部不执行。 Consistency（一致性）：数据库在事务执行前后状态都必须是稳定的或者是一致的。 Isolation（隔离性）：事务之间不会相互影响。 ​ 由锁机制和MVCC机制来实现的 ​ MVCC(多版本并发控制)：优化读写性能（读不加锁、读写不冲突） Durability（持久性）：事务执行成功后必须全部写入磁盘。 总结来说，事务的隔离性 由多版本控制机制和锁实现，而原子性、一致性和持久性通过InnoDB的redo log、undo log和Force Log at Commit机制来实现。 RedoLog数据库日志和数据落盘机制，如下图所示: redo log写入磁盘时，必须进行一次操作系统的fsync操作，防止redo log只是写入了操作系统的磁盘缓存中。参数innodb_flush_log_at_trx_commit可以控制redo log日志刷新到磁盘的策略 UndoLog数据库崩溃重启后需要从redo log中把未落盘的脏页数据恢复出来，重新写入磁盘，保证用户的数据不丢失。当然，在崩溃恢复中还需要回滚没有提交的事务。由于回滚操作需要undo日志的支持，undo日志的完整性和可靠性需要redo日志来保证，所以崩溃恢复先做redo恢复数据，然后做undo回滚。 在事务执行的过程中，除了记录redo log，还会记录一定量的undo log。undo log记录了数据在每个操作前的状态，如果事务执行过程中需要回滚，就可以根据undo log进行回滚操作。 数据和回滚日志的逻辑存储结构 undo log的存储不同于redo log，它存放在数据库内部的一个特殊的段(segment)中，这个段称为回滚段。回滚段位于共享表空间中。undo段中的以undo page为更小的组织单位。undo page和存储数据库数据和索引的页类似。因为redo log是物理日志，记录的是数据库页的物理修改操作。所以undolog（也看成数据库数据）的写入也会产生redo log，也就是undo log的产生会伴随着redo log的产生，这是因为undo log也需要持久性的保护。如上图所示，表空间中有回滚段和叶节点段和非叶节点段，而三者都有对应的页结构。 我们再来总结一下数据库事务的整个流程，如下图所示 事务进行过程中，每次sql语句执行，都会记录undo log和redo log，然后更新数据形成脏页，然后redo log按照时间或者空间等条件进行落盘，undo log和脏页按照checkpoint进行落盘，落盘后相应的redo log就可以删除了。此时，事务还未COMMIT，如果发生崩溃，则首先检查checkpoint记录，使用相应的redo log进行数据和undo log的恢复，然后查看undo log的状态发现事务尚未提交，然后就使用undo log进行事务回滚。事务执行COMMIT操作时，会将本事务相关的所有redo log都进行落盘，只有所有redo log落盘成功，才算COMMIT成功。然后内存中的数据脏页继续按照checkpoint进行落盘。如果此时发生了崩溃，则只使用redo log恢复数据。 隔离性事务并发问题 在事务的并发操作中可能会出现一些问题： 丢失更新：两个事务针对同一数据都发生修改操作时，会存在丢失更新问题。 脏读：一个事务读取到另一个事务未提交的数据。 不可重复读：一个事务因读取到另一个事务已提交的update或者delete数据。导致对同一条记录读取两次以上的结果不一致。 幻读：一个事务因读取到另一个事务已提交的insert数据。导致对同一张表读取两次以上的结果不一致。 事务隔离级别现在来看看MySQL数据库为我们提供的四种隔离级别（由低到高）：① Read uncommitted (读未提交)：最低级别，任何情况都无法保证。② Read committed (RC，读已提交)：可避免脏读的发生。③ Repeatable read (RR，可重复读)：可避免脏读、不可重复读的发生。（注意事项：InnoDB的RR还可以解决幻读，主要原因是Next-Key锁，只有RR才能使用Next-Key锁）④ Serializable (串行化)：可避免脏读、不可重复读、幻读的发生。（由MVCC降级为Locking-Base CC） InnoDB的MVCC实现我们首先来看一下wiki上对MVCC的定义： 123Multiversion concurrency control (MCC or MVCC), is a concurrency control methodcommonly used by database management systems to provide concurrent access to thedatabase and in programming languages to implement transactional memory. 当前读和快照读在MVCC并发控制中，读操作可以分成两类：快照读 (snapshot read)与当前读 (current read)。 快照读，读取的是记录的可见版本 (有可能是历史版本)，不用加锁。 当前读，读取的是记录的最新版本，并且当前读返回的记录，都会加上锁，保证其他事务不会再并发修改这条记录。 在一个支持MVCC并发控制的系统中，哪些读操作是快照读？哪些操作又是当前读呢？以MySQL InnoDB为例： 快照读：简单的select操作，属于快照读，不加锁。(当然，也有例外，下面会分析) 不加读锁 读历史版本 当前读：特殊的读操作，插入/更新/删除操作，属于当前读，需要加锁。 加行写锁 读当前版本 一致性非锁定读 一致性非锁定读(consistent nonlocking read)是指InnoDB存储引擎通过多版本控制(MVCC)读取当前数据库中行数据的方式。 如果读取的行正在执行DELETE或UPDATE操作，这时读取操作不会因此去等待行上锁的释放。相反地，InnoDB会去读取行的一个最新可见快照。 MVCC 在mysql 中的实现依赖的是 undo log 与 read view 。 Undo Log 结构InnoDB行记录有三个隐藏字段：分别对应该行的rowid、事务号db_trx_id和回滚指针db_roll_ptr， 其中 db_trx_id表示最近修改的事务的id，db_roll_ptr指向回滚段中的undo log。 根据行为的不同，undo log分为两种：insert undo log和update undo log insert undo log：是在 insert 操作中产生的 undo log。因为 insert 操作的记录只对事务本身可见， rollback 在该事务中直接删除 ，不需要进行 purge 操作 update undo log ：是 update 或 delete 操作中产生的 undo log，因为会对已经存在的记录产生影响， rollback MVCC机制会找他的历史版本进行恢复 是 update 或 delete 操作中产生的 undo log，因为会对已经存在的记录产生影响，为了提供 MVCC机制，因此 update undo log 不能在事务提交时就进行删除，而是将事务提交时放到入 history list 上，等待 purge 线程进行最后的删除操作。 如下图所示（初始状态）： 当事务2使用UPDATE语句修改该行数据时，会首先使用排他锁锁定改行，将该行当前的值复制到undolog中，然后再真正地修改当前行的值，最后填写事务ID，使用回滚指针指向undo log中修改前的行。 如下图所示（第一次修改）： 当事务3进行修改与事务2的处理过程类似，如下图所示（第二次修改） 事务链表MySQL中的事务在开始到提交这段过程中，都会被保存到一个叫trx_sys的事务链表中，这是一个基本的链表结构： 1ct-trx --&gt; trx11 --&gt; trx9 --&gt; trx6 --&gt; trx5 --&gt; trx3; 事务链表中保存的都是还未提交的事务，事务一旦被提交，则会被从事务链表中摘除 RR隔离级别下，在每个事务开始的时候，会将当前系统中的所有的活跃事务拷贝到一个列表中(readview) RC隔离级别下，在每个语句开始的时候，会将当前系统中的所有的活跃事务拷贝到一个列表中(readview) show engine innodb status ,就能够看到事务列表。 ReadView当前事务（读）能读哪个历史版本？ Read View是事务开启时当前所有事务的一个集合，这个类中存储了当前Read View中最大事务ID及最小事务ID。 这就是当前活跃的事务列表。如下所示， 1ct-trx --&gt; trx11 --&gt; trx9 --&gt; trx6 --&gt; trx5 --&gt; trx3; ct-trx 表示当前事务的id，对应上面的read_view数据结构如下， 1234read_view-&gt;creator_trx_id = ct-trx;read_view-&gt;up_limit_id = trx3; 低水位read_view-&gt;low_limit_id = trx11; 高水位read_view-&gt;trx_ids = [trx11, trx9, trx6, trx5, trx3]; low_limit_id是“高水位”，即当时活跃事务的最大id，如果读到row的db_trx_id&gt;=low_limit_id，说明这些id在此之前的数据都没有提交，如注释中的描述，这些数据都不可见。 1234if (trx_id &gt;= view-&gt;low_limit_id) &#123;return(FALSE);&#125;注：readview 部分源码 up_limit_id是“低水位”，即当时活跃事务列表的最小事务id，如果row的db_trx_id&lt;up_limit_id,说明这些数据在事务创建的id时都已经提交，如注释中的描述，这些数据均可见。 123if (trx_id &lt; view-&gt;up_limit_id) &#123; return(TRUE);&#125; row的db_trx_id在low_limit_id和up_limit_id之间，则查找该记录的db_trx_id是否在自己事务的read_view-&gt;trx_ids列表中，如果在则该记录的当前版本不可见，否则该记录的当前版本可见。 不同隔离级别ReadView实现方式 read-commited: 12345678910函数：ha_innobase::external_lockif (trx-&gt;isolation_level &lt;= TRX_ISO_READ_COMMITTED &amp;&amp; trx-&gt;global_read_view) &#123; / At low transaction isolation levels we let each consistent read set its own snapshot / read_view_close_for_mysql(trx); repeatable read： 在repeatable read的隔离级别下，创建事务trx结构的时候，就生成了当前的global read view。使用trx_assign_read_view函数创建，一直维持到事务结束。在事务结束这段时间内 每一次查询都不会重新重建Read View ， 从而实现了可重复读。 八、MySQL 落盘InnoDB架构图 InnoDB磁盘文件InnoDB的主要的磁盘文件主要分为三大块：一是系统表空间，二是用户表空间，三是redo日志文件和归档文件。二进制文件(binlog)等文件是MySQL Server层维护的文件，所以未列入InnoDB的磁盘文件中。 系统表空间和用户表空间 InnoDB系统表空间包含InnoDB数据字典(元数据以及相关对象)并且double write buffer,change buffer,undo logs的存储区域。 系统表空间也默认包含任何用户在系统表空间创建的表数据和索引数据。系统表空间是一个共享的表空间因为它是被多个表共享的。 系统表空间是由一个或者多个数据文件组成。默认情况下,1个初始大小为10MB，名为ibdata1的系统数据文件在MySQL的data目录下被创建。用户可以使用innodb_data_file_path 对数据文件的大小和数量进行配置。 innodb_data_file_path 的格式如下： 1innodb_data_file_path=datafile1[,datafile2]... 用户可以通过多个文件组成一个表空间，同时制定文件的属性： 1innodb_data_file_path = /db/ibdata1:1000M;/dr2/db/ibdata2:1000M:autoextend 设置innodb_data_file_path参数之后，所有基于InnoDB存储引擎的表的数据都会记录到该系统表空间中，如果设置了参数innodb_file_per_table，则用户可以将每个基于InnoDB存储引擎的表产生一个独立的用户表空间。用户表空间的命名规则为：表名.ibd。通过这种方式，用户不用将所有数据都存放于默认的系统表空间中，但是用户表空间只存储该表的数据、索引和插入缓冲BITMAP等信息，其余信息还是存放在默认的系统表空间中。 下图显示InnoDB存储引擎对于文件的存储方式，其中frm文件是表结构定义文件，记录每个表的表结构定义。 系统表空间（共享表空间）1、数据字典(data dictionary)：记录数据库相关信息2、doublewrite write buffer：解决部分写失败（页断裂）3、insert buffer：内存insert buffer数据，周期写入共享表空间，防止意外宕机4、回滚段(rollback segments)5、undo空间：undo页 用户表空间1、每个表的数据和索引都会存在自已的表空间中2、undo空间：undo页 （需要设置） 重做日志文件和归档文件 默认情况下，在InnoDB存储引擎的数据目录下会有两个名为ib_logfile0和ib_logfile1的文件，这就是InnoDB的重做日志文件(redo log file)，它记录了对于InnoDB存储引擎的事务日志。 当InnoDB的数据存储文件发生错误时，重做日志文件就能派上用场。InnoDB存储引擎可以使用重做日志文件将数据恢复为正确状态，以此来保证数据的正确性和完整性。 每个InnoDB存储引擎至少有1个重做日志文件组(group)，每个文件组下至少有2个重做日志文件，如默认的ib_logfile0和ib_logfile1。 为了得到更高的可靠性，用户可以设置多个镜像日志组，将不同的文件组放在不同的磁盘上，以此来提高重做日志的高可用性。 在日志组中每个重做日志文件的大小一致，并以【循环写入】的方式运行。InnoDB存储引擎先写入重做日志文件1，当文件被写满时，会切换到重做日志文件2，再当重做日志文件2也被写满时，再切换到重做日志文件1。 用户可以使用innodb_log_file_size来设置重做日志文件的大小，这对InnoDB存储引擎的性能有着非常大的影响。 如果重做日志文件设置的太大，数据丢失时，恢复时可能需要很长的时间；另一方面，如果设置的太小，重做日志文件太小会导致依据checkpoint的检查需要频繁刷新脏页到磁盘中，导致性能的抖动。 重做日志的落盘机制InnoDB对于数据文件和日志文件的刷盘遵守WAL(Write ahead redo log) 和Force-log-at-commit两种规则，二者保证了事务的持久性。WAL要求数据的变更写入到磁盘前，首先必须将内存中的日志写入到磁盘；Force-log-at-commit要求当一 个事务提交时，所有产生的日志都必须刷新到磁盘上，如果日志刷新成功后，缓冲池中的数据刷新到磁盘前数据库发生了宕机，那么重启时，数据库可以从日志中恢复数据。 如上图所示，InnoDB在缓冲池中变更数据时，会首先将相关变更写入重做日志缓冲中，然后再按时或者当事务提交时写入磁盘，这符合Force-log-at-commit原则；当重做日志写入磁盘后，缓冲池中的变更数据才会依据checkpoint机制择时写入到磁盘中，这符合WAL原则。 在checkpoint择时机制中，就有重做日志文件写满的判断，所以，如前文所述，如果重做日志文件太小，经常被写满，就会频繁导致checkpoint将更改的数据写入磁盘，导致性能抖动。 操作系统的文件系统是带有缓存的，当InnoDB向磁盘写入数据时，有可能只是写入到了文件系统的缓存中，没有真正的“落袋为安”。 InnoDB的innodb_flush_log_at_trx_commit属性可以控制每次事务提交时InnoDB的行为。当属性值为0时，事务提交时，不会对重做日志进行写入操作，而是等待主线程按时写入；当属性值为1时，事务提交时，会将重做日志写入文件系统缓存，并且调用文件系统的fsync，将文件系统缓冲中的数据真正写入磁盘存储，确保不会出现数据丢失；当属性值为2时，事务提交时，也会将日志文件写入文件系统缓存，但是不会调用fsync，而是让文件系统自己去判断何时将缓存写入磁盘。 日志的刷盘机制如下图所示： innodb_flush_log_at_commit是InnoDB性能调优的一个基础参数，涉及InnoDB的写入效率和数据安全。 当参数值为0时，写入效率最高，但是数据安全最低； 参数值为1时，写入效率最低，但是数据安全最高； 参数值为2时，二者都是中等水平。 一般建议将该属性值设置为1，以获得较高的数据安全性，而且也只有设置为1，才能保证事务的持久性。","categories":[],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://liudong-code.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"keywords":[]},{"title":"缓存","slug":"缓存","date":"2020-03-05T08:53:57.000Z","updated":"2020-03-06T09:17:19.621Z","comments":true,"path":"2020/03/05/缓存/","link":"","permalink":"https://liudong-code.github.io/2020/03/05/%E7%BC%93%E5%AD%98/","excerpt":"","text":"一、什么是缓存？ 缓存就是数据交换的缓冲区（称作：Cache），当某一硬件要读取数据时，会首先从缓存汇总查询数据，有则直接执行，不存在时从内存中获取。由于缓存的数据比内存快的多，所以缓存的作用就是帮助硬件更快的运行。 缓存往往使用的是RAM（断电既掉的非永久存储），所以在用完后还是会把文件送到硬盘等存储器中永久存储。电脑中最大缓存就是内存条，硬盘上也有16M或者32M的缓存。 高速缓存是用来协调CPU与主存之间存取速度的差异而设置的。一般CPU工作速度高，但内存的工作速度相对较低，为了解决这个问题，通常使用高速缓存，高速缓存的存取速度介于CPU与主存之间。系统将一些CPU在最近几个时间段经常访问的内容存在高速缓存，这样就在一定程度上缓解了由于主存速度低造成的CPU“停工待料”的情况。 缓存就是把一些外存上的数据保存在内存上而已，为什么保存在内存上，我们运行的所有程序里面的变量都是存放在内存中的，所以如果想将值放入内存上，可以通过变量的方式存储。在JAVA中一些缓存一般都是通过Map集合来实现的。 缓存在不同的场景下，作用是不一样的具体举例说明： 操作系统磁盘缓存 ——&gt; 减少磁盘机械操作。 数据库缓存——&gt;减少文件系统IO。 应用程序缓存——&gt;减少对数据库的查询。 Web服务器缓存——&gt;减少应用服务器请求。 客户端浏览器缓存——&gt;减少对网站的访问。 二、常见的缓存策略有哪些，如何做到缓存(比如redis)与DB里的数据一致性，你们项目中用到了什么缓存系统，如何设计的。 【1】由于不同系统的数据访问模式不同，同一种缓存策略很难在不同的数据访问模式下取得满意的性能，研究人员提出不同缓存策略以适应不同的需求。缓存策略的分类： 1）、基于访问的时间：此类算法按各缓存项被访问时间来组织缓存队列，决定替换对象。如 LRU； 2）、基于访问频率：此类算法用缓存项的被访问频率来组织缓存。如 LFU、LRU2、2Q、LIRS； 3）、访问时间与频率兼顾：通过兼顾访问时间和频率。使得数据模式在变化时缓存策略仍有较好性能。如 FBR、LRUF、ALRFU。多数此类算法具有一个可调或自适应参数，通过该参数的调节使缓存策略在基于访问时间与频率间取得一个平衡； 4）、基于访问模式：某些应用有较明确的数据访问特点，进而产生与其相适应的缓存策略。如专用的 VoD 系统设计的A&amp;L缓存策略，同时适应随机、顺序两种访问模式的 SARC策略； 【2】、数据不一致性产生的原因： 1）、先操作缓存，再写数据库成功之前，如果有读请求发生，可能导致旧数据入缓存，引发数据不一致。在分布式环境下，数据的读写都是并发的，一个服务多机器部署，对同一个数据进行读写，在数据库层面并不能保证完成顺序，就有可能后读的操作先完成（读取到的是脏数据），如果不采用给缓存设置过期时间策略，该数据永远都是脏数据。【解决办法】： 可采用更新前后双删除缓存策略； 可以通过“串行化”解决，保证同一个数据的读写落在同一个后端服务上； 2）、先操作数据库，再清除缓存。如果删缓存失败了，就会出现数据不一致问题。【解决办法】：①、将删除失败的 key 值存入队列中重复删除 （1）更新数据库数据。 （2）缓存因为种种问题删除失败。 （3）将需要删除的key发送至消息队列。 （4）自己消费消息，获得需要删除的key。 （5）继续重试删除操作，直到成功。 缺点：对业务线代码造成大量的侵入。于是有了方案二。 ②、方案二：通过订阅 binlog 获取需要重新删除的 Key 值数据。在应用程序中，另起一段程序，获得这个订阅程序传来的消息，进行删除缓存操作 ​ （1）更新数据库数据​ （2）数据库会将操作信息写入binlog日志当中​ （3）订阅程序提取出所需要的数据以及key​ （4）另起一段非业务代码，获得该信息​ （5）尝试删除缓存操作，发现删除失败​ （6）将这些信息发送至消息队列​ （7）重新从消息队列中获得该数据，重试操作。 三、如何防止缓存穿透、缓存击穿、缓存雪崩和缓存刷新。 【1】、缓存穿透：缓存穿透是说收到一个请求，但是该请求缓存中不存在，只能去数据库中查询，然后放进缓存。但当有好多请求同时访问同一个数据时，业务系统把这些请求全发到了数据库；或者恶意构造一个逻辑上不存在的数据，然后大量发送这个请求，这样每次都会被发送到数据库，最总导致数据库挂掉。 【解决的办法】：对于恶意访问，一种思路是先做校验，对恶意数据直接过滤掉，不要发送至数据库层；第二种思路是缓存空结果，就是对查询不存在的数据也记录在缓存中，这样就可以有效的减少查询数据库的次数。非恶意访问，结合缓存击穿说明。 【2】、缓存击穿：上面提到的某个数据没有，然后好多请求查询数据库，可以归为缓存击穿的范畴：对于热点数据，当缓存失效的一瞬间，所有的请求都被下放到数据库去请求更新缓存，数据库被压垮。 【解决的办法】：防范此类问题，一种思路是加全局锁，就是所有访问某个数据的请求都共享一个锁，获得锁的那个才有资格去访问数据库，其他线程必须等待。但现在大部分系统都是分布式的，本地锁无法控制其他服务器也等待，所以要用到全局锁，比如Redis的setnx实现全局锁。另一种思想是对即将过期的数据进行主动刷新，比如新起一个线程轮询数据，或者比如把所有的数据划分为不同的缓存区间，定期分区间刷新数据。第二个思路与缓存雪崩有点关系。 【3】、缓存雪崩：缓存雪崩是指当我们给所有的缓存设置了同样的过期时间，当某一时刻，整个缓存的数据全部过期了，然后瞬间所有的请求都被抛向了数据库，数据库就崩掉了。 【解决的办法】：解决思路要么是分治，划分更小的缓存区间，按区间过期；要么给每个key的过期时间加一个随机值，避免同时过期，达到错峰刷新缓存的目的。 对于 Redis 挂掉了，请求全部走数据库，也属于缓存雪崩，我们可以有以下思路进行解决： 事发前：实现 Redis 的高可用（主从架构+Sentinel 或者 Redis Cluster），尽可能避免 Redis 挂掉这种情况。 事发中：万一 Redis 真的挂了，我们可以设置本地缓存（ehcache）+ 限流（hystrix），尽量避免我们的数据库被干掉。 事发后：Redis 持久化，重启后自动从磁盘上加载数据，快速恢复缓存数据。 四、Redis内存用完会发生什么？ 如果达到设置的上限，Redis 的写命令会返回错误信息（但是读命令还是可以正常返回），或者将 Redis 当缓存使用，配置缓存淘汰机制，当 Redis 达到内存的上线时会冲掉旧的数据。 五、Redis 的 List 结构相关的操作 【1】、PUSH操作：是从队列头部和尾部增加节点的操作。 RPUSH KEY VALUE [VALUE …] ：从队列的右端入队一个或者多个数据，如果key值不存在，会自动创建一个空的列表。如果对应的key不是一个List，则会返回一个错误。 LPUSH KEY VALUE [VALUE…] ：从队列的左边入队一个或多个元素。复杂度O(1)。 RPUSHX KEY VALUE：从队列的右边入队一个元素，仅队列存在时有效，当队列不存在时，不进行任何操作。 LPUSHX KEY VALUE：从队列的左边入队一个元素，仅队列存在时有效。当队列不存在时，不进行任何操作。 【2】、POP操作：获取并删除头尾节点的操作。 LPOP KEY：从队列左边出队一个元素，复杂度O(1)。如果list为空，则返回nil。 RPOP KEY：从队列的右边出队一个元素，复杂度O(1)。如果list为空，则返回nil。 BLPOP KEY[KEY…] TIMEOUT：删除&amp;获取KEY中最左边的第一个元素，当队列为空时，阻塞TIMEOUT时间，单位是秒（这个时间内尝试获取KEY中的数据），超过TIMEOUT后如果仍未数据则返回(nil)。 123451 redis&gt; BLPOP queue 12 (nil)3 (1.10s) BRPOP KEY[KEY…] TIMEOUT：删除&amp;获取KEY中最后一个元素，或阻塞TIMEOUT。如上↑ 【4】、其他 LLEN KEY：获取队列（List）的长度。 LRANG KEY START STOP：从列表中获取指定（START-STOP）长度的元素。负数表示从右向左数。需要注意的是，超出范围的下标不会产生错误：如果start&gt;end，会得到空列表，如果end超过队尾，则Redis会将其当做列表的最后一个元素。 12345678910111213141516171819 1 redis&gt; rpush q1 a b c d f e g 2 (integer) 7 3 redis&gt; lrange q1 0 -1 4 1) \"a\" 5 2) \"b\" 6 3) \"c\" 7 4) \"d\" 8 5) \"f\" 9 6) \"e\"10 7) \"g\" LINDEX KEY INDEX：获取一个元素，通过其索引列表。我们之前介绍的操作都是对list的两端进行的，所以算法复杂度都只有O(1)。而这个操作是指定位置来进行的，每次操作，list都得找到对应的位置，因此算法复杂度为O(N)。list的下表是从0开始的，index为负的时候是从右向左数。-1表示最后一个元素。当下标超出的时候，会返回nul。所以不用像操作数组一样担心范围越界的情况。 LSET KEY INDEX：重置队列中INDEX位置的值。当index越界的时候，这里会报异常。 LREM KEY COUNT VALUE：从列表中删除COUNT个VALUE元素。COUNT参数有三种情况： ​ count &gt; 0: 表示从头向尾（左到右）移除值为value的元素。​ count &lt; 0: 表示从尾向头（右向左）移除值为value的元素。​ count = 0: 表示移除所有值为value的元素。 LTRIM KEY START STOP：修剪到指定范围内的清单，相当与截取，只保留START-STOP之间的数据。 12345678910111213141516171 redis&gt; rpush q a b c d e f g 2 (integer) 7 3 redis&gt; lrange q 0 -1 4 1) \"a\" 5 2) \"b\" 6 3) \"c\" 7 4) \"d\" 8 5) \"e\" 9 6) \"f\"10 7) \"g\"11 redis&gt; ltrim q 1 412 OK13 redis&gt; lrange q 0 -114 1) \"b\"15 2) \"c\"16 3) \"d\"17 4) \"e\" LINSERT KEY BEFORE|AFTER 元素 VALUE：在列表中的另一个元素之前或之后插入VAULE。当 key 不存在时，这个List被视为空列表，任何操作都不会发生。当key存在，但保存的不是 List，则会报 error。该命令会返回修改之后的 List的长度，如果找不到元素，则会返回 -1。 六、Redis的数据结构都有哪些【1】、String：可以是字符串，整数或者浮点数，对整个字符串或者字符串中的一部分执行操作，对整个整数或者浮点执行自增(increment)或者自减(decrement)操作。【2】、List：一个链表，链表上的每个节点都包含了一个字符串，链表的两端推入或者弹出元素，根据偏移量对链表进行修剪(trim)，读取单个或者多个元素，根据值查找或者移除元素。可参考5【3】、Set：包含字符串的无序收集器(unordered collection)、并且被包含的每个字符串都是独一无二的。添加，获取，移除单个元素，检查一个元素是否存在于集合中，计算交集（sinter），并集（suion），差集（sdiff），从集合里面随机获取元素。【4】、SortSet：是一个排好序的 Set，它在 Set 的基础上增加了一个顺序属性 score，这个属性在添加修改元素时可以指定，每次指定后，SortSet 会自动重新按新的值排序。sorted set 的内部使用 HashMap 和跳跃表(SkipList)来保证数据的存储和有序，HashMap 里放的是成员到 score 的映射，而跳跃表里存放的是所有的成员，排序依据是 HashMap 里存的 score。 123456789101112131415161718192021 192.168.2.124:6379&gt; zadd myzset 1 \"one\" 2 \"two\" 3 \"three\" #添加元素(integer) 3192.168.2.129:6379&gt; zrange myzset 0 -11) \"one\"2) \"two\"3) \"three\"192.168.2.129:6379&gt; zrange myzset 0 -1 withscores1) \"one\"2) \"1\"3) \"two\"4) \"2\"5) \"three\"6) \"3\"192.168.2.129:6379&gt; zrem myzset one //删除元素(integer) 1192.168.2.129:6379&gt; zrange myzset 0 -1 withscores1) \"two\"2) \"2\"3) \"three\"4) \"3\"192.168.2.124:6379 【 5】、hash：Hash 是一个 String 类型的 field 和 value 之间的映射表，即 redis 的 Hash 数据类型的 key（hash表名称）对应的 value 实际的内部存储结构为一个 HashMap，因此 Hash 特别适合存储对象。相对于把一个对象的每个属性存储为 String 类型，将整个对象存储在 Hash 类型中会占用更少内存。 123456789192.168.2.124:6379&gt; hset myhash name zhangsan(integer) 1192.168.2.124:6379&gt; hset myhash age 20(integer) 1192.168.2.124:6379&gt; hget myhash name\"zhangsan\"192.168.2.124:6379&gt; hget myhash age\"20\"192.168.2.124:6379&gt; 七、Redis 的使用要注意什么，讲讲持久化方式，内存设置，集群的应用和优劣势，淘汰策略等。使用阶段我们从数据存储和数据获取两个方面来说明开发时的注意事项：【1】数据存储：因为内存空间的局限性，注定了能存储的数据量有限，如何在有限的空间内存储更多的数据信息是我们应该关注的。Redis内存储的都是键值对，那么如何减小键值对所占据的内存空间就是空间优化的本质。在能清晰表达业务含义的基础上尽可能缩减Key的字符长度，比如一个键是user:{id}:logintime ，可以使用业务属性的简写来u:{id}:lgt,只要能清晰表达业务意义，使用简写形式是有其必要性的。在不影响使用的情况下，缩减Value的数据大小。如果Value是较大的数据信息，比如图片，大文本等，可以使用压缩工具压缩过后再存入Redis；如果Value是对象序列化或者gson信息，可以考虑去除非必要的业务属性。减少键值对的数量，对于大量的String类型的小对象，可以尝试使用Hash的形式组合他们，在Hash对象内Field数量少于1000，且Value的字符长度小于40时，内部使用ziplist的编码形式，能够极大的降低小对象占据的内存空间。Redis内维护了一个[0-9999]的整数对象池，类似Java内的运行时常量池，只创建一个常量，使用时都去引用这个常量，所以当存储的value是这个范围内的数字时均是引用向都一个内存地址，所以能够降低一些内存空间耗费。但是共享对象池和maxmemory+LRU的内存回收策略冲突，因为共享Value对象的lru值也共享，难以通过lru知道哪个Key的最后引用时间，所以永远也不能回收内存。如果多次数据操作要求原子性，可使用Multi来实现Redis的事务。【2】数据查询：Redis 是一种数据库，和其他数据库一样，操作时也需要有连接对象，连接对象的创建和销毁也需要耗费资源，复用连接对象很有必要，所以推荐使用连接池来管理连接。Redis数据存储在内存中，查询很快，但不代表连接也很快。一次Redis查询可能IO部分占据了请求时间的绝大部分比例，缩短IO时间是开发过程中很需要注意的一点。对于一个业务内的多次查询，考虑使用Pipeline，将多次查询合并为一次查询，命令会被执行多次，但是只有一个IO传输，能够有效的提高响应速度。对于多次String类型的查询，使用mget，将多次请求合并为一次，同时命令和会被合并为一次，能有效提高响应速度，对于Hash内多个Field查询，使用hmget，起到和mget同样的效果。Redis是单线程执行的，也就是说同一时间只能执行一条命令，如果一条命令执行的时间较长，其他线程在此期间均会被阻塞，所以在操作Redis时要注意操作指令的涉及的数据量，尽量降低单次操作的执行时间。【持久化方式】：RDB 时间点快照 AOF 记录服务器执行的所有写操作命令，并在服务器启动时，通过重新执行这些命令来还原数据集。可参考14深度解析【内存设置】：maxmemory used_memory【虚拟内存】： vm-enabled yes【集群的应用和优劣势】：参考9【内存优化】：http://www.infoq.com/cn/articles/tq-redis-memory-usage-optimization-storage【淘汰策略】：http://wiki.jikexueyuan.com/project/redis/data-elimination-mechanism.html Redis 提供 6 种数据淘汰策略： volatile-lru：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用 的数据淘汰 volatile-ttl：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数 据淘汰 volatile-random：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据 淘汰 allkeys-lru：从数据集（server.db[i].dict）中挑选最近最少使用的数据淘汰 allkeys-random：从数据集（server.db[i].dict）中任意选择数据淘汰 no-enviction（驱逐）：禁止驱逐数据 八、Redis2 和 Redis3 的区别，Redis3 内部通讯机制集群方式的区别：Redis3 采用Cluster，Redis2 采用客户端分区方案和代理方案；通信过程说明： 1） 集群中的每个节点都会单独开辟一个TCP通道， 用于节点之间彼此通信， 通信端口号在基础端口上加10000。 2） 每个节点在固定周期内通过特定规则选择几个节点发送 ping 消息。 3） 接收到 ping 消息的节点用 pong 消息作为响应。 九、当前 Redis 集群有哪些玩法，各自优缺点，场景。【1】数据共享：Redis 提供多个节点实例间的数据共享，也就是 Redis A,B,C,D彼此之间的数据是同步的，同样彼此之间也可以通信，而对于客户端操作的 keys 是由 Redis 系统自行分配到各个节点中。【2】主从复制：Redis 的多个实例间通信时，一旦其中的一个节点故障，那么 Redis 集群就不能继续正常工作，所以需要一种复制机制（Master-Slave）机制，做到一旦节点A故障了，那么其从节点A1和A2就可以接管并继续提供与A同样的工作服务，当然如果节点A,A1,A2节点都出现问题，那么同样这个集群不会继续保持工作，但是这种情况比较罕见，即使出现了，也会及时发现并修复使用。建议：部署主从复制机制（Master-Slave）。【3】哈希槽值：Redis 集群中使用哈希槽来存储客户端的 keys，而在 Redis 中，目前存在16384个哈希槽，它们被全部分配给所有的节点，正如上图所示，所有的哈希槽值被节点A，B，C分配完成了。参考：https://www.cnblogs.com/RENQIWEI1995/p/8931678.html 十、Memcache 的原理，哪些数据适合放在缓存中。 首先要说明一点，MemCache 的数据存放在内存中，存放在内存中个人认为意味着几点： 【1】访问数据的速度比传统的关系型数据库要快，因为 Oracle、MySQL 这些传统的关系型数据库为了保持数据的持久性，数据存放在硬盘中，IO操作速度慢； 【2】MemCache 的数据存放在内存中同时意味着只要 MemCache 重启了，数据就会消失； 【3】既然 MemCache 的数据存放在内存中，那么势必受到机器位数的限制，这个之前的文章写过很多次了，32位机器最多只能使用2GB的内存空间，64位机器可以认为没有上限。 然后我们来看一下 MemCache 的原理，MemCache 最重要的莫不是内存分配的内容了，MemCache 采用的内存分配方式是固定空间分配，一张图说明： 这张图片里面涉及了slab_class、slab、page、chunk四个概念，它们之间的关系是：【1】、MemCache将内存空间分为一组slab【2】、每个slab下又有若干个page，每个page默认是1M，如果一个slab占用100M内存的话，那么这个slab下应该有100个page【3】、每个page里面包含一组chunk，chunk是真正存放数据的地方，同一个slab里面的chunk的大小是固定的【4】、有相同大小chunk的slab被组织在一起，称为slab_classMemCache内存分配的方式称为allocator，slab的数量是有限的，几个、十几个或者几十个，这个和启动参数的配置相关。MemCache中的value过来存放的地方是由value的大小决定的，value总是会被存放到与chunk大小最接近的一个slab中，比如slab[1]的chunk大小为80字节、slab[2]的chunk大小为100字节、slab[3]的chunk大小为128字节（相邻slab内的chunk基本以1.25为比例进行增长，MemCache启动时可以用-f指定这个比例），那么过来一个88字节的value，这个value将被放到2号slab中。放slab的时候，首先slab要申请内存，申请内存是以page为单位的，所以在放入第一个数据的时候，无论大小为多少，都会有1M大小的page被分配给该slab。申请到page后，slab会将这个page的内存按chunk的大小进行切分，这样就变成了一个chunk数组，最后从这个chunk数组中选择一个用于存储数据。如果这个slab中没有chunk可以分配了怎么办，如果MemCache启动没有追加-M（禁止LRU，这种情况下内存不够会报Out Of Memory错误），那么MemCache会把这个slab中最近最少使用的chunk中的数据清理掉，然后放上最新的数据。针对MemCache的内存分配及回收算法，总结三点： 【1】、MemCache的内存分配chunk里面会有内存浪费，88字节的value分配在128字节（紧接着大的用）的chunk中，就损失了30字节，但是这也避免了管理内存碎片的问题 【2】、MemCache的LRU算法不是针对全局的，是针对slab的 【3】、应该可以理解为什么MemCache存放的value大小是限制的，因为一个新数据过来，slab会先以page为单位申请一块内存，申请的内存最多就只有1M，所以value大小自然不能大于1M了。 再总结 MemCache 的特性和限制： 上面已经对于MemCache做了一个比较详细的解读，这里再次总结MemCache的限制和特性：1】、MemCache中可以保存的item数据量是没有限制的，只要内存足够2】、MemCache单进程在32位机中最大使用内存为2G，这个之前的文章提了多次了，64位机则没有限制3】、Key最大为250个字节，超过该长度无法存储4】、单个item最大数据是1MB，超过1MB的数据不予存储5】、MemCache服务端是不安全的，比如已知某个MemCache节点，可以直接telnet过去，并通过flush_all让已经存在的键值对立即失效6】、不能够遍历MemCache中所有的item，因为这个操作的速度相对缓慢且会阻塞其他的操作7】、MemCache的高性能源自于两阶段哈希结构：第一阶段在客户端，通过Hash算法根据Key值算出一个节点；第二阶段在服务端，通过一个内部的Hash算法，查找真正的item并返回给客户端。从实现的角度看，MemCache是一个非阻塞的、基于事件的服务器程序8】、MemCache设置添加某一个Key值的时候，传入expiry为0表示这个Key值永久有效，这个Key值也会在30天之后失效。 MemCache适合存储： 变化频繁，具有不稳定性的数据,不需要实时入库, (比如用户在线状态、在线人数..)门户网站的新闻等，觉得页面静态化仍不能满足要求，可以放入到memcache中.(配合jquey的ajax请求)。 十一、Redis 和 Memcached 的内存管理的区别可参考博客1：http://lib.csdn.net/article/redis/55323可参考博客2：https://www.cnblogs.com/work115/p/5584646.html 十二、Redis 的并发竞争问题如何解决，了解 Redis 事务的 CAS 操作吗？Redis 为单进程单线程模式，采用队列模式将并发访问变为串行访问。Redis 本身没有锁的概念，Redis 对于多个客户端连接并不存在竞争，但是在 Jedis 客户端对 Redis 进行并发访问时会发生连接超时、数据转换错误、阻塞、客户端关闭连接等问题，这些问题均是由于客户端连接混乱造成。对此有2种解决方法：【1】客户端角度，为保证每个客户端间正常有序与 Redis 进行通信，对连接进行池化，同时对客户端读写 Redis 操作采用内部锁 synchronized。【2】服务器角度，利用 setnx 实现锁 MULTI，EXEC，DISCARD，WATCH 四个命令是 Redis 事务的四个基础命令。其中： ☆ MULTI，告诉 Redis 服务器开启一个事务。注意，只是开启，而不是执行 ☆ EXEC，告诉 Redis 开始执行事务 ☆ DISCARD，告诉 Redis 取消事务 ☆ WATCH，监视某一个键值对，它的作用是在事务执行之前如果监视的键值被修改，事务会被取消。 【Redis 事务机制】：https://www.jianshu.com/p/d777eb9f27df【CAS 操作】：https://www.jianshu.com/p/d777eb9f27df 十三、Redis 的选举算法和流程是怎样的Raft 采用心跳机制触发 Leader 选举。系统启动后，全部节点初始化为 Follower，term 为0。节点如果收到了 RequestVote 或者AppendEntries，就会保持自己的 Follower 身份。如果一段时间内没收到 AppendEntries 消息直到选举超时，说明在该节点的超时时间内还没发现 Leader，Follower 就会转换成 Candidate，自己开始竞选 Leader。一旦转化为 Candidate，该节点立即开始下面几件事情： 1）、增加自己的term。 2）、启动一个新的定时器。 3）、给自己投一票。 4）、向所有其他节点发送RequestVote，并等待其他节点的回复。✔ 如果在这过程中收到了其他节点发送的AppendEntries，就说明已经有Leader产生，自己就转换成Follower，选举结束。✔ 如果在计时器超时前，节点收到多数节点的同意投票，就转换成Leader。同时向所有其他节点发送AppendEntries，告知自己成为了Leader。✔ 每个节点在一个term内只能投一票，采取先到先得的策略，Candidate前面说到已经投给了自己，Follower会投给第一个收到RequestVote的节点。每个Follower有一个计时器，在计时器超时时仍然没有接受到来自Leader的心跳RPC, 则自己转换为Candidate, 开始请求投票，就是上面的的竞选Leader步骤。✔ 如果多个Candidate发起投票，每个Candidate都没拿到多数的投票（Split Vote），那么就会等到计时器超时后重新成为Candidate，重复前面竞选Leader步骤。✔ Raft协议的定时器采取随机超时时间，这是选举Leader的关键。每个节点定时器的超时时间随机设置，随机选取配置时间的1倍到2倍之间。由于随机配置，所以各个Follower同时转成Candidate的时间一般不一样，在同一个term内，先转为Candidate的节点会先发起投票，从而获得多数票。多个节点同时转换为Candidate的可能性很小。即使几个Candidate同时发起投票，在该term内有几个节点获得一样高的票数，只是这个term无法选出Leader。由于各个节点定时器的超时时间随机生成，那么最先进入下一个term的节点，将更有机会成为Leader。连续多次发生在一个term内节点获得一样高票数在理论上几率很小，实际上可以认为完全不可能发生。一般1-2个term类，Leader就会被选出来。 【Sentinel 的选举流程】：Sentinel 集群正常运行的时候每个节点 epoch 相同，当需要故障转移的时候会在集群中选出 Leader执行故障转移操作。Sentinel采 用了Raft 协议实现了 Sentinel 间选举 Leader 的算法，不过也不完全跟论文描述的步骤一致。Sentinel 集群运行过程中故障转移完成，所有 Sentinel 又会恢复平等。Leader 仅仅是故障转移操作出现的角色。 【选举流程】：1）、某个 Sentinel 认定 master 客观下线的节点后，该 Sentinel 会先看看自己有没有投过票，如果自己已经投过票给其他 Sentinel 了，在2倍故障转移的超时时间自己就不会成为 Leader。相当于它是一个 Follower。 2）、如果该 Sentinel 还没投过票，那么它就成为 Candidate。 3）、和 Raft 协议描述的一样，成为 Candidate，Sentinel 需要完成几件事情。 【1】更新故障转移状态为start 【2】当前epoch加1，相当于进入一个新term，在Sentinel中epoch就是Raft协议中的term。 【3】更新自己的超时时间为当前时间随机加上一段时间，随机时间为1s内的随机毫秒数。 【4】向其他节点发送is-master-down-by-addr命令请求投票。命令会带上自己的epoch。 【5】给自己投一票，在 Sentinel 中，投票的方式是把自己 master 结构体里的 leader 和 leader_epoch 改成投给的 Sentinel 和它的 epoch。 4）、其他Sentinel会收到Candidate的is-master-down-by-addr命令。如果Sentinel当前epoch和Candidate传给他的epoch一样，说明他已经把自己master结构体里的leader和leader_epoch改成其他Candidate，相当于把票投给了其他Candidate。投过票给别的Sentinel后，在当前epoch内自己就只能成为Follower。 5）、Candidate会不断的统计自己的票数，直到他发现认同他成为Leader的票数超过一半而且超过它配置的quorum（quorum可以参考《redis sentinel设计与实现》）。Sentinel比Raft协议增加了quorum，这样一个Sentinel能否当选Leader还取决于它配置的quorum。 6）、如果在一个选举时间内，Candidate没有获得超过一半且超过它配置的quorum的票数，自己的这次选举就失败了。 7）、如果在一个epoch内，没有一个Candidate获得更多的票数。那么等待超过2倍故障转移的超时时间后，Candidate增加epoch重新投票。 8）、如果某个Candidate获得超过一半且超过它配置的quorum的票数，那么它就成为了Leader。 9）、与Raft协议不同，Leader并不会把自己成为Leader的消息发给其他Sentinel。其他Sentinel等待Leader从slave选出master后，检测到新的master正常工作后，就会去掉客观下线的标识，从而不需要进入故障转移流程。 十四、Redis 的持久化的机制，AOF和RDB的区别。Redis的持久化机制：Redis 提供两种方式进行持久化，一种是RDB持久化（原理是将Reids在内存中的数据库记录定时dump到磁盘上的RDB持久化），另外一种是AOF（append only file）持久化（原理是将Reids的操作日志以追加的方式写入文件）。 AOF和RDB的区别：RDB持久化是指在指定的时间间隔内将内存中的数据集快照写入磁盘，实际操作过程是fork一个子进程，先将数据集写入临时文件，写入成功后，再替换之前的文件，用二进制压缩存储。 AOF持久化以日志的形式记录服务器所处理的每一个写、删除操作，查询操作不会记录，以文本的方式记录，可以打开文件看到详细的操作记录。 【二者优缺点】：RDB存在哪些优势：【1】一旦采用该方式，那么你的整个Redis数据库将只包含一个文件，这对于文件备份而言是非常完美的。比如，你可能打算每个小时归档一次最近24小时的数据，同时还要每天归档一次最近30天的数据。通过这样的备份策略，一旦系统出现灾难性故障，我们可以非常容易的进行恢复。【2】对于灾难恢复而言，RDB是非常不错的选择。因为我们可以非常轻松的将一个单独的文件压缩后再转移到其它存储介质上。【3】性能最大化。对于Redis的服务进程而言，在开始持久化时，它唯一需要做的只是fork出子进程，之后再由子进程完成这些持久化的工作，这样就可以极大的避免服务进程执行IO操作了。【4】相比于AOF机制，如果数据集很大，RDB的启动效率会更高。 【RDB又存在哪些劣势】：【1】如果你想保证数据的高可用性，即最大限度的避免数据丢失，那么RDB将不是一个很好的选择。因为系统一旦在定时持久化之前出现宕机现象，此前没有来得及写入磁盘的数据都将丢失。【2】由于RDB是通过fork子进程来协助完成数据持久化工作的，因此，如果当数据集较大时，可能会导致整个服务器停止服务几百毫秒，甚至是1秒钟。 【AOF 的优势有哪些】：【1】该机制可以带来更高的数据安全性，即数据持久性。Redis中提供了3中同步策略，即每秒同步、每修改同步和不同步。事实上，每秒同步也是异步完成的，其效率也是非常高的，所差的是一旦系统出现宕机现象，那么这一秒钟之内修改的数据将会丢失。而每修改同步，我们可以将其视为同步持久化，即每次发生的数据变化都会被立即记录到磁盘中。可以预见，这种方式在效率上是最低的。至于无同步，无需多言，我想大家都能正确的理解它。【2】由于该机制对日志文件的写入操作采用的是append模式，因此在写入过程中即使出现宕机现象，也不会破坏日志文件中已经存在的内容。然而如果我们本次操作只是写入了一半数据就出现了系统崩溃问题，不用担心，在Redis下一次启动之前，我们可以通过redis-check-aof工具来帮助我们解决数据一致性的问题。【3】如果日志过大，Redis可以自动启用rewrite机制。即Redis以append模式不断的将修改数据写入到老的磁盘文件中，同时Redis还会创建一个新的文件用于记录此期间有哪些修改命令被执行。因此在进行rewrite切换时可以更好的保证数据安全性。【4】AOF包含一个格式清晰、易于理解的日志文件用于记录所有的修改操作。事实上，我们也可以通过该文件完成数据的重建。 【AOF 的劣势有哪些】：【1】对于相同数量的数据集而言，AOF文件通常要大于RDB文件。RDB 在恢复大数据集时的速度比 AOF 的恢复速度要快。【2】根据同步策略的不同，AOF在运行效率上往往会慢于RDB。总之，每秒同步策略的效率是比较高的，同步禁用策略的效率和RDB一样高效。 二者选择的标准，就是看系统是愿意牺牲一些性能，换取更高的缓存一致性（aof），还是愿意写操作频繁的时候，不启用备份来换取更高的性能，待手动运行save的时候，再做备份（rdb）。rdb这个就更有些 eventually consistent的意思了。 十五、缓存预热 新的缓存系统没有任何数据，在缓存重建数据的过程中，系统性能和数据负载都不太好，所以最好在系统上线之前就把缓存的热点数据加载到缓存中，这种缓存预加载手段就是缓存预热。 十六、缓存热备 缓存热备既当一个缓存服务器不可用时能实时切换到备用缓存服务器，不影响缓存使用。集群模式下，每个主节点都会有一个或多个从节点备用，一旦主节点挂掉，从节点会被哨兵提升为主节点使用。 十七、Redis 的集群怎么同步的数据的Redis 集群没有使用一致性hash, 而是引入了哈希槽的概念。 Reds 集群有16384个哈希槽,每个key通过CRC16校验后对16384取模来决定放置哪个槽.集群的每个节点负责一部分hash槽。这种结构很容易添加或者删除节点，并且无论是添加删除或者修改某一个节点，都不会造成集群不可用的状态。 使用哈希槽的好处就在于可以方便的添加或移除节点。 当需要增加节点时，只需要把其他节点的某些哈希槽挪到新节点就可以了； 当需要移除节点时，只需要把移除节点上的哈希槽挪到其他节点就行了； 在这一点上，我们以后新增或移除节点的时候不用先停掉所有的 redis 服务 Redis集群的主从架构： 为了使在部分节点失败或者大部分节点无法通信的情况下集群仍然可用，所以集群使用了主从复制模型,每个节点都会有N-1个复制品。 例如有A，B，C三个节点的集群,在没有复制模型的情况下,如果节点B失败了，那么整个集群就会以为缺少B节点所承担的哈希槽这个范围的槽而不可用。 然而如果在集群创建的时候（或者过一段时间）我们为每个节点添加一个从节点A1，B1，C1,那么整个集群便有三个master节点和三个slave节点组成，这样在节点B失败后，集群便会选举B1为新的主节点继续服务，整个集群便不会因为槽找不到而不可用了。当然如果B和B1都down了，那集群还是不可用的，不过这种情况微乎其妙，基本不用考虑，出发你交换机挂了吧，或者机房断电。 Redis 集群搭建的方式有很多种，但从 redis 3.0 版本之后，支持 redis-cluster 集群，它是 Redis 官方提供的解决方案，Redis Cluster 采用的是 无中心架构 ，每个节点保存数据和整个集群状态，每个节点都和其他节点有所连接。其架构如下： 客户端与 redis 节点直连，不需要中间件 proxy 层，客户端不需要连接集群所有节点，连接集群汇中任何一个节点即可。所有的 redis 节点彼此互联（PING-PONG 机制），内部使用二进制协议优化传输速度和带宽。 分布式存储机制-槽 【1】、redis_cluster 把所有的节点映射到 [0-16383] slot 槽上，cluster 负责维护 node&lt;-&gt;slot&lt;-&gt;value 三者之间的关系。【2】、Redis 集群中内置了 16384 个哈希槽，当需要在 Redis 集群中放置一个 key-value 时，redis 先将 key 使用 CRC16 算法算出一个结果，然后把结果对 16384 求余数，这样每个 key 都会对应一个编号 0-16383 之间的哈希槽，redis 会根据节点数量大致均等的将哈希槽映射到不同的节点上。 1234例如，当有三个节点时，槽分布的值如下：节点1： 0-5460节点2： 5461-10921节点3: 10922-16383 十八、知道哪些 Redis 的优化操作一、Linux 操作系统【1】ulimit 与 TCP backlog：1）、修改 ulimit：通过 ulimit 修改 open files 参数，redis 建议把 open files 至少设置成 10032，因为 maxclients 是10000 [客户端的数据是以文件的形式进行保存的] ，另外 redis 内部最多会使用 32 个文件描述符。 12345ulimit -n 10032 #但重启后就无效了，也可以通过配置文件limits.conf 的形式持久修改#修改了，重新登录后就立刻生效.可以用CentOS ulimit -a 查看确认[root@dev ~]# ulimit -a#... 省略open files (-n) 10032 2）、修改 TCP backlog：redis 默认的 tcp-backlog 为 511，可通过配置 tcp-backlog 进行调整，如果 Linux 的 tcp-backlog 小于 redis 的 tcp-backlog，日志里会出有 warning。此参数确定了 TCP 连接中已完成队列(完成三次握手之后)的长度， 当然此值必须小于或等于 Linux 系统定义的 [/proc/sys/net/core/somaxconn] 值，而 Linux 的默认参数值是128。当系统并发量大并且客户端速度缓慢的时候，可以将这二个参数一起参考设定 12345678#建议修改为 2048 修改somaxconn#该内核参数默认值一般是128，对于负载很大的服务程序来说大大的不够。一般会将它修改为2048或者更大。echo 2048 &gt; /proc/sys/net/core/somaxconn #但是这样系统重启后保存不了#持久化设置: 在 /etc/sysctl.conf 中添加如下:#net.core.somaxconn = 2048#然后在终端中执行:sysctl -p 【2】vm.overcommit_mermory：表示内核在分配内存时候做检查的方式。 1）、redis 建议将 vm.overcommit_memory 设置为1，防止极端情况下 fork 出错。 2）、vm.overcommit_memory 取值说明：Linux 对大多数申请内存的回复均为 YES，以运行更多程序，因为申请后并不是立马使用，该技术叫 vm.overcommit。 ■ 0：内核将检查是否有足够的内存，如果足够，申请通过，否则内存申请失败把错误返回给应用进程。 ■ 1：表示内核容许超量使用内存直到用完为止。 ■ 2：内存绝不过量使用内存，既系统整个内存空间不能超过 swap+50% 的 RAM[（random access memory）即随机存储内存 ]值，50% 是 overcommit_ratio 的默认值，支持修改。 1echo \"vm.overcommit_memory=1\" &gt; /etc/sysctl.conf 【3】swappiness 参数： 1）：swappiness 参数决定操作系统使用 swap 的倾向程度，取值范围是0~100，swappiness 的值越大，说明操作系统可能使用 swap 的概率越高，swappiness 值越低，表示操作系统更加倾向于使用物理内存。 2）、建议 Linux3.5 以上设置为1，否则建议设置为0。 1echo \"vm.swappiness=1\" &gt; /etc/sysctl.conf 【4】Transparent Huge Pages：支持大内存分页（2MB）分配，默认开启，redis 建议关闭此功能。 1sudo chkconfig --add disable-transparent-hugepages 【5】OOM killer：会在可用内存不足时选择性杀掉用户进程，OOM killer 会为每个用户进程设置一个权重，权重越大被 kill 的可能性越大。每个进程的权重放在 [/proc/{progress_id}/oom_adj]。对于 Redis 服务器来说，可以将所有 Redis 的 oom_adj 设置为最低值或者稍小的值，降低被 OOM killer 杀掉的概率。应该设置与进程有关，无法一次性设置。 二、Redis 关键参数【1】客户端最大连接数（maxclients）： 1）、现象：如果连接数不够，或者请求返回比较慢导致连接数不足，可能会报[ max number of clients reached ]。 2）、优化：调整 maxclients，或者优化 redis 命令处理性能。要注意该参数受到操作系统最大文件句柄的限制（ulimit -n ） 【2】repl-ping-slave-period/repl-timeout： 1）、说明：slave 会每隔 repl-ping-slave-period（默认10秒）ping 一次 master，如果查过 repl-timeout（默认 60秒）都没有收到响应，就会认为 Master 挂掉。 2）、优化：如果 Master 明明没挂掉但被阻塞住了也会报这个错。可以适当调大 repl-timeout 【3】client-output-buffer-limit： 1）说明：客户端输出缓冲区大小。 2）、当使用主从复制时，性能压测下，数据量会急剧增长，导致从节点需要复制的数据很大，消耗时长增加。slave 没挂但被阻塞住了，比如正在 loading Master 发过来的 RDB，Master 的指令不能立刻发送给 slave，就会放在 output-buffer 中，在配置文件中有如下配置： 1client-output-buffer-limit slave 256mb 64mb 60 上述配置说明：负责发送给 slave的 client，如果 buffer 超过 256m 或者连续 60秒超过 64m，就会被立刻强行关闭。所以此时应该相应调大数值，否则就会出现很悲剧的循环：Master 传输一个很大的 RDB 给 slave，slave 努力地装载，但是还没装载完，Master 对 client 的缓存存满了，关闭后再来一次。 三、Redis 性能测试 Redis 官网自动 Redis 性能测试工具 Redis-benchmark，可以有效的测试 Redis 服务的性能。 【1】案例一：命令如下，100个并发连接，100000个请求，检测host为127.0.0.1 端口为 6379 的 redis 服务器性能 12345678910111213141516./redis-benchmark -h 127.0.0.1 -p 6379 -c 100 -n 100000#对集合写入测试 结果如下100000 requests completed in 2.38 seconds #100000个请求在2.38秒内完成20 parallel clients #每次请求有20个并发客户端3 bytes payload #每次写入3个字节的数据keep alive: 1 #保持一个连接，一台服务器来处理这些请求93.06% &lt;= 15 milliseconds99.96% &lt;= 31 milliseconds99.98% &lt;= 46 milliseconds99.99% &lt;= 62 milliseconds100.00% &lt;= 62 milliseconds#所有请求在62毫秒内完成42105.26 requests per second#每秒处理42105.26次请求 【2】案例二：命令如下，测试指定操作命令的性能。 1./redis-benchmark -t set,lpush -n 100000 -q 四、查找慢查询语句 Redis 提供了记录耗时操作语句的功能，当语句执行（不包括命令排队时间）超过了阈值，则被认为是慢查询。 【1】参数设置：[ slowlog-log-slower-than ]：记录运行耗时语句的阈值，单位是微妙（1秒=1000毫秒=1000 000微妙，默认值：10000）。当值为0时，记录所有请求。当值&lt;0时，不记录任何请求。 [ slowlog-max-len ]：该参数用于设置慢查询保存的条数。 【2】功能使用：[ slowlog get ]：用于查询慢查询信息。[ slowlog len ]：显示当前 redis 有多少条慢查询 十九、Reids 的主从复制机制原理一、Redis 复制 复制（Replication）：是 Redis 实现高可用的基础。且在复制过程中，主节点/从节点都是非阻塞的，但是从节点在执行同步时使用的是旧数据集提供查询。 Redis 复制启动图与流程说明： 1）、当从节点连接到主节点时，会发送 psync 命令给主节点，runId 是主节点的 ID，如果没有默认是 -1；offset 是从节点保存复制偏移量，主节点根据复制偏移量仅发送从节点所需的增量部分，如果是第一次复制则为 -1； 2）、如果主节点回复 +FULLRESYNC，那么从节点将触发全量复制流程。 3）、如果主节点回复 +CONTINUE，那么从节点触发部分复制。 4）、如果主节点回复 +ERR，说名主节点不支持该命令。 二、Redis 主从全量复制 主从复制：主机数据更新后根据配置和策略，自动同步到备机的 master/slaver 机制，Master以写为主，Slave以读为主。 三、Redis 主从部分复制 四、Redis 主从搭建 【1】修改主节点 redis.conf 文件：使用[./redis-server redis.conf]启动主节点。 1234567891011121314bind 127.0.0.1Port 7000#是否开启保护模式，默认为 yes 是开启。要是配置里没有指定 bind 和密码。开启该参数后，redis只会本地进行访问，拒绝外部访问。#要是开启了密码和 bind，可以开启。否则最好关闭，设置为no。protected-mode no #修改 redis 安全密码requirepass \"123456789\"master 设置密码masterauth \"123456789\"#开启appendonly 模式后,redis 将每一次写操作请求都追加到appendonly.aof 文件中appendonly yes 【2】新建从节点并配置复制主节点信息（配从(库)不配主(库)）每次与 master 断开之后，都需要重新连接，除非你配置进 redis.conf 文件 info replication（Redis Sentinel 还使用该信息来发现 slave 实例） 123#slaveof 主库IP 主库端口slaveof 127.0.0.0 7000 【3】Redis 主/从复制注意点： 1）、主/从节点应启用持久化：master 和 slave 中应启用持久化。当由于性能要求不能启用持久化时，应配置实例避免自动重启，因为主节点未持久化又重启时，内存数据为空，会导致从节点同步主节点空数据。 2）、从节点应配置只读属性：主从复制中，从节点应配置只读属性[replica-read-only yes]。 3）、从节点应配置对主节点的验证：主节点通过 requirepass 配置了密码时，从节点应使用[masterauth ]配置对主节点的访问密码。 4）、主节点配置写查询接收条件：为了尽量保证主从一致性，主节点应配置当至少有 N 个 slave，并且滞后小于 M 秒时，才接收客户端写入命令[min-slaves-to-write &lt;slave 数量&gt; min-slaves-max-lag &lt;秒数&gt;] 五、主从复制常用3招 一主二仆： 一个 Master 两个 Slave； 薪火相传：上一个 Slave 可以是下一个 Slave 的 Master，Slave 同样可以接收其他 Slaves 的连接和同步请求，那么该 Slave 作为了链条中下一个的 Master，可以有效减轻 Master 的写压力。中途变更转向会清除之前的数据，重新建立拷贝最新的slaveof 新主库IP 新主库端口； 反客为主：SLAVEOF NO ONE。使当前数据库停止与其他数据库的同步，转成主数据库 六、主从复制原理 【1】slave 启动成功连接到 master 后会发送一个 sync 命令。 【2】Master 接到命令启动后台的存盘进程，同时收集所有接收到的用于修改数据集命令，在后台进程执行完毕之后，master 将传送整个数据文件到 slave，以完成一次完全同步。 【3】全量复制：而 slave 服务在接收到数据库文件数据后，将其存盘并加载到内存中。 【4】增量复制：Master 继续将新的所有收集到的修改命令依次传给 slave，完成同步。 【5】但是只要是重新连接 master，一次完全同步（全量复制)将被自动执行。 复制的缺点：由于所有的写操作都是先在 Master 上操作，然后同步更新到 Slave 上，所以从 Master 同步到 Slave 机器有一定的延迟，当系统很繁忙的时候，延迟问题会更加严重，Slave 机器数量的增加也会使这个问题更加严重。 二十、Redis 的线程模型是什么一、概述【1】、Redis 是基于 Reactor 模式开发的网络事件处理器：这个处理器被称为文件事件处理器（file event handler），这个文件事件处理器是单线程的，所以 Redis 才叫做单线程的模型： 文件事件处理器使用 I/O 多路复用（multiplexing）机制监听多个套接字 Socket，根据 Socket 上的事件来选择对应的事件处理器进行处理。 当被监听的套接字准备好执行连接应答（accept）、读取（read）、写入（write）、关闭（close）等操作时。与操作相对应的文件事件就会产生，这时文件事件处理器就会调用套接字之前关联好的事件处理器来处理这些事件。 【2】、虽然文件事件处理器以单线程的方式运行，但其使用 I/O 多路复用程序来监听多个套接字，文件事件处理器既实现了高性能的网络通信模型，又可以很好地与 Redis 服务器中其他同样以单线程方式运行的模块进行对接，这保持了 Redis 内部单线程设计的简单性。 二、文件事件处理器的结构 【1】、文件事件处理器的结构包含 4 个部分： ● 多个 socket ● IO 多路复用程序 ● 文件事件分派器 ● 事件处理器（连接应答处理器、命令请求处理器、命令回复处理器） 【2】、多个 socket 可能会并发产生不同的操作，每个操作对应不同的文件事件，但是 IO 多路复用程序会监听多个 socket，会将 socket 产生的事件放入队列中排队，以有序（sequentially）、同步（synchronously）、每次一个套接字的方式向文件事件分派器传送套接字。当上一个套接字产生的事件被处理完毕之后（该套接字为事件所关联的事件处理器执行完毕）， I/O 多路复用程序才会继续向文件事件分派器传送下一个套接字， 如图： 文件事件分派器接收 I/O 多路复用程序传来的套接字， 并根据套接字产生的事件的类型， 调用相应的事件处理器。服务器会为执行不同任务的套接字关联不同的事件处理器， 这些处理器是一个个函数， 它们定义了某个事件发生时， 服务器应该执行的动作。 【3】、I/O 多路复用程序的实现：Redis 的 I/O 多路复用程序的所有功能都是通过包装常见的select 、 epoll 、 evport 和 kqueue 这些 I/O 多路复用函数库来实现的， 每个 I/O 多路复用函数库在 Redis 源码中都对应一个单独的文件， 比如 ae_select.c 、 ae_epoll.c 、 ae_kqueue.c ， 诸如此类。因为 Redis 为每个 I/O 多路复用函数库都实现了相同的 API ， 所以 I/O 多路复用程序的底层实现是可以互换的， 如下图所示： Redis 在 I/O 多路复用程序的实现源码中用 #include 宏定义了相应的规则， 程序会在编译时自动选择系统中性能最高的 I/O 多路复用函数库来作为 Redis 的 I/O 多路复用程序的底层实现： 123456789101112131415/* 包括此系统支持的最佳复用层以下应按性能降序排列。 */#ifdef HAVE_EVPORT#include \"ae_evport.c\"#else #ifdef HAVE_EPOLL #include \"ae_epoll.c\" #else #ifdef HAVE_KQUEUE #include \"ae_kqueue.c\" #else #include \"ae_select.c\" #endif #endif 【4】、事件的类型：I/O 多路复用程序可以监听多个套接字的 ae.h/AE_READABLE 事件和 ae.h/AE_WRITABLE 事件， 这两类事件和套接字操作之间的对应关系如下： ■ 当套接字变得可读时（客户端对套接字执行 write 操作，或者执行 close 操作）， 或者有新的可应答（acceptable）套接字出现时（客户端对服务器的监听套接字执行 connect 操作）， 套接字产生 AE_READABLE 事件。 ■ 当套接字变得可写时（客户端对套接字执行 read 操作）， 套接字产生 AE_WRITABLE 事件。 I/O 多路复用程序允许服务器同时监听套接字的 AE_READABLE 事件和 AE_WRITABLE 事件， 如果一个套接字同时产生了这两种事件， 那么文件事件分派器会优先处理 AE_READABLE 事件， 等到 AE_READABLE 事件处理完之后， 才处理 AE_WRITABLE 事件。这也就是说， 如果一个套接字又可读又可写的话， 那么服务器将先读套接字， 后写套接字。 【5】、API：ae.c/aeCreateFileEvent 函数接受一个套接字描述符、 一个事件类型、 以及一个事件处理器作为参数， 将给定套接字的给定事件加入到 I/O 多路复用程序的监听范围之内， 并对事件和事件处理器进行关联。 ae.c/aeDeleteFileEvent 函数接受一个套接字描述符和一个监听事件类型作为参数， 让 I/O 多路复用程序取消对给定套接字的给定事件的监听， 并取消事件和事件处理器之间的关联。 ae.c/aeGetFileEvents 函数接受一个套接字描述符， 返回该套接字正在被监听的事件类型： 如果套接字没有任何事件被监听， 那么函数返回 AE_NONE 。如果套接字的读事件正在被监听， 那么函数返回 AE_READABLE 。如果套接字的写事件正在被监听， 那么函数返回 AE_WRITABLE 。如果套接字的读事件和写事件正在被监听， 那么函数返回 AE_READABLE | AE_WRITABLE 。ae.c/aeWait 函数接受一个套接字描述符、一个事件类型和一个毫秒数为参数， 在给定的时间内阻塞并等待套接字的给定类型事件产生， 当事件成功产生， 或者等待超时之后， 函数返回。 ae.c/aeApiPoll 函数接受一个 sys/time.h/struct timeval 结构为参数， 并在指定的时间內， 阻塞并等待所有被 aeCreateFileEvent 函数设置为监听状态的套接字产生文件事件， 当有至少一个事件产生， 或者等待超时后， 函数返回。 ae.c/aeProcessEvents 函数是文件事件分派器， 它先调用 aeApiPoll 函数来等待事件产生， 然后遍历所有已产生的事件， 并调用相应的事件处理器来处理这些事件。 ae.c/aeGetApiName 函数返回 I/O 多路复用程序底层所使用的 I/O 多路复用函数库的名称： 返回 “epoll” 表示底层为 epoll 函数库， 返回”select” 表示底层为 select 函数库， 诸如此类。 【6】、文件事件的处理器：Redis 为文件事件编写了多个处理器， 这些事件处理器分别用于实现不同的网络通讯需求， 比如： 为了对连接服务器的各个客户端进行应答， 服务器要为监听套接字关联连接应答处理器。 为了接收客户端传来的命令请求， 服务器要为客户端套接字关联命令请求处理器。 为了向客户端返回命令的执行结果， 服务器要为客户端套接字关联命令回复处理器。 当主服务器和从服务器进行复制操作时， 主从服务器都需要关联特别为复制功能编写的复制处理器。 在这些事件处理器里面， 服务器最常用的要数与客户端进行通信的连接应答处理器、 命令请求处理器和命令回复处理器。 【7】、连接应答处理器：networking.c/acceptTcpHandler 函数是 Redis 的连接应答处理器， 这个处理器用于对连接服务器监听套接字的客户端进行应答， 具体实现为sys/socket.h/accept 函数的包装。 当 Redis 服务器进行初始化的时候， 程序会将这个连接应答处理器和服务器监听套接字的 AE_READABLE 事件关联起来， 当有客户端用sys/socket.h/connect 函数连接服务器监听套接字的时候， 套接字就会产生 AE_READABLE 事件， 引发连接应答处理器执行， 并执行相应的套接字应答操作， 如图 IMAGE_SERVER_ACCEPT_CONNECT 所示。 【8】、命令请求处理器：networking.c/readQueryFromClient 函数是 Redis 的命令请求处理器， 这个处理器负责从套接字中读入客户端发送的命令请求内容， 具体实现为 unistd.h/read 函数的包装。 当一个客户端通过连接应答处理器成功连接到服务器之后， 服务器会将客户端套接字的 AE_READABLE 事件和命令请求处理器关联起来， 当客户端向服务器发送命令请求的时候， 套接字就会产生 AE_READABLE 事件， 引发命令请求处理器执行， 并执行相应的套接字读入操作， 如图 IMAGE_SERVER_RECIVE_COMMAND_REQUEST 所示。 当命令回复发送完毕之后， 服务器就会解除命令回复处理器与客户端套接字的 AE_WRITABLE 事件之间的关联 三、客户端与 redis 的一次通信过程 【1】客户端 socket01 向 redis 的 server socket 请求建立连接，此时 server socket 会产生一个 AE_READABLE 事件，IO 多路复用程序监听到 server socket 产生的事件后，将该事件压入队列中。文件事件分派器从队列中获取该事件，交给连接应答处理器。连接应答处理器会创建一个能与客户端通信的 socket01，并将该 socket01 的 AE_READABLE 事件与命令请求处理器关联。 【2】假设此时客户端发送了一个 set key value 请求，此时 redis 中的 socket01 会产生 AE_READABLE 事件，IO 多路复用程序将事件压入队列，此时事件分派器从队列中获取到该事件，由于前面 socket01 的 AE_READABLE 事件已经与命令请求处理器关联，因此事件分派器将事件交给命令请求处理器来处理。命令请求处理器读取 socket01 的 key value 并在自己内存中完成 key value 的设置。操作完成后，它会将 socket01 的 AE_WRITABLE 事件与命令回复处理器关联。 【3】如果此时客户端准备好接收返回结果了，那么 redis 中的 socket01 会产生一个 AE_WRITABLE 事件，同样压入队列中，事件分派器找到相关联的命令回复处理器，由命令回复处理器对 socket01 输入本次操作的一个结果，比如 ok，之后解除 socket01 的 AE_WRITABLE 事件与命令回复处理器的关联。这样便完成了一次通信。 四、为啥 redis 单线程模型也能效率这么高■ 纯内存操作 ■ 核心是基于非阻塞的 IO 多路复用机制 ■ 单线程反而避免了多线程的频繁上下文切换问题 二十一、如何看待缓存的使用（本地缓存，集中式缓存），简述本地缓存和集中式缓存和优缺点为什么要有本地缓存？在系统中，有些数据，数据量小，但是访问十分频繁（例如国家标准行政区域数据），针对这种场景，需要将数据搞到应用的本地缓存中，以提升系统的访问效率，减少无谓的数据库访问（数据库访问占用数据库连接，同时网络消耗比较大），但是有一点需要注意，就是缓存的占用空间以及缓存的失效策略。 为什么是本地缓存，而不是分布式的集群缓存？​ 目前的数据，大多是业务无关的小数据缓存，没有必要搞分布式的集群缓存，目前涉及到订单和商品的数据，会直接走DB进行请求，再加上分布式缓存的构建，集群维护成本比较高，不太适合紧急的业务项目。 本地缓存在那个区域？​ 目前考虑的是占用了JVM的heap区域，再细化一点的就是heap中的old区，目前的数据量来看，都是一些小数据，加起来没有几百兆，放在heap区域最快最方便。后期如果需要放置在本地缓存的数据大的时候，可以考虑在off-heap区域，但是off-heap区域的话，需要考虑对象的序列化（因为off-heap区域存储的是二进制的数据），另外一个的话就是off-heap的GC问题。其实，如果真的数据量比较大，那其实就可以考虑搞一个集中式的缓存系统，可以是单机，也可以是集群，来承担缓存的作用 本地缓存和分布式缓存的比较： 分布式缓存一致性更好一点，本地缓存 每个实例都有自己的缓存，可能会存在不一致的情况。 本地缓存会占用堆内存，影响垃圾回收、影响系统性能。分布式缓存两大开销会导致其慢于本地缓存，网络延迟和对象序列化 进程内缓存适用于较小且频率可见的访问场景，尤其适用于不变对象，对于较大且不可预见的访问，最好采用分布式缓存。 二十二、本地缓存在并发使用时的注意事项使用本地缓存需要注意两个问题： 1 内存管理，及时解除无用对象的引用。防止大量无用对象进入old区，引发full gc。 2 数据同步，如果应用是一个集群，需要保持各台机器的数据一致性。 问题1的解决可以采用LRU算法( Least Recently Used )，预先定好缓存大小。达到最大值后，清除最近最少使用的对象。 问题2比较复杂，需要有一个集中的地方控制缓存一致，比如可以采用消息中间件，写时进行异步复制。这种方式成本较大。","categories":[],"tags":[{"name":"Redis相关","slug":"Redis相关","permalink":"https://liudong-code.github.io/tags/Redis%E7%9B%B8%E5%85%B3/"}],"keywords":[]},{"title":"hashCode和equals","slug":"hashCode和equals","date":"2020-03-04T07:59:33.000Z","updated":"2020-03-04T08:05:09.349Z","comments":true,"path":"2020/03/04/hashCode和equals/","link":"","permalink":"https://liudong-code.github.io/2020/03/04/hashCode%E5%92%8Cequals/","excerpt":"","text":"equals()用来判断两个对象是否相同，再Object类中是通过判断对象间的内存地址来决定是否相同 equals() 方法用于比较两个对象是否相等，它与 == 相等比较符有着本质的不同。 在万物皆对象的 Java 体系中，系统把判断对象是否相等的权力交给程序员。具体的措施是把 equals() 方法写到 Object 类中，并让所有类继承 Object 类。 这样程序员就能在自定义的类中重写 equals() 方法, 从而实现自己的比较逻辑. hashCode() 获取哈希码，也称为散列码，返回一个int整数。这个哈希码的作用是确定该对象在哈希表中的索引位置。 hashCode() 的意思是哈希值, 哈希值是经哈希函数运算后得到的结果，哈希函数能够保证相同的输入能够得到相同的输出(哈希值)，但是不能够保证不同的输入总是能得出不同的输出。 当输入的样本量足够大时，是会产生哈希冲突的，也就是说不同的输入产生了相同的输出。 暂且不谈冲突，就相同的输入能够产生相同的输出这点而言，是及其宝贵的。它使得系统只需要通过简单的运算，在时间复杂度O(1)的情况下就能得出数据的映射关系，根据这种特性，散列表应运而生。 一种主流的散列表实现是：用数组作为哈希函数的输出域，输入值经过哈希函数计算后得到哈希值。然后根据哈希值，在数组种找到对应的存储单元。当发生冲突时，对应的存储单元以链表的形式保存冲突的数据。 hashCode() 与 equals() 之间的关系","categories":[],"tags":[{"name":"Java基础知识","slug":"Java基础知识","permalink":"https://liudong-code.github.io/tags/Java%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"keywords":[]},{"title":"HashSet内部是如何工作的","slug":"HashSet内部是如何工作的","date":"2020-03-04T07:47:57.000Z","updated":"2020-03-04T07:56:17.778Z","comments":true,"path":"2020/03/04/HashSet内部是如何工作的/","link":"","permalink":"https://liudong-code.github.io/2020/03/04/HashSet%E5%86%85%E9%83%A8%E6%98%AF%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C%E7%9A%84/","excerpt":"","text":"Java 中的 HashSet，内部是如何工作的？概述：HashSet 的内部采用 HashMap来实现。由于 Map 需要 key 和 value，所以HashSet中所有 key 的都有一个默认 value。类似于HashMap，HashSet 不允许重复的 key，只允许有一个null key，意思就是 HashSet 中只允许存储一个 null 对象。 HashSet： 实现了Set接口 HashSet依赖的数据结构是哈希表 因为实现的是Set接口，所以不允许有重复的值 插入到HashSet中的对象不保证与插入的顺序保持一致。对象的插入是根据它的hashcode HashSet中允许有NULL值 HashSet也实现了Searlizable和Cloneable两个接口 HashSet的构造函数：123456789HashSet h = new HashSet(); 默认初始化大小是16，默认装载因子是0.75.HashSet h = new HashSet(int initialCapacity); 默认装载因子是0.75HashSet h = new HashSet(int initialCapacity, float loadFactor);HashSet h = new HashSet(Collection C); 什么是初始化大小与装载因子：初始化尺寸就是当创建哈希表（HashSet内部用哈希表的数据结构）的时候桶（buckets）的数量。如果当前的尺寸已经满了，那么桶的数量会自动增长。 装载因子衡量的是在HashSet自动增长之前允许有多满。当哈希表中实体的数量已经超出装载因子与当前容量的积，那么哈希表就会再次进行哈希（也就是内部数据结构重建），这样哈希表大致有两倍桶的数量。 1234567 表中已经存储的元素的数量装载因子 = *-----------------------------------------* 哈希表的大小 例如：如果内部容量为16，装载因子为0.75，那么当表中有12个元素的时候，桶的数量就会自动增长。 性能影响： 装载因子和初始化容量是影响HashSet操作的两个主要因素。装载因子为0.75的时候可以提供关于时间和空间复杂度方面更有效的性能。如果我们加大这个装载因子，那么内存的上限就会减小（因为它减少了内部重建的操作），但是将影响哈希表中的add与查询的操作。为了减少再哈希操作，我们应该选择一个合适的初始化大小。如果初始化容量大于实体的最大数量除以装载因子，那么就不会有再哈希的动作发生了。 HashSet中的一些重要方法： boolean add(E e)：如果不存在则添加，存在则返回false。 void clear() ：移除Set中所有的元素 boolean contains(Object o)：如果这个元素在set中存在，那么返回true。 boolean remove(Object o)：如果这个元素在set中存在，那么从set中删除。 Iterator iterator()：返回set中这个元素的迭代器。 HashSet内部是如何工作的？ 所有Set接口的类内部都是由Map做支撑的。HashSet用HashMap对它的内部对象进行排序。你一定好奇输入一个值到HashMap，我们需要的是一个键值对，但是我们传给HashSet的是一个值。 那么HashMap是如何排序的？ 实际上我们插入到HashSet中的值在map对象中起的是键的作用，因为它的值Java用了一个常量。所以在键值对中所有的键的值都是一样的。 如果我们在Java Doc中看一下HashSet的实现，大致是这样的： 12345678910111213141516171819private transient HashMap map;// Constructor - 1// All the constructors are internally creating HashMap Object.public HashSet()&#123; // Creating internally backing HashMap object map = new HashMap&lt;&gt;();&#125;// Constructor - 2public HashSet(int initialCapacity)&#123; // Creating internally backing HashMap object map = new HashMap&lt;&gt;(initialCapacity);&#125;// Dummy value to associate with an Object in Mapprivate static final Object PRESENT = new Object(); 如果我们看下HashSet中的add方法： 12345public boolean add(E e)&#123; return map.put(e, PRESENT) == null;&#125; 我们可以注意到，HashSet类的add()方法内部调用的是HashMap的put()方法，通过你指定的值作为key，常量“PRESENT”作为值传过去。 remove()也是用类似的方法工作。它内部调用的是Map接口的remove。 12345678public boolean remove(Object o)&#123; return map.remove(o) == PRESENT;&#125; HashSet操作的时间复杂度： HashSet底层的数据结构是哈希表，所以HashSet的add，remove与查询（包括contain方法）的分摊（平均或者一般情况）时间复杂度是O(1)。","categories":[],"tags":[{"name":"Java基础知识","slug":"Java基础知识","permalink":"https://liudong-code.github.io/tags/Java%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"keywords":[]},{"title":"Java反射机制","slug":"Java反射机制","date":"2020-03-04T07:30:28.000Z","updated":"2020-03-04T07:34:53.604Z","comments":true,"path":"2020/03/04/Java反射机制/","link":"","permalink":"https://liudong-code.github.io/2020/03/04/Java%E5%8F%8D%E5%B0%84%E6%9C%BA%E5%88%B6/","excerpt":"","text":"1：SUN提供的反射机制的类：java.lang.Class java.lang.reflect.Constructor java.lang.reflect.Field java.lang.reflect.Method java.lang.reflect.Modifier 2：什么是反射 JAVA反射机制是在运行状态中，对于任意一个类。都能都知道这个类的所有属性和方法，对于任意一个对象，都能够调用它的任意一个方法和属性；这种动态获取的信息以及动态调用对象的方法的功能称之为java语言的反射机制； 3：反射的作用反编译 .class –à .java 通过反射机制可以访问java对象中的属性，方法，构造方法 4：创建Class对象的三种方式1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public class Person implements China&#123; private String name; private int age ; private char sex ; public Person() &#123; super (); &#125; public Person(String name, int age, char sex) &#123; super (); this .name = name; this .age = age; this .sex = sex; &#125; public String getName() &#123; return name ; &#125; public void setName(String name) &#123; this .name = name; &#125; public int getAge() &#123; return age ; &#125; public void setAge(int age) &#123; this .age = age; &#125; public char getSex() &#123; return sex ; &#125; public void setSex(char sex) &#123; this .sex = sex; &#125; public void eat() &#123; System. out .println(\"吃了\" ); &#125; @Override public String toString() &#123; return \"Person [name=\" + name + \", age=\" + age + \", sex=\" + sex + \"]\" ; &#125; @Override public void sayChina() &#123; // TODO Auto-generated method stub System. out .println(\"作者：\" + AUTHOR + \"国籍：\"+ NATIONAL ); &#125; @Override public String sayHello(String name, int age, char sex) &#123; // TODO Auto-generated method stub return \"姓名:\" + name + \"年龄：\"+ age + \"性别:\" + sex; &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435public class ClassDemo02 &#123; public static void main(String[] args) &#123; Person p1 = new Person(\"小明\" ,20,'男' ); Person p2 = new Person(\"小红\" ,23,'女' ); //创建Class对象的方式一：(对象.getClass())，获取person类中的字节码文件 Class class1 = p1.getClass(); System. out.println(p1.getClass().getName()); Class class2 = p2.getClass(); System. out.println(class1 == class2 ); System. out.println(\"==============================\" ); //创建Class对象的方式二：(类.class:需要输入一个明确的类，任意一个类型都有一个静态的class属性) Class class3 = Person.class; System. out.println(class1 == class2); System. out.println(\"==============================\" ); //创建Class对象的方式三：(forName():传入时只需要以字符串的方式传入即可) //通过Class类的一个forName（String className)静态方法返回一个Class对象，className必须是全路径名称； //Class.forName()有异常：ClassNotFoundException Class class4 = null; try &#123; class4 = Class.forName(\"cn.itcast.Person\"); &#125; catch (ClassNotFoundException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; System. out.println(class4 == class3); &#125;&#125; 注意：在开发中一般使用第三种方法，因为第三种接收的是一个字符串路径，将来可以通过配置文件获取，通用性好； 4：newInstance()方法 —&gt; 获取class类型之后,可以创建该类型的对象 1public T newInstance()throws InstantiationException,IllegalAccessException 1234567891011public class reflect03 &#123; public static void main(String[] args) throws Exception &#123; Class c1 = Class.forName(\"com.itheima04.Test_20171106.Test_20171207.Person\"); //创建此Class对象所表示类的一个新实例, //newInstance方法调用的是Person的空参数构造方法 Object o = c1.newInstance(); System.out.println(o.toString());&#125;","categories":[],"tags":[{"name":"Java基础知识","slug":"Java基础知识","permalink":"https://liudong-code.github.io/tags/Java%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"keywords":[]},{"title":"Map类","slug":"HashMap类","date":"2020-03-04T06:02:37.000Z","updated":"2020-03-04T07:21:15.247Z","comments":true,"path":"2020/03/04/HashMap类/","link":"","permalink":"https://liudong-code.github.io/2020/03/04/HashMap%E7%B1%BB/","excerpt":"","text":"摘要HashMap是Java程序员使用频率最高的用于映射(键值对)处理的数据类型。随着JDK（Java Developmet Kit）版本的更新，JDK1.8对HashMap底层的实现进行了优化，例如引入红黑树的数据结构和扩容的优化等。本文结合JDK1.7和JDK1.8的区别，深入探讨HashMap的结构实现和功能原理。 简介Java为数据结构中的映射定义了一个接口java.util.Map，此接口主要有四个常用的实现类，分别是HashMap、Hashtable、LinkedHashMap和TreeMap，类继承关系如下图所示： 下面针对各个实现类的特点做一些说明： (1) HashMap：它根据键的hashCode值存储数据，大多数情况下可以直接定位到它的值，因而具有很快的访问速度，但遍历顺序却是不确定的。 HashMap最多只允许一条记录的键为null，允许多条记录的值为null。HashMap非线程安全，即任一时刻可以有多个线程同时写HashMap，可能会导致数据的不一致。如果需要满足线程安全，可以用 Collections的synchronizedMap方法使HashMap具有线程安全的能力，或者使用ConcurrentHashMap。 (2) Hashtable：Hashtable是遗留类，很多映射的常用功能与HashMap类似，不同的是它承自Dictionary类，并且是线程安全的，任一时间只有一个线程能写Hashtable，并发性不如ConcurrentHashMap，因为ConcurrentHashMap引入了分段锁。Hashtable不建议在新代码中使用，不需要线程安全的场合可以用HashMap替换，需要线程安全的场合可以用ConcurrentHashMap替换。 (3) LinkedHashMap：LinkedHashMap是HashMap的一个子类，保存了记录的插入顺序，在用Iterator遍历LinkedHashMap时，先得到的记录肯定是先插入的，也可以在构造时带参数，按照访问次序排序。 4) TreeMap：TreeMap实现SortedMap接口，能够把它保存的记录根据键排序，默认是按键值的升序排序，也可以指定排序的比较器，当用Iterator遍历TreeMap时，得到的记录是排过序的。如果使用排序的映射，建议使用TreeMap。在使用TreeMap时，key必须实现Comparable接口或者在构造TreeMap传入自定义的Comparator，否则会在运行时抛出java.lang.ClassCastException类型的异常。 对于上述四种Map类型的类，要求映射中的key是不可变对象。不可变对象是该对象在创建后它的哈希值不会被改变。如果对象的哈希值发生变化，Map对象很可能就定位不到映射的位置了。 通过上面的比较，我们知道了HashMap是Java的Map家族中一个普通成员，鉴于它可以满足大多数场景的使用条件，所以是使用频度最高的一个。下文我们主要结合源码，从存储结构、常用方法分析、扩容以及安全性等方面深入讲解HashMap的工作原理。 内部实现 搞清楚HashMap，首先需要知道HashMap是什么，即它的存储结构-字段；其次弄明白它能干什么，即它的功能实现-方法。下面我们针对这两个方面详细展开讲解。 存储结构-字段 从结构实现来讲，HashMap是数组+链表+红黑树（JDK1.8增加了红黑树部分）实现的，如下如所示。 这里需要讲明白两个问题：数据底层具体存储的是什么？这样的存储方式有什么优点呢？ (1) 从源码可知，HashMap类中有一个非常重要的字段，就是 Node[] table，即哈希桶数组，明显它是一个Node的数组。我们来看Node[JDK1.8]是何物。 1234567891011121314static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; //用来定位数组索引位置 final K key; V value; Node&lt;K,V&gt; next; //链表的下一个node Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; ... &#125; public final K getKey()&#123; ... &#125; public final V getValue() &#123; ... &#125; public final String toString() &#123; ... &#125; public final int hashCode() &#123; ... &#125; public final V setValue(V newValue) &#123; ... &#125; public final boolean equals(Object o) &#123; ... &#125;&#125; Node是HashMap的一个内部类，实现了Map.Entry接口，本质是就是一个映射(键值对)。上图中的每个黑色圆点就是一个Node对象。 (2) HashMap就是使用哈希表来存储的。哈希表为解决冲突，可以采用开放地址法和链地址法等来解决问题，Java中HashMap采用了链地址法。链地址法，简单来说，就是数组加链表的结合。在每个数组元素上都一个链表结构，当数据被Hash后，得到数组下标，把数据放在对应下标元素的链表上。例如程序执行下面代码： 1map.put(\"美团\",\"小美\"); 系统将调用”美团”这个key的hashCode()方法得到其hashCode 值（该方法适用于每个Java对象），然后再通过Hash算法的后两步运算（高位运算和取模运算，下文有介绍）来定位该键值对的存储位置，有时两个key会定位到相同的位置，表示发生了Hash碰撞。当然Hash算法计算结果越分散均匀，Hash碰撞的概率就越小，map的存取效率就会越高。 如果哈希桶数组很大，即使较差的Hash算法也会比较分散，如果哈希桶数组数组很小，即使好的Hash算法也会出现较多碰撞，所以就需要在空间成本和时间成本之间权衡，其实就是在根据实际情况确定哈希桶数组的大小，并在此基础上设计好的hash算法减少Hash碰撞。那么通过什么方式来控制map使得Hash碰撞的概率又小，哈希桶数组（Node[] table）占用空间又少呢？答案就是好的Hash算法和扩容机制。 在理解Hash和扩容流程之前，我们得先了解下HashMap的几个字段。从HashMap的默认构造函数源码可知，构造函数就是对下面几个字段进行初始化，源码如下： 1234int threshold; // 所能容纳的key-value对极限 final float loadFactor; // 负载因子 int modCount; int size; 首先，Node[] table的初始化长度length(默认值是16)，Load factor为负载因子(默认值是0.75)，threshold是HashMap所能容纳的最大数据量的Node(键值对)个数。threshold = length * Load factor。也就是说，在数组定义好长度之后，负载因子越大，所能容纳的键值对个数越多。 结合负载因子的定义公式可知，threshold就是在此Load factor和length(数组长度)对应下允许的最大元素数目，超过这个数目就重新resize(扩容)，扩容后的HashMap容量是之前容量的两倍。默认的负载因子0.75是对空间和时间效率的一个平衡选择，建议大家不要修改，除非在时间和空间比较特殊的情况下，如果内存空间很多而又对时间效率要求很高，可以降低负载因子Load factor的值；相反，如果内存空间紧张而对时间效率要求不高，可以增加负载因子loadFactor的值，这个值可以大于1。 size这个字段其实很好理解，就是HashMap中实际存在的键值对数量。注意和table的长度length、容纳最大键值对数量threshold的区别。而modCount字段主要用来记录HashMap内部结构发生变化的次数，主要用于迭代的快速失败。强调一点，内部结构发生变化指的是结构发生变化，例如put新键值对，但是某个key对应的value值被覆盖不属于结构变化。 在HashMap中，哈希桶数组table的长度length大小必须为2的n次方(一定是合数)，这是一种非常规的设计，常规的设计是把桶的大小设计为素数。相对来说素数导致冲突的概率要小于合数，具体证明可以参考http://blog.csdn.net/liuqiyao_01/article/details/14475159，Hashtable初始化桶大小为11，就是桶大小设计为素数的应用（Hashtable扩容后不能保证还是素数）。HashMap采用这种非常规设计，主要是为了在取模和扩容时做优化，同时为了减少冲突，HashMap定位哈希桶索引位置时，也加入了高位参与运算的过程。 这里存在一个问题，即使负载因子和Hash算法设计的再合理，也免不了会出现拉链过长的情况，一旦出现拉链过长，则会严重影响HashMap的性能。于是，在JDK1.8版本中，对数据结构做了进一步的优化，引入了红黑树。而当链表长度太长（默认超过8）时，链表就转换为红黑树，利用红黑树快速增删改查的特点提高HashMap的性能，其中会用到红黑树的插入、删除、查找等算法。本文不再对红黑树展开讨论，想了解更多红黑树数据结构的工作原理可以参考http://blog.csdn.net/v_july_v/article/details/6105630。 功能实现-方法 HashMap的内部功能实现很多，本文主要从根据key获取哈希桶数组索引位置、put方法的详细执行、扩容过程三个具有代表性的点深入展开讲解。 1. 确定哈希桶数组索引位置 不管增加、删除、查找键值对，定位到哈希桶数组的位置都是很关键的第一步。前面说过HashMap的数据结构是数组和链表的结合，所以我们当然希望这个HashMap里面的元素位置尽量分布均匀些，尽量使得每个位置上的元素数量只有一个，那么当我们用hash算法求得这个位置的时候，马上就可以知道对应位置的元素就是我们要的，不用遍历链表，大大优化了查询的效率。HashMap定位数组索引位置，直接决定了hash方法的离散性能。先看看源码的实现(方法一+方法二): 1234567891011方法一：static final int hash(Object key) &#123; //jdk1.8 &amp; jdk1.7 int h; // h = key.hashCode() 为第一步 取hashCode值 // h ^ (h &gt;&gt;&gt; 16) 为第二步 高位参与运算 return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125;方法二：static int indexFor(int h, int length) &#123; //jdk1.7的源码，jdk1.8没有这个方法，但是实现原理一样的 return h &amp; (length-1); //第三步 取模运算&#125; 这里的Hash算法本质上就是三步：取key的hashCode值、高位运算、取模运算。 对于任意给定的对象，只要它的hashCode()返回值相同，那么程序调用方法一所计算得到的Hash码值总是相同的。我们首先想到的就是把hash值对数组长度取模运算，这样一来，元素的分布相对来说是比较均匀的。但是，模运算的消耗还是比较大的，在HashMap中是这样做的：调用方法二来计算该对象应该保存在table数组的哪个索引处。 这个方法非常巧妙，它通过h &amp; (table.length -1)来得到该对象的保存位，而HashMap底层数组的长度总是2的n次方，这是HashMap在速度上的优化。当length总是2的n次方时，h&amp; (length-1)运算等价于对length取模，也就是h%length，但是&amp;比%具有更高的效率。 在JDK1.8的实现中，优化了高位运算的算法，通过hashCode()的高16位异或低16位实现的：(h = k.hashCode()) ^ (h &gt;&gt;&gt; 16)，主要是从速度、功效、质量来考虑的，这么做可以在数组table的length比较小的时候，也能保证考虑到高低Bit都参与到Hash的计算中，同时不会有太大的开销。 下面举例说明下，n为table的长度。 2. 分析HashMap的put方法 HashMap的put方法执行过程可以通过下图来理解，自己有兴趣可以去对比源码更清楚地研究学习。 ①.判断键值对数组table[i]是否为空或为null，否则执行resize()进行扩容； ②.根据键值key计算hash值得到插入的数组索引i，如果table[i]==null，直接新建节点添加，转向⑥，如果table[i]不为空，转向③； ③.判断table[i]的首个元素是否和key一样，如果相同直接覆盖value，否则转向④，这里的相同指的是hashCode以及equals； ④.判断table[i] 是否为treeNode，即table[i] 是否是红黑树，如果是红黑树，则直接在树中插入键值对，否则转向⑤； ⑤.遍历table[i]，判断链表长度是否大于8，大于8的话把链表转换为红黑树，在红黑树中执行插入操作，否则进行链表的插入操作；遍历过程中若发现key已经存在直接覆盖value即可； ⑥.插入成功后，判断实际存在的键值对数量size是否超多了最大容量threshold，如果超过，进行扩容。 JDK1.8HashMap的put方法源码如下: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556 public V put(K key, V value) &#123; // 对key的hashCode()做hash return putVal(hash(key), key, value, false, true); &#125; final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // 步骤①：tab为空则创建 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 步骤②：计算index，并对null做处理 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; // 步骤③：节点key存在，直接覆盖value if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // 步骤④：判断该链为红黑树 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); // 步骤⑤：该链为链表 else &#123; for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key,value,null); //链表长度大于8转换为红黑树进行处理 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; // key已经存在直接覆盖value if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; // 步骤⑥：超过最大容量 就扩容 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125; 3. 扩容机制 扩容(resize)就是重新计算容量，向HashMap对象里不停的添加元素，而HashMap对象内部的数组无法装载更多的元素时，对象就需要扩大数组的长度，以便能装入更多的元素。当然Java里的数组是无法自动扩容的，方法是使用一个新的数组代替已有的容量小的数组，就像我们用一个小桶装水，如果想装更多的水，就得换大水桶。 我们分析下resize的源码，鉴于JDK1.8融入了红黑树，较复杂，为了便于理解我们仍然使用JDK1.7的代码，好理解一些，本质上区别不大，具体区别后文再说。 12345678910111213void resize(int newCapacity) &#123; //传入新的容量 Entry[] oldTable = table; //引用扩容前的Entry数组 int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) &#123; //扩容前的数组大小如果已经达到最大(2^30)了 threshold = Integer.MAX_VALUE; //修改阈值为int的最大值(2^31-1)，这样以后就不会扩容了 return; &#125; Entry[] newTable = new Entry[newCapacity]; //初始化一个新的Entry数组 transfer(newTable); //！！将数据转移到新的Entry数组里 table = newTable; //HashMap的table属性引用新的Entry数组 threshold = (int)(newCapacity * loadFactor);//修改阈值&#125; 这里就是使用一个容量更大的数组来代替已有的容量小的数组，transfer()方法将原有Entry数组的元素拷贝到新的Entry数组里。 1234567891011121314151617void transfer(Entry[] newTable) &#123; Entry[] src = table; //src引用了旧的Entry数组 int newCapacity = newTable.length; for (int j = 0; j &lt; src.length; j++) &#123; //遍历旧的Entry数组 Entry&lt;K,V&gt; e = src[j]; //取得旧Entry数组的每个元素 if (e != null) &#123; src[j] = null;//释放旧Entry数组的对象引用（for循环后，旧的Entry数组不再引用任何对象） do &#123; Entry&lt;K,V&gt; next = e.next; int i = indexFor(e.hash, newCapacity); //！！重新计算每个元素在数组中的位置 e.next = newTable[i]; //标记[1] newTable[i] = e; //将元素放在数组上 e = next; //访问下一个Entry链上的元素 &#125; while (e != null); &#125; &#125;&#125; newTable[i]的引用赋给了e.next，也就是使用了单链表的头插入方式，同一位置上新元素总会被放在链表的头部位置；这样先放在一个索引上的元素终会被放到Entry链的尾部(如果发生了hash冲突的话），这一点和Jdk1.8有区别，下文详解。在旧数组中同一条Entry链上的元素，通过重新计算索引位置后，有可能被放到了新数组的不同位置上。 下面举个例子说明下扩容过程。假设了我们的hash算法就是简单的用key mod 一下表的大小（也就是数组的长度）。其中的哈希桶数组table的size=2， 所以key = 3、7、5，put顺序依次为 5、7、3。在mod 2以后都冲突在table[1]这里了。这里假设负载因子 loadFactor=1，即当键值对的实际大小size 大于 table的实际大小时进行扩容。接下来的三个步骤是哈希桶数组 resize成4，然后所有的Node重新rehash的过程。 下面我们讲解下JDK1.8做了哪些优化。经过观测可以发现，我们使用的是2次幂的扩展(指长度扩为原来2倍)，所以，元素的位置要么是在原位置，要么是在原位置再移动2次幂的位置。看下图可以明白这句话的意思，n为table的长度，图（a）表示扩容前的key1和key2两种key确定索引位置的示例，图（b）表示扩容后key1和key2两种key确定索引位置的示例，其中hash1是key1对应的哈希与高位运算结果。 元素在重新计算hash之后，因为n变为2倍，那么n-1的mask范围在高位多1bit(红色)，因此新的index就会发生这样的变化： 因此，我们在扩充HashMap的时候，不需要像JDK1.7的实现那样重新计算hash，只需要看看原来的hash值新增的那个bit是1还是0就好了，是0的话索引没变，是1的话索引变成“原索引+oldCap”，可以看看下图为16扩充为32的resize示意图： 这个设计确实非常的巧妙，既省去了重新计算hash值的时间，而且同时，由于新增的1bit是0还是1可以认为是随机的，因此resize的过程，均匀的把之前的冲突的节点分散到新的bucket了。这一块就是JDK1.8新增的优化点。有一点注意区别，JDK1.7中rehash的时候，旧链表迁移新链表的时候，如果在新表的数组索引位置相同，则链表元素会倒置，但是从上图可以看出，JDK1.8不会倒置。有兴趣的同学可以研究下JDK1.8的resize源码，写的很赞，如下: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) &#123; // 超过最大值就不再扩充了，就只好随你碰撞去吧 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; // 没超过最大值，就扩充为原来的2倍 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; // 计算新的resize上限 if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; @SuppressWarnings(&#123;\"rawtypes\"，\"unchecked\"&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) &#123; // 把每个bucket都移动到新的buckets中 for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // 链表优化重hash的代码块 Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; // 原索引 if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; // 原索引+oldCap else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); // 原索引放到bucket里 if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; // 原索引+oldCap放到bucket里 if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; 线程安全性 在多线程使用场景中，应该尽量避免使用线程不安全的HashMap，而使用线程安全的ConcurrentHashMap。那么为什么说HashMap是线程不安全的，下面举例子说明在并发的多线程使用场景中使用HashMap可能造成死循环。代码例子如下(便于理解，仍然使用JDK1.7的环境)： 1234567891011121314151617181920public class HashMapInfiniteLoop &#123; private static HashMap&lt;Integer,String&gt; map = new HashMap&lt;Integer,String&gt;(2，0.75f); public static void main(String[] args) &#123; map.put(5， \"C\"); new Thread(\"Thread1\") &#123; public void run() &#123; map.put(7, \"B\"); System.out.println(map); &#125;; &#125;.start(); new Thread(\"Thread2\") &#123; public void run() &#123; map.put(3, \"A); System.out.println(map); &#125;; &#125;.start(); &#125; &#125; 其中，map初始化为一个长度为2的数组，loadFactor=0.75，threshold=2*0.75=1，也就是说当put第二个key的时候，map就需要进行resize。 通过设置断点让线程1和线程2同时debug到transfer方法(3.3小节代码块)的首行。注意此时两个线程已经成功添加数据。放开thread1的断点至transfer方法的“Entry next = e.next;” 这一行；然后放开线程2的的断点，让线程2进行resize。结果如下图。 注意，Thread1的 e 指向了key(3)，而next指向了key(7)，其在线程二rehash后，指向了线程二重组后的链表。 线程一被调度回来执行，先是执行 newTalbe[i] = e， 然后是e = next，导致了e指向了key(7)，而下一次循环的next = e.next导致了next指向了key(3)。 e.next = newTable[i] 导致 key(3).next 指向了 key(7)。注意：此时的key(7).next 已经指向了key(3)， 环形链表就这样出现了。 于是，当我们用线程一调用map.get(11)时，悲剧就出现了——Infinite Loop。 JDK1.8与JDK1.7的性能对比 HashMap中，如果key经过hash算法得出的数组索引位置全部不相同，即Hash算法非常好，那样的话，getKey方法的时间复杂度就是O(1)，如果Hash算法技术的结果碰撞非常多，假如Hash算极其差，所有的Hash算法结果得出的索引位置一样，那样所有的键值对都集中到一个桶中，或者在一个链表中，或者在一个红黑树中，时间复杂度分别为O(n)和O(lgn)。 鉴于JDK1.8做了多方面的优化，总体性能优于JDK1.7，下面我们从两个方面用例子证明这一点。 Hash较均匀的情况为了便于测试，我们先写一个类Key，如下： 123456789101112131415161718192021222324252627class Key implements Comparable&lt;Key&gt; &#123; private final int value; Key(int value) &#123; this.value = value; &#125; @Override public int compareTo(Key o) &#123; return Integer.compare(this.value, o.value); &#125; @Override public boolean equals(Object o) &#123; if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; Key key = (Key) o; return value == key.value; &#125; @Override public int hashCode() &#123; return value; &#125;&#125; 这个类复写了equals方法，并且提供了相当好的hashCode函数，任何一个值的hashCode都不会相同，因为直接使用value当做hashcode。为了避免频繁的GC，我将不变的Key实例缓存了起来，而不是一遍一遍的创建它们。代码如下： 123456789101112131415public class Keys &#123; public static final int MAX_KEY = 10_000_000; private static final Key[] KEYS_CACHE = new Key[MAX_KEY]; static &#123; for (int i = 0; i &lt; MAX_KEY; ++i) &#123; KEYS_CACHE[i] = new Key(i); &#125; &#125; public static Key of(int value) &#123; return KEYS_CACHE[value]; &#125;&#125; 现在开始我们的试验，测试需要做的仅仅是，创建不同size的HashMap（1、10、100、……10000000），屏蔽了扩容的情况，代码如下： 1234567891011121314151617181920static void test(int mapSize) &#123; HashMap&lt;Key, Integer&gt; map = new HashMap&lt;Key,Integer&gt;(mapSize); for (int i = 0; i &lt; mapSize; ++i) &#123; map.put(Keys.of(i), i); &#125; long beginTime = System.nanoTime(); //获取纳秒 for (int i = 0; i &lt; mapSize; i++) &#123; map.get(Keys.of(i)); &#125; long endTime = System.nanoTime(); System.out.println(endTime - beginTime); &#125; public static void main(String[] args) &#123; for(int i=10;i&lt;= 1000 0000;i*= 10)&#123; test(i); &#125; &#125; 在测试中会查找不同的值，然后度量花费的时间，为了计算getKey的平均时间，我们遍历所有的get方法，计算总的时间，除以key的数量，计算一个平均值，主要用来比较，绝对值可能会受很多环境因素的影响。结果如下： 通过观测测试结果可知，JDK1.8的性能要高于JDK1.7 15%以上，在某些size的区域上，甚至高于100%。由于Hash算法较均匀，JDK1.8引入的红黑树效果不明显，下面我们看看Hash不均匀的的情况。 Hash极不均匀的情况假设我们又一个非常差的Key，它们所有的实例都返回相同的hashCode值。这是使用HashMap最坏的情况。代码修改如下： 123456789class Key implements Comparable&lt;Key&gt; &#123; //... @Override public int hashCode() &#123; return 1; &#125;&#125; 仍然执行main方法，得出的结果如下表所示： 从表中结果中可知，随着size的变大，JDK1.7的花费时间是增长的趋势，而JDK1.8是明显的降低趋势，并且呈现对数增长稳定。当一个链表太长的时候，HashMap会动态的将它替换成一个红黑树，这话的话会将时间复杂度从O(n)降为O(logn)。hash算法均匀和不均匀所花费的时间明显也不相同，这两种情况的相对比较，可以说明一个好的hash算法的重要性。 测试环境：处理器为2.2 GHz Intel Core i7，内存为16 GB 1600 MHz DDR3，SSD硬盘，使用默认的JVM参数，运行在64位的OS X 10.10.1上。从表中结果中可知，随着size的变大，JDK1.7的花费时间是增长的趋势，而JDK1.8是明显的降低趋势，并且呈现对数增长稳定。当一个链表太长的时候，HashMap会动态的将它替换成一个红黑树，这话的话会将时间复杂度从O(n)降为O(logn)。hash算法均匀和不均匀所花费的时间明显也不相同，这两种情况的相对比较，可以说明一个好的hash算法的重要性。 测试环境：处理器为2.2 GHz Intel Core i7，内存为16 GB 1600 MHz DDR3，SSD硬盘，使用默认的JVM参数，运行在64位的OS X 10.10.1上。 小结(1) 扩容是一个特别耗性能的操作，所以当程序员在使用HashMap的时候，估算map的大小，初始化的时候给一个大致的数值，避免map进行频繁的扩容。 (2) 负载因子是可以修改的，也可以大于1，但是建议不要轻易修改，除非情况非常特殊。 (3) HashMap是线程不安全的，不要在并发的环境中同时操作HashMap，建议使用ConcurrentHashMap。 (4) JDK1.8引入红黑树大程度优化了HashMap的性能。 (5) 还没升级JDK1.8的，现在开始升级吧。HashMap的性能提升仅仅是JDK1.8的冰山一角。 原文链接：https://zhuanlan.zhihu.com/p/21673805","categories":[],"tags":[{"name":"Java基础知识","slug":"Java基础知识","permalink":"https://liudong-code.github.io/tags/Java%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"keywords":[]},{"title":"数组和链表数据结构描述，各自的时间复杂度","slug":"数组和链表数据结构描述，各自的时间复杂度","date":"2020-03-04T05:55:00.000Z","updated":"2020-03-04T05:57:03.450Z","comments":true,"path":"2020/03/04/数组和链表数据结构描述，各自的时间复杂度/","link":"","permalink":"https://liudong-code.github.io/2020/03/04/%E6%95%B0%E7%BB%84%E5%92%8C%E9%93%BE%E8%A1%A8%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E6%8F%8F%E8%BF%B0%EF%BC%8C%E5%90%84%E8%87%AA%E7%9A%84%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6/","excerpt":"","text":"两种数据结构都是线性表，在排序和查找等算法中都有广泛的应用一、各自的特点：数组：数组是将元素在内存中连续存放，由于每个元素占用内存相同，可以通过下标迅速访问数组中任何元素。但是如果要在数组中增加一个元素，需要移动大量元素，在内存中空出一个元素的空间，然后将要增加的元素放在其中。同样的道理，如果想删除一个元素，同样需要移动大量元素去填掉被移动的元素。如果应用需要快速访问数据，很少或不插入和删除元素，就应该用数组。 链表：链表恰好相反，链表中的元素在内存中不是顺序存储的，而是通过存在元素中的指针联系到一起。比如：上一个元素有个指针指到下一个元素，以此类推，直到最后一个元素。如果要访问链表中一个元素，需要从第一个元素开始，一直找到需要的元素位置。但是增加和删除一个元素对于链表数据结构就非常简单了，只要修改元素中的指针就可以了。如果应用需要经常插入和删除元素你就需要用链表数据结构了。 二、数组和链表的区别：1、从逻辑结构角度来看： 数组必须事先定义固定的长度（元素个数），不能适应数据动态地增减的情况。当数据增加时，可能超出原先定义的元素个数；当数据减少时，造成内存浪费。链表动态地进行存储分配，可以适应数据动态地增减的情况，且可以方便地插入、删除数据项。（数组中插入、删除数据项时，需要移动其它数据项）2、数组元素在栈区，链表元素在堆区； 3、从内存存储角度来看： (静态)数组从栈中分配空间, 对于程序员方便快速,但自由度小。链表从堆中分配空间, 自由度大但申请管理比较麻烦。数组利用下标定位，时间复杂度为O(1)，链表定位元素时间复杂度O(n)；数组插入或删除元素的时间复杂度O(n)，链表的时间复杂度O(1)。","categories":[],"tags":[{"name":"Java基础知识","slug":"Java基础知识","permalink":"https://liudong-code.github.io/tags/Java%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"keywords":[]},{"title":"java8的新特性","slug":"java8的新特性","date":"2020-03-04T04:33:02.000Z","updated":"2020-03-04T05:52:32.381Z","comments":true,"path":"2020/03/04/java8的新特性/","link":"","permalink":"https://liudong-code.github.io/2020/03/04/java8%E7%9A%84%E6%96%B0%E7%89%B9%E6%80%A7/","excerpt":"","text":"一、接口的默认方法 Java 8允许我们给接口添加一个非抽象的方法实现，只需要使用 default关键字即可，这个特征又叫做扩展方法，示例如下： 123456interface Formula &#123; default double sqrt(int a) &#123; return Math.sqrt(a); &#125;&#125; Formula接口在拥有calculate方法之外同时还定义了sqrt方法，实现了Formula接口的子类只需要实现一个calculate方法，默认方法sqrt将在子类上可以直接使用。 12345678910Formula formula = new Formula() &#123; @Override public double calculate(int a) &#123; return sqrt(a * 100); &#125;&#125;；formula.calculate(100); // 100.0formula.sqrt(16); // 4.0 文中的formula被实现为一个匿名类的实例，该代码非常容易理解，6行代码实现了计算 sqrt(a * 100)。在下一节中，我们将会看到实现单方法接口的更简单的做法。 译者注： 在Java中只有单继承，如果要让一个类赋予新的特性，通常是使用接口来实现，在C++中支持多继承，允许一个子类同时具有多个父类的接口与功能，在其他语言中，让一个类同时具有其他的可复用代码的方法叫做mixin。新的Java 8 的这个特新在编译器实现的角度上来说更加接近Scala的trait。 在C#中也有名为扩展方法的概念，允许给已存在的类型扩展方法，和Java 8的这个在语义上有差别。 二、Lambda 表达式 首先看看在老版本的Java中是如何排列字符串的： 12345678List&lt;String&gt; names = Arrays.asList(\"peter\", \"anna\", \"mike\", \"xenia\");Collections.sort(names, new Comparator&lt;String&gt;() &#123; @Override public int compare(String a, String b) &#123; return b.compareTo(a); &#125;&#125;); 只需要给静态方法 Collections.sort 传入一个List对象以及一个比较器来按指定顺序排列。通常做法都是创建一个匿名的比较器对象然后将其传递给sort方法。 在Java 8 中你就没必要使用这种传统的匿名对象的方式了，Java 8提供了更简洁的语法，lambda表达式： 123 Collections.sort(names, (String a, String b) -&gt; &#123; return b.compareTo(a);&#125;); 看到了吧，代码变得更段且更具有可读性，但是实际上还可以写得更短： 1Collections.sort(names, (String a, String b) -&gt; b.compareTo(a)); 对于函数体只有一行代码的，你可以去掉大括号{}以及return关键字，但是你还可以写得更短点： 1Collections.sort(names, (a, b) -&gt; b.compareTo(a)); Java编译器可以自动推导出参数类型，所以你可以不用再写一次类型。接下来我们看看lambda表达式还能作出什么更方便的东西来： 三、函数式接口Lambda表达式是如何在java的类型系统中表示的呢？每一个lambda表达式都对应一个类型，通常是接口类型。而“函数式接口”是指仅仅只包含一个抽象方法的接口，每一个该类型的lambda表达式都会被匹配到这个抽象方法。因为 默认方法 不算抽象方法，所以你也可以给你的函数式接口添加默认方法。 我们可以将lambda表达式当作任意只包含一个抽象方法的接口类型，确保你的接口一定达到这个要求，你只需要给你的接口添加 @FunctionalInterface 注解，编译器如果发现你标注了这个注解的接口有多于一个抽象方法的时候会报错的。 1234567 @FunctionalInterfaceinterface Converter&lt;F, T&gt; &#123; T convert(F from);&#125;Converter&lt;String, Integer&gt; converter = (from) -&gt; Integer.valueOf(from);Integer converted = converter.convert(\"123\");System.out.println(converted); // 123 需要注意如果@FunctionalInterface如果没有指定，上面的代码也是对的。 译者注 将lambda表达式映射到一个单方法的接口上，这种做法在Java 8之前就有别的语言实现，比如Rhino JavaScript解释器，如果一个函数参数接收一个单方法的接口而你传递的是一个function，Rhino 解释器会自动做一个单接口的实例到function的适配器，典型的应用场景有 org.w3c.dom.events.EventTarget 的addEventListener 第二个参数 EventListener。 四、方法与构造函数引用 前一节中的代码还可以通过静态方法引用来表示： 123Converter&lt;String, Integer&gt; converter = Integer::valueOf;Integer converted = converter.convert(\"123\");System.out.println(converted); // 123 Java 8 允许你使用 :: 关键字来传递方法或者构造函数引用，上面的代码展示了如何引用一个静态方法，我们也可以引用一个对象的方法： 123converter = something::startsWith;String converted = converter.convert(\"Java\");System.out.println(converted); // \"J\" 接下来看看构造函数是如何使用::关键字来引用的，首先我们定义一个包含多个构造函数的简单类： 123456789101112class Person &#123; String firstName; String lastName; Person() &#123;&#125; Person(String firstName, String lastName) &#123; this.firstName = firstName; this.lastName = lastName; &#125;&#125; 接下来我们指定一个用来创建Person对象的对象工厂接口： 123 interface PersonFactory&lt;P extends Person&gt; &#123; P create(String firstName, String lastName);&#125; 这里我们使用构造函数引用来将他们关联起来，而不是实现一个完整的工厂： 12PersonFactory&lt;Person&gt; personFactory = Person::new;Person person = personFactory.create(\"Peter\", \"Parker\"); 我们只需要使用 Person::new 来获取Person类构造函数的引用，Java编译器会自动根据PersonFactory.create方法的签名来选择合适的构造函数。 五、Lambda 作用域 在lambda表达式中访问外层作用域和老版本的匿名对象中的方式很相似。你可以直接访问标记了final的外层局部变量，或者实例的字段以及静态变量。 六、访问局部变量 我们可以直接在lambda表达式中访问外层的局部变量： 12345final int num = 1;Converter&lt;Integer, String&gt; stringConverter = (from) -&gt; String.valueOf(from + num); stringConverter.convert(2); // 3 但是和匿名对象不同的是，这里的变量num可以不用声明为final，该代码同样正确： 12345int num = 1;Converter&lt;Integer, String&gt; stringConverter = (from) -&gt; String.valueOf(from + num); stringConverter.convert(2); // 3 不过这里的num必须不可被后面的代码修改（即隐性的具有final的语义），例如下面的就无法编译： 12345int num = 1;Converter&lt;Integer, String&gt; stringConverter = (from) -&gt; String.valueOf(from + num);num = 3; 在lambda表达式中试图修改num同样是不允许的。 七、访问对象字段与静态变量 和本地变量不同的是，lambda内部对于实例的字段以及静态变量是即可读又可写。该行为和匿名对象是一致的： 1234567891011121314151617class Lambda4 &#123; static int outerStaticNum; int outerNum; void testScopes() &#123; Converter&lt;Integer, String&gt; stringConverter1 = (from) -&gt; &#123; outerNum = 23; return String.valueOf(from); &#125;; Converter&lt;Integer, String&gt; stringConverter2 = (from) -&gt; &#123; outerStaticNum = 72; return String.valueOf(from); &#125;; &#125;&#125; 八、访问接口的默认方法 还记得第一节中的formula例子么，接口Formula定义了一个默认方法sqrt可以直接被formula的实例包括匿名对象访问到，但是在lambda表达式中这个是不行的。Lambda表达式中是无法访问到默认方法的，以下代码将无法编译： 12Formula formula = (a) -&gt; sqrt( a * 100);Built-in Functional Interfaces JDK 1.8 API包含了很多内建的函数式接口，在老Java中常用到的比如Comparator或者Runnable接口，这些接口都增加了@FunctionalInterface注解以便能用在lambda上。Java 8 API同样还提供了很多全新的函数式接口来让工作更加方便，有一些接口是来自Google Guava库里的，即便你对这些很熟悉了，还是有必要看看这些是如何扩展到lambda上使用的。 Predicate接口 Predicate 接口只有一个参数，返回boolean类型。该接口包含多种默认方法来将Predicate组合成其他复杂的逻辑（比如：与，或，非）： 12345678910Predicate&lt;String&gt; predicate = (s) -&gt; s.length() &gt; 0;predicate.test(\"foo\"); // truepredicate.negate().test(\"foo\"); // falsePredicate&lt;Boolean&gt; nonNull = Objects::nonNull;Predicate&lt;Boolean&gt; isNull = Objects::isNull;Predicate&lt;String&gt; isEmpty = String::isEmpty;Predicate&lt;String&gt; isNotEmpty = isEmpty.negate(); Function 接口 Function 接口有一个参数并且返回一个结果，并附带了一些可以和其他函数组合的默认方法（compose, andThen）： 12345Function&lt;String, Integer&gt; toInteger = Integer::valueOf;Function&lt;String, String&gt; backToString = toInteger.andThen(String::valueOf); backToString.apply(\"123\"); // \"123\" Supplier 接口 Supplier 接口返回一个任意范型的值，和Function接口不同的是该接口没有任何参数 12Supplier&lt;Person&gt; personSupplier = Person::new;personSupplier.get(); // new Person Consumer 接口 Consumer 接口表示执行在单个参数上的操作。 12Consumer&lt;Person&gt; greeter = (p) -&gt; System.out.println(\"Hello, \" + p.firstName);greeter.accept(new Person(\"Luke\", \"Skywalker\")); Comparator 接口1234567Comparator&lt;Person&gt; comparator = (p1, p2) -&gt; p1.firstName.compareTo(p2.firstName);Person p1 = new Person(\"John\", \"Doe\");Person p2 = new Person(\"Alice\", \"Wonderland\");comparator.compare(p1, p2); // &gt; 0comparator.reversed().compare(p1, p2); // &lt; 0 Optional 接口Optional 不是函数是接口，这是个用来防止NullPointerException异常的辅助类型，这是下一届中将要用到的重要概念，现在先简单的看看这个接口能干什么： Optional 被定义为一个简单的容器，其值可能是null或者不是null。在Java 8之前一般某个函数应该返回非空对象但是偶尔却可能返回了null，而在Java 8中，不推荐你返回null而是返回Optional。 1234567Optional&lt;String&gt; optional = Optional.of(\"bam\");optional.isPresent(); // trueoptional.get(); // \"bam\"optional.orElse(\"fallback\"); // \"bam\"optional.ifPresent((s) -&gt; System.out.println(s.charAt(0))); // \"b\" Stream 接口java.util.Stream 表示能应用在一组元素上一次执行的操作序列。Stream 操作分为中间操作或者最终操作两种，最终操作返回一特定类型的计算结果，而中间操作返回Stream本身，这样你就可以将多个操作依次串起来。Stream 的创建需要指定一个数据源，比如 java.util.Collection的子类，List或者Set， Map不支持。Stream的操作可以串行执行或者并行执行。 首先看看Stream是怎么用，首先创建实例代码的用到的数据List： 123456789List&lt;String&gt; stringCollection = new ArrayList&lt;&gt;();stringCollection.add(\"ddd2\");stringCollection.add(\"aaa2\");stringCollection.add(\"bbb1\");stringCollection.add(\"aaa1\");stringCollection.add(\"bbb3\");stringCollection.add(\"ccc\");stringCollection.add(\"bbb2\");stringCollection.add(\"ddd1\"); Java 8扩展了集合类，可以通过 Collection.stream() 或者 Collection.parallelStream() 来创建一个Stream。下面几节将详细解释常用的Stream操作： Filter 过滤 过滤通过一个predicate接口来过滤并只保留符合条件的元素，该操作属于中间操作，所以我们可以在过滤后的结果来应用其他Stream操作（比如forEach）。forEach需要一个函数来对过滤后的元素依次执行。forEach是一个最终操作，所以我们不能在forEach之后来执行其他Stream操作。 123456stringCollection .stream() .filter((s) -&gt; s.startsWith(\"a\")) .forEach(System.out::println); // \"aaa2\", \"aaa1\" Sort 排序 排序是一个中间操作，返回的是排序好后的Stream。如果你不指定一个自定义的Comparator则会使用默认排序。 123456stringCollection .stream() .sorted() .filter((s) -&gt; s.startsWith(\"a\")) .forEach(System.out::println); // \"aaa1\", \"aaa2\" 需要注意的是，排序只创建了一个排列好后的Stream，而不会影响原有的数据源，排序之后原数据stringCollection是不会被修改的： 12 System.out.println(stringCollection);// ddd2, aaa2, bbb1, aaa1, bbb3, ccc, bbb2, ddd1 Map 映射 中间操作map会将元素根据指定的Function接口来依次将元素转成另外的对象，下面的示例展示了将字符串转换为大写字符串。你也可以通过map来讲对象转换成其他类型，map返回的Stream类型是根据你map传递进去的函数的返回值决定的。 123456stringCollection .stream() .map(String::toUpperCase) .sorted((a, b) -&gt; b.compareTo(a)) .forEach(System.out::println); // \"DDD2\", \"DDD1\", \"CCC\", \"BBB3\", \"BBB2\", \"AAA2\", \"AAA1\" Match 匹配 Stream提供了多种匹配操作，允许检测指定的Predicate是否匹配整个Stream。所有的匹配操作都是最终操作，并返回一个boolean类型的值 1234567891011121314151617181920boolean anyStartsWithA = stringCollection .stream() .anyMatch((s) -&gt; s.startsWith(\"a\")); System.out.println(anyStartsWithA); // trueboolean allStartsWithA = stringCollection .stream() .allMatch((s) -&gt; s.startsWith(\"a\"));System.out.println(allStartsWithA); // falseboolean noneStartsWithZ = stringCollection .stream() .noneMatch((s) -&gt; s.startsWith(\"z\"));System.out.println(noneStartsWithZ); // true Count 计数 计数是一个最终操作，返回Stream中元素的个数，返回值类型是long。 1234567long startsWithB = stringCollection .stream() .filter((s) -&gt; s.startsWith(\"b\")) .count(); System.out.println(startsWithB); // 3 Reduce 规约 这是一个最终操作，允许通过指定的函数来讲stream中的多个元素规约为一个元素，规越后的结果是通过Optional接口表示的： 123456789Optional&lt;String&gt; reduced = stringCollection .stream() .sorted() .reduce((s1, s2) -&gt; s1 + \"#\" + s2);reduced.ifPresent(System.out::println);// \"aaa1#aaa2#bbb1#bbb2#bbb3#ccc#ddd1#ddd2\" 并行Streams 前面提到过Stream有串行和并行两种，串行Stream上的操作是在一个线程中依次完成，而并行Stream则是在多个线程上同时执行。 下面的例子展示了是如何通过并行Stream来提升性能： 首先我们创建一个没有重复元素的大表： 123456 int max = 1000000;List&lt;String&gt; values = new ArrayList&lt;&gt;(max);for (int i = 0; i &lt; max; i++) &#123; UUID uuid = UUID.randomUUID(); values.add(uuid.toString());&#125; 然后我们计算一下排序这个Stream要耗时多久， 串行排序： 123456789long t0 = System.nanoTime();long count = values.stream().sorted().count();System.out.println(count);long t1 = System.nanoTime();long millis = TimeUnit.NANOSECONDS.toMillis(t1 - t0);System.out.println(String.format(\"sequential sort took: %d ms\", millis)); // 串行耗时: 899 ms 并行排序： 123456789long t0 = System.nanoTime();long count = values.parallelStream().sorted().count();System.out.println(count);long t1 = System.nanoTime();long millis = TimeUnit.NANOSECONDS.toMillis(t1 - t0);System.out.println(String.format(\"parallel sort took: %d ms\", millis)); // 并行排序耗时: 472 ms上面两个代码几乎是一样的，但是并行版的快了50%之多，唯一需要做的改动就是将stream()改为parallelStream()。 Map 前面提到过，Map类型不支持stream，不过Map提供了一些新的有用的方法来处理一些日常任务。 1234567Map&lt;Integer, String&gt; map = new HashMap&lt;&gt;();for (int i = 0; i &lt; 10; i++) &#123; map.putIfAbsent(i, \"val\" + i);&#125;map.forEach((id, val) -&gt; System.out.println(val)); 以上代码很容易理解， putIfAbsent 不需要我们做额外的存在性检查，而forEach则接收一个Consumer接口来对map里的每一个键值对进行操作。 下面的例子展示了map上的其他有用的函数： 12345678910map.computeIfPresent(3, (num, val) -&gt; val + num);map.computeIfPresent(9, (num, val) -&gt; null);map.containsKey(9); // falsemap.computeIfAbsent(23, num -&gt; \"val\" + num);map.containsKey(23); // truemap.computeIfAbsent(3, num -&gt; \"bam\");map.get(3); 接下来展示如何在Map里删除一个键值全都匹配的项： 1234map.remove(3, \"val3\");map.get(3); // val33 map.remove(3, \"val33\");map.get(3); // null 另外一个有用的方法： 1map.getOrDefault(42, \"not found\"); // not found 对Map的元素做合并也变得很容易了： 1234map.merge(9, \"val9\", (value, newValue) -&gt; value.concat(newValue));map.get(9); // val9 map.merge(9, \"concat\", (value, newValue) -&gt; value.concat(newValue));map.get(9); // val9concat Merge做的事情是如果键名不存在则插入，否则则对原键对应的值做合并操作并重新插入到map中。 九、Date API java 8 在包java.time下包含了一组全新的时间日期API。新的日期API和开源的Joda-Time库差不多，但又不完全一样，下面的例子展示了这组新API里最重要的一些部分： Clock 时钟 Clock类提供了访问当前日期和时间的方法，Clock是时区敏感的，可以用来取代 System.currentTimeMillis() 来获取当前的微秒数。某一个特定的时间点也可以使用Instant类来表示，Instant类也可以用来创建老的java.util.Date对象。 123456Clock clock = Clock.systemDefaultZone();long millis = clock.millis(); Instant instant = clock.instant();Date legacyDate = Date.from(instant); // legacy java.util.Date Timezones 时区 在新API中时区使用ZoneId来表示。时区可以很方便的使用静态方法of来获取到。 时区定义了到UTS时间的时间差，在Instant时间点对象到本地日期对象之间转换的时候是极其重要的。 1234567891011System.out.println(ZoneId.getAvailableZoneIds()); // prints all available timezone ids ZoneId zone1 = ZoneId.of(\"Europe/Berlin\");ZoneId zone2 = ZoneId.of(\"Brazil/East\");System.out.println(zone1.getRules());System.out.println(zone2.getRules());// ZoneRules[currentStandardOffset=+01:00]// ZoneRules[currentStandardOffset=-03:00] LocalTime 本地时间 LocalTime 定义了一个没有时区信息的时间，例如 晚上10点，或者 17:30:15。下面的例子使用前面代码创建的时区创建了两个本地时间。之后比较时间并以小时和分钟为单位计算两个时间的时间差： 1234567891011LocalTime now1 = LocalTime.now(zone1); LocalTime now2 = LocalTime.now(zone2); System.out.println(now1.isBefore(now2)); // falselong hoursBetween = ChronoUnit.HOURS.between(now1, now2);long minutesBetween = ChronoUnit.MINUTES.between(now1, now2);System.out.println(hoursBetween); // -3System.out.println(minutesBetween); // -239 LocalTime 提供了多种工厂方法来简化对象的创建，包括解析时间字符串。 1234567891011LocalTime late = LocalTime.of(23, 59, 59);System.out.println(late); // 23:59:59 DateTimeFormatter germanFormatter = DateTimeFormatter .ofLocalizedTime(FormatStyle.SHORT) .withLocale(Locale.GERMAN);LocalTime leetTime = LocalTime.parse(\"13:37\", germanFormatter);System.out.println(leetTime); // 13:37 LocalDate 本地日期 LocalDate 表示了一个确切的日期，比如 2014-03-11。该对象值是不可变的，用起来和LocalTime基本一致。下面的例子展示了如何给Date对象加减天/月/年。另外要注意的是这些对象是不可变的，操作返回的总是一个新实例。 12345678LocalDate today = LocalDate.now();LocalDate tomorrow = today.plus(1, ChronoUnit.DAYS);LocalDate yesterday = tomorrow.minusDays(2); LocalDate independenceDay = LocalDate.of(2014, Month.JULY, 4);DayOfWeek dayOfWeek = independenceDay.getDayOfWeek();System.out.println(dayOfWeek); // FRIDAY 从字符串解析一个LocalDate类型和解析LocalTime一样简单： 12345678DateTimeFormatter germanFormatter = DateTimeFormatter .ofLocalizedDate(FormatStyle.MEDIUM) .withLocale(Locale.GERMAN); LocalDate xmas = LocalDate.parse(\"24.12.2014\", germanFormatter);System.out.println(xmas); // 2014-12-24 LocalDateTime 本地日期时间 LocalDateTime 同时表示了时间和日期，相当于前两节内容合并到一个对象上了。LocalDateTime和LocalTime还有LocalDate一样，都是不可变的。 LocalDateTime提供了一些能访问具体字段的方法。 12345678910LocalDateTime sylvester = LocalDateTime.of(2014, Month.DECEMBER, 31, 23, 59, 59);DayOfWeek dayOfWeek = sylvester.getDayOfWeek();System.out.println(dayOfWeek); // WEDNESDAYMonth month = sylvester.getMonth();System.out.println(month); // DECEMBERlong minuteOfDay = sylvester.getLong(ChronoField.MINUTE_OF_DAY);System.out.println(minuteOfDay); // 1439 只要附加上时区信息，就可以将其转换为一个时间点Instant对象，Instant时间点对象可以很容易的转换为老式的java.util.Date 123456Instant instant = sylvester .atZone(ZoneId.systemDefault()) .toInstant(); Date legacyDate = Date.from(instant);System.out.println(legacyDate); // Wed Dec 31 23:59:59 CET 2014 格式化LocalDateTime和格式化时间和日期一样的，除了使用预定义好的格式外，我们也可以自己定义格式： 12345DateTimeFormatter formatter = DateTimeFormatter.ofPattern(\"MMM dd, yyyy - HH:mm\"); LocalDateTime parsed = LocalDateTime.parse(\"Nov 03, 2014 - 07:13\", formatter);String string = formatter.format(parsed);System.out.println(string); // Nov 03, 2014 - 07:13 和java.text.NumberFormat不一样的是新版的DateTimeFormatter是不可变的，所以它是线程安全的。关于时间日期格式的详细信息：http://download.java.net/jdk8/docs/api/java/time/format/DateTimeFormatter.html 十、Annotation 注解 在Java 8中支持多重注解了，先看个例子来理解一下是什么意思。首先定义一个包装类Hints注解用来放置一组具体的Hint注解： 12345678@interface Hints &#123; Hint[] value();&#125; @Repeatable(Hints.class)@interface Hint &#123; String value();&#125; Java 8允许我们把同一个类型的注解使用多次，只需要给该注解标注一下@Repeatable即可。 例 1: 使用包装类当容器来存多个注解（老方法） 12@Hints(&#123;@Hint(\"hint1\"), @Hint(\"hint2\")&#125;)class Person &#123;&#125; 例 2：使用多重注解（新方法） 123@Hint(\"hint1\")@Hint(\"hint2\")class Person &#123;&#125; 第二个例子里java编译器会隐性的帮你定义好@Hints注解，了解这一点有助于你用反射来获取这些信息： 12345678Hint hint = Person.class.getAnnotation(Hint.class);System.out.println(hint); // null Hints hints1 = Person.class.getAnnotation(Hints.class);System.out.println(hints1.value().length); // 2Hint[] hints2 = Person.class.getAnnotationsByType(Hint.class);System.out.println(hints2.length); // 2 即便我们没有在Person类上定义@Hints注解，我们还是可以通过 getAnnotation(Hints.class) 来获取 @Hints注解，更加方便的方法是使用 getAnnotationsByType 可以直接获取到所有的@Hint注解。另外Java 8的注解还增加到两种新的target上了： 12@Target(&#123;ElementType.TYPE_PARAMETER, ElementType.TYPE_USE&#125;)@interface MyAnnotation &#123;&#125; 关于Java 8的新特性就写到这了，肯定还有更多的特性等待发掘。JDK 1.8里还有很多很有用的东西，比如Arrays.parallelSort, StampedLock和CompletableFuture等等。","categories":[],"tags":[{"name":"Java基础知识","slug":"Java基础知识","permalink":"https://liudong-code.github.io/tags/Java%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"keywords":[]},{"title":"动态代理与cglib实现的区别","slug":"动态代理与cglib实现的区别","date":"2020-03-04T03:54:58.000Z","updated":"2020-03-04T04:06:13.224Z","comments":true,"path":"2020/03/04/动态代理与cglib实现的区别/","link":"","permalink":"https://liudong-code.github.io/2020/03/04/%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%E4%B8%8Ecglib%E5%AE%9E%E7%8E%B0%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"JDK动态代理特点 Interface：对于JDK Proxy,业务类是需要一个Interface的，这是一个缺陷； Proxy：Proxy类是动态产生的，这个类在调用Proxy.newProxyInstance()方法之后，产生一个Proxy类的实力。实际上，这个Proxy类也是存在的，不仅仅是类的实例，这个Proxy类可以保存在硬盘上； Method：对于业务委托类的每个方法，现在Proxy类里面都不用静态显示出来。 InvocationHandler：这个类在业务委托类执行时，会先调用invoke方法。invoke方法在执行想要的代理操作，可以实现对业务方法的再包装。 总结： JDK动态代理类实现了InvocationHandler接口，重写的invoke方法。 JDK动态代理的基础是反射机制（method.invoke(对象，参数)）Proxy.newProxyInstance() jdk动态代理代码123456789101112131415161718192021222324252627282930313233343536373839 public class JDKProxyFactory implements InvocationHandler &#123; private Object target; public JDKProxyFactory(Object target) &#123; super(); this.target = target; &#125; // 创建代理对象 public Object createProxy() &#123; // 1.得到目标对象的类加载器 ClassLoader classLoader = target.getClass().getClassLoader(); // 2.得到目标对象的实现接口 Class&lt;?&gt;[] interfaces = target.getClass().getInterfaces(); // 3.第三个参数需要一个实现invocationHandler接口的对象 Object newProxyInstance = Proxy.newProxyInstance(classLoader, interfaces, this); return newProxyInstance; &#125; // 第一个参数:代理对象.一般不使用;第二个参数:需要增强的方法;第三个参数:方法中的参数 public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; System.out.println(\"这是增强方法前......\"); Object invoke = method.invoke(target, args); System.out.println(\"这是增强方法后......\"); return invoke; &#125; public static void main(String[] args) &#123; // 1.创建对象 FoodServiceImpl foodService = new FoodServiceImpl(); // 2.创建代理对象 JDKProxyFactory proxy = new JDKProxyFactory(foodService); // 3.调用代理对象的增强方法,得到增强后的对象 FoodService createProxy = (FoodService) proxy.createProxy(); createProxy.makeChicken(); &#125;&#125; cglib动态代理特点​ 原理是对指定的目标生成一个子类，并覆盖其中方法实现增强，但因为采用的是继承，所以不能对final修饰的类进行代理。 注意:​ jdk的动态代理只可以为接口去完成操作，而cglib它可以为没有实现接口的类去做代理，也可以为实现接口的类去做代理。 cglib动态代理代码1234567891011121314151617181920212223242526272829303132333435363738394041 public class CglibProxyFactory implements MethodInterceptor &#123; //得到目标对象 private Object target; //使用构造方法传递目标对象 public CglibProxyFactory(Object target) &#123; super(); this.target = target; &#125; //创建代理对象 public Object createProxy()&#123; //1.创建Enhancer Enhancer enhancer = new Enhancer(); //2.传递目标对象的class enhancer.setSuperclass(target.getClass()); //3.设置回调操作 enhancer.setCallback(this); return enhancer.create(); &#125; //参数一:代理对象;参数二:需要增强的方法;参数三:需要增强方法的参数;参数四:需要增强的方法的代理 public Object intercept(Object proxy, Method method, Object[] args, MethodProxy methodProxy) throws Throwable &#123; System.out.println(\"这是增强方法前......\"); Object invoke = methodProxy.invoke(target, args); System.out.println(\"这是增强方法后......\"); return invoke; &#125; public static void main(String[] args) &#123; // 1.创建对象 FoodServiceImpl foodService = new FoodServiceImpl(); // 2.创建代理对象 CglibProxyFactory proxy = new CglibProxyFactory(foodService); // 3.调用代理对象的增强方法,得到增强后的对象 FoodService createProxy = (FoodService) proxy.createProxy(); createProxy.makeChicken(); &#125;&#125;","categories":[],"tags":[{"name":"Java基础知识","slug":"Java基础知识","permalink":"https://liudong-code.github.io/tags/Java%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"keywords":[]},{"title":"单例设计模式","slug":"单例设计模式","date":"2020-03-04T02:29:37.000Z","updated":"2020-03-04T03:51:24.448Z","comments":true,"path":"2020/03/04/单例设计模式/","link":"","permalink":"https://liudong-code.github.io/2020/03/04/%E5%8D%95%E4%BE%8B%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"1、单例模式：在当前系统中，某个类型的对象，最多只能有一个，就需要使用单例设计模式 2、单例模式的设计原则：1、构造方法私有化2、在类中创建好该类对象3、在类中，给外界提供获取该对象的公有方式 2.1饿汉式在加载类的同时，就要初始化静态成员变量，所以就同时将该类对象创建出来了饿汉式：一有机会，马上就吃，不去等待。（一旦加载类型，马上创建对象） 12345678910111213141516171819202122//单例设计模式之饿汉式//能多早创建对象就多早创建对象public class Demo1_SingtonleHunger &#123; public static void main(String[] args) &#123; SingtonHunger sh1 = SingtonHunger.getInstance(); SingtonHunger sh2 = SingtonHunger.getInstance(); System.out.println(sh1==sh2); &#125;&#125;class SingtonHunger&#123; //1.先私有构造方法 private SingtonHunger()&#123;&#125; //2.私有创建对象的方法 private static SingtonHunger sh = new SingtonHunger(); //3.通过创建对外界可见的方法来调用构造方法 public static SingtonHunger getInstance()&#123; return sh; &#125;&#125; ​ 一上来就把单例对象创建出来了，要用的时候直接返回即可，这种可以说是单例模式中最简单的一种实现方式。但是问题也比较明显。单例在还没有使用到的时候，初始化就已经完成了。也就是说，如果程序从头到位都没用使用这个单例的话，单例的对象还是会创建。这就造成了不必要的资源浪费。所以不推荐这种实现方式。 2.2懒汉式在加载类的时候，不同时创建该类对象，等到需要获取这个对象时，才去创建这个对象懒汉式：不着急、能不创建的时候，就不创建，能拖就拖 注意事项：1、只有在sl == null的时候，才会创建对象2、sl的判断和sl的赋值，不希望分离开，否则在多线程环境下，会出现多个对象的状态，所以sl的判断和sl的赋值，需要放到一个同步代码块中。3、同步代码块的效率非常低，不是每次获取对象的时候，都需要判断锁对象，只有在sl为null的时候， 才应该判断锁对象，因此在外层需要嵌套一个if判断，判断sl是否为null 123456789101112131415161718192021222324252627282930313233//单例模式之懒汉式//能多晚创建对象就多晚创建对象public class Demo2_SingletonLazy &#123; public static void main(String[] args) &#123; SingtonLazy sl1 = SingtonLazy.getInstance(); SingtonLazy sl2 = SingtonLazy.getInstance(); System.out.println(sl1==sl2); &#125;&#125; class SingtonLazy &#123; // 1.私有构造方法 private SingtonLazy() &#123; &#125; // 2.私有创建对象的引用 private static SingtonLazy sl; // 3.提供对外公开的方法创建对象 public static SingtonLazy getInstance() &#123; //最外层判断是否为空用来提高效率 if (sl == null) &#123; synchronized (SingtonLazy.class) &#123; //加上同步锁提高线程安全 if (sl == null) &#123; sl = new SingtonLazy(); &#125; &#125; &#125; return sl; &#125;&#125; 另外一种懒汉式 public class SingletonDemo2 { 1234567891011121314//类初始化时，不初始化这个对象(延时加载，真正用的时候再创建)private static SingletonDemo2 instance; //构造器私有化private SingletonDemo2()&#123;&#125; //方法同步，调用效率低public static synchronized SingletonDemo2 getInstance()&#123; if(instance==null)&#123; instance=new SingletonDemo2(); &#125; return instance; &#125;&#125; 这种如果是多线程的，会有JVM 指令重排的机制，破坏其单例类，所以双重锁判断机制好一些 2.3静态内部类实现123456789101112public class SingletonDemo3 &#123; private static class SingletonClassInstance&#123; private static final SingletonDemo3 instance=new SingletonDemo3(); &#125; private SingletonDemo3()&#123;&#125; public static SingletonDemo3 getInstance()&#123; return SingletonClassInstance.instance; &#125; &#125; 2.4枚举类​ 1234567public enum SingletonDemo4 &#123; //枚举元素本身就是单例 INSTANCE; //添加自己需要的操作 public void singletonOperation()&#123; &#125;&#125; ​","categories":[],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://liudong-code.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"keywords":[]},{"title":"深拷贝和浅拷贝","slug":"深拷贝和浅拷贝","date":"2020-03-04T02:01:02.000Z","updated":"2020-03-04T02:20:23.508Z","comments":true,"path":"2020/03/04/深拷贝和浅拷贝/","link":"","permalink":"https://liudong-code.github.io/2020/03/04/%E6%B7%B1%E6%8B%B7%E8%B4%9D%E5%92%8C%E6%B5%85%E6%8B%B7%E8%B4%9D/","excerpt":"","text":"介绍开发过程中，有时会遇到把现有的一个对象的所有成员属性拷贝给另一个对象的需求。 比如说对象 A 和对象 B，二者都是 ClassC 的对象，具有成员变量 a 和 b，现在对对象 A 进行拷贝赋值给 B，也就是 B.a = A.a; B.b = A.b; 这时再去改变 B 的属性 a 或者 b 时，可能会遇到问题：假设 a 是基础数据类型，b 是引用类型。 当改变 B.a 的值时，没有问题； 当改变 B.b 的值时，同时也会改变 A.b 的值，因为其实上面的例子中只是把 A.b 赋值给了 B.b，因为是 b 引用类型的，所以它们是指向同一个地址的。这可能就会给我们使用埋下隐患。 1Java 中的数据类型分为基本数据类型和引用数据类型。对于这两种数据类型，在进行赋值操作、用作方法参数或返回值时，会有值传递和引用（地址）传递的差别。 拷贝分类上面的问题，其实就是因为对拷贝的不熟悉导致的。 根据对对象属性的拷贝程度（基本数据类和引用类型），会分为两种： 浅拷贝 (Shallow Copy) 深拷贝 (Deep Copy) 浅拷贝1. 浅拷贝介绍浅拷贝是按位拷贝对象，它会创建一个新对象，这个对象有着原始对象属性值的一份精确拷贝。如果属性是基本类型，拷贝的就是基本类型的值；如果属性是内存地址（引用类型），拷贝的就是内存地址 ，因此如果其中一个对象改变了这个地址，就会影响到另一个对象。即默认拷贝构造函数只是对对象进行浅拷贝复制(逐个成员依次拷贝)，即只复制对象空间而不复制资源。 2. 浅拷贝特点(1) 对于基本数据类型的成员对象，因为基础数据类型是值传递的，所以是直接将属性值赋值给新的对象。基础类型的拷贝，其中一个对象修改该值，不会影响另外一个。 (2) 对于引用类型，比如数组或者类对象，因为引用类型是引用传递，所以浅拷贝只是把内存地址赋值给了成员变量，它们指向了同一内存空间。改变其中一个，会对另外一个也产生影响。 3. 浅拷贝的实现实现对象拷贝的类，需要实现 Cloneable 接口，并覆写 clone() 方法。 123456789101112131415161718192021public class Subject &#123; private String name; public Subject(String name) &#123; this.name = name; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; @Override public String toString() &#123; return \"[Subject: \" + this.hashCode() + \",name:\" + name + \"]\"; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class Student implements Cloneable &#123; //引用类型 private Subject subject; //基础数据类型 private String name; private int age; public Subject getSubject() &#123; return subject; &#125; public void setSubject(Subject subject) &#123; this.subject = subject; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; /** * 重写clone()方法 * @return */ @Override public Object clone() &#123; //浅拷贝 try &#123; // 直接调用父类的clone()方法 return super.clone(); &#125; catch (CloneNotSupportedException e) &#123; return null; &#125; &#125; @Override public String toString() &#123; return \"[Student: \" + this.hashCode() + \",subject:\" + subject + \",name:\" + name + \",age:\" + age + \"]\"; &#125;&#125; 1234567891011121314public class ShallowCopy &#123; public static void main(String[] args) &#123; Subject subject = new Subject(\"yuwen\"); Student studentA = new Student(); studentA.setSubject(subject); studentA.setName(\"Lynn\"); studentA.setAge(20); Student studentB = (Student) studentA.clone(); studentB.setName(\"Lily\"); studentB.setAge(18); Subject subjectB = studentB.getSubject(); subjectB.setName(\"lishi\"); System.out.println(\"studentA:\" + studentA.toString()); System.out.println(\"studentB:\" + studentB.toString()); 输出的结果 12studentA:[Student: 460141958,subject:[Subject:1163157884,name:lishi],name:Lynn,age:20]studentB:[Student: 1956725890,subject[Subject:1163157884,name:lishi],name:Lily,age:18] 深拷贝1. 深拷贝介绍通过上面的例子可以看到，浅拷贝会带来数据安全方面的隐患，例如我们只是想修改了 studentB 的 subject，但是 studentA 的 subject 也被修改了，因为它们都是指向的同一个地址。所以，此种情况下，我们需要用到深拷贝 1深拷贝，在拷贝引用类型成员变量时，为引用类型的数据成员另辟了一个独立的内存空间，实现真正内容上的拷贝。 2. 深拷贝特点(1) 对于基本数据类型的成员对象，因为基础数据类型是值传递的，所以是直接将属性值赋值给新的对象。基础类型的拷贝，其中一个对象修改该值，不会影响另外一个（和浅拷贝一样）。 (2) 对于引用类型，比如数组或者类对象，深拷贝会新建一个对象空间，然后拷贝里面的内容，所以它们指向了不同的内存空间。改变其中一个，不会对另外一个也产生影响。 (3) 对于有多层对象的，每个对象都需要实现 Cloneable 并重写 clone() 方法，进而实现了对象的串行层层拷贝。 (4) 深拷贝相比于浅拷贝速度较慢并且花销较大。 3. 深拷贝的实现对于 Student 的引用类型的成员变量 Subject ，需要实现 Cloneable 并重写 clone() 方法。 123456789101112131415161718192021222324252627public class Subject implements Cloneable &#123; private String name; public Subject(String name) &#123; this.name = name; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; @Override protected Object clone() throws CloneNotSupportedException &#123; //Subject 如果也有引用类型的成员属性，也应该和 Student 类一样实现 return super.clone(); &#125; @Override public String toString() &#123; return \"[Subject: \" + this.hashCode() + \",name:\" + name + \"]\"; &#125;&#125; 在 Student 的 clone() 方法中，需要拿到拷贝自己后产生的新的对象，然后对新的对象的引用类型再调用拷贝操作，实现对引用类型成员变量的深拷贝。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class Student implements Cloneable &#123; //引用类型 private Subject subject; //基础数据类型 private String name; private int age; public Subject getSubject() &#123; return subject; &#125; public void setSubject(Subject subject) &#123; this.subject = subject; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; /** * 重写clone()方法 * @return */ @Override public Object clone() &#123; //深拷贝 try &#123; // 直接调用父类的clone()方法 Student student = (Student) super.clone(); student.subject = (Subject) subject.clone(); return student; &#125; catch (CloneNotSupportedException e) &#123; return null; &#125; &#125; @Override public String toString() &#123; return \"[Student: \" + this.hashCode() + \",subject:\" + subject + \",name:\" + name + \",age:\" + age + \"]\"; &#125;&#125; 一样的使用方式 12345678910111213141516public class ShallowCopy &#123; public static void main(String[] args) &#123; Subject subject = new Subject(\"yuwen\"); Student studentA = new Student(); studentA.setSubject(subject); studentA.setName(\"Lynn\"); studentA.setAge(20); Student studentB = (Student) studentA.clone(); studentB.setName(\"Lily\"); studentB.setAge(18); Subject subjectB = studentB.getSubject(); subjectB.setName(\"lishi\"); System.out.println(\"studentA:\" + studentA.toString()); System.out.println(\"studentB:\" + studentB.toString()); &#125;&#125; 输出结果： 12studentA:[Student: 460141958,subject:[Subject:1163157884,name:yuwen],name:Lynn,age:20]studentB:[Student: 1956725890,subject:[Subject:356573597,name:lishi],name:Lily,age:18] 由输出结果可见，深拷贝后，不管是基础数据类型还是引用类型的成员变量，修改其值都不会相互造成影响。","categories":[],"tags":[{"name":"Java基础知识","slug":"Java基础知识","permalink":"https://liudong-code.github.io/tags/Java%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"keywords":[]},{"title":"java内存溢出","slug":"java内存溢出","date":"2020-03-02T02:16:07.000Z","updated":"2020-03-02T02:56:53.160Z","comments":true,"path":"2020/03/02/java内存溢出/","link":"","permalink":"https://liudong-code.github.io/2020/03/02/java%E5%86%85%E5%AD%98%E6%BA%A2%E5%87%BA/","excerpt":"","text":"–本文源自于《深入理解Java虚拟机》 概述​ 在Java虚拟机规范描述中，除了程序计数器以外，虚拟机内存的其他几个运行时区域都有发生OutOfMemoryError（OOM）异常的可能。 Java堆溢出​ Java堆用于储存对象实例，只要不断的创建对象，并且保证GCRoots到对象之间有可达路径来避免垃圾回收机制清除这些对象，那么在对象到达最大堆的容量之后，就会产生内存溢出异常。 java.lang.OutOfMem0ryError:Java heap space 123456789101112131415161718/**Vm Args:-Xms20m XMx20m -XX:+HeapDumpOnOUtOFMemoryError*/public class JvmTest &#123; public static void main(String[] args) &#123; List&lt;String&gt; aList = new ArrayList&lt;String&gt;(); try&#123; while(true)&#123; aList.add(\"asdasdasdas\"); &#125; &#125;catch(Throwable e)&#123; System.out.println(aList.size()); e.printStackTrace(); &#125; &#125;&#125; 解决手段：​ 先通过内存映像分析工具（如：Eclipse Memory Analyzer）堆Dump出来的堆转储快照进行分析，重点确认内存中的对象是否是必要的，也就是药神分清楚到底是出现了内存泄漏还是内存溢出。 ​ 如果是内存泄漏，可以进一步通过工具查看泄露对象到GC Roots的引用链，于是就能找到泄漏对象是通过怎样的路径与GC Roots 相关联并导致垃圾回收器无法自动回收他们的。掌握了泄漏对象的类型信息，及GCRoots引用链的信息，就可以比较准确的定位出泄漏代码的 位置。 ​ 如果不是泄漏，就是在内存中对象还活着，应当检查虚拟机的堆参数（-Xmx与Xms） 虚拟机栈和本地方法栈溢出​ Java虚拟机规范中描述了两种异常 如果线程请求的栈深度大于虚拟机所允许的最大深度，将抛出StackOverflowError异常。 如果虚拟机在扩展栈时无法申请到足够的内存空间，则抛出OutOfMemoryError异常。 123456789101112public class JvmTest &#123; private int i = 0; public void a()&#123; System.out.println(i++); a(); &#125; public static void main(String[] args) &#123; JvmTest j = new JvmTest(); j.a(); &#125;｝ 方法区和运行时常量池溢出–待补 本机直接内存溢出–待补","categories":[],"tags":[{"name":"JVM","slug":"JVM","permalink":"https://liudong-code.github.io/tags/JVM/"}],"keywords":[]},{"title":"throw和throws的区别","slug":"throw和throws的区别","date":"2020-02-29T11:12:48.000Z","updated":"2020-02-29T11:18:05.497Z","comments":true,"path":"2020/02/29/throw和throws的区别/","link":"","permalink":"https://liudong-code.github.io/2020/02/29/throw%E5%92%8Cthrows%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"抛出异常有三种形式 throw throws 系统自动抛异常 一、系统自动抛异常 当程序语句出现一些逻辑错误、主义错误或类型转换错误时，系统会自动抛出异常：（举个栗子） 12345public static void main(String[] args) &#123; int a = 5, b =0; System.out.println(5/b); // 此处系统会自动抛出ArithmeticException异常 //function(); &#125; 二、throw throw是语句抛出一个异常，一般是在代码块的内部，当程序出现某种逻辑错误时由程序员主动抛出某种特定类型的异常 123456789public static void main(String[] args) &#123; String s = \"abc\"; if(s.equals(\"abc\")) &#123; throw new NumberFormatException(); &#125; else &#123; System.out.println(s); &#125; //function(); &#125; 运行时，系统会抛出如下异常： 1Exception in thread \"main\" java.lang.NumberFormatException at...... 三、throws 12345678910111213141516当某个方法可能会抛出某种异常时用于throws 声明可能抛出的异常，然后交给上层调用它的方法程序处理public class testThrows()&#123;public static void function() throws NumberFormatException &#123; String s = \"abc\"; System.out.println(Double.parseDouble(s)); &#125; public static void main(String[] args) &#123; try &#123; function(); &#125; catch (NumberFormatException e) &#123; System.err.println(\"非数据类型不能强制类型转换。\"); //e.printStackTrace(); &#125; &#125; 四、throw与throws的比较 throws出现在方法函数头；而throw出现在函数体。throws表示出现异常的一种可能性，并不一定会发生这些异常；throw则是抛出了异常，执行throw则一定抛出了某种异常对象。两者都是消极处理异常的方式（这里的消极并不是说这种方式不好），只是抛出或者可能抛出异常，但是不会由函数去处理异常，真正的处理异常由函数的上层调用处理。 五、编程习惯： 在写程序时，对可能会出现异常的部分通常要用try{…}catch{…}去捕捉它并对它进行处理；用try{…}catch{…}捕捉了异常之后一定要对在catch{…}中对其进行处理，那怕是最简单的一句输出语句，或栈输入e.printStackTrace();用try{…}catch{…}捕捉了异常之后一定要对在catch{…}中对其进行处理，那怕是最简单的一句输出语句，或栈输入e.printStackTrace();如果是捕捉IO输入输出流中的异常，一定要在try{…}catch{…}后加finally{…}把输入输出流关闭；如果是捕捉IO输入输出流中的异常，一定要在try{…}catch{…}后加finally{…}把输入输出流关闭；如果在函数体内用throw抛出了某种异常，最好要在函数名中加throws抛异常声明，然后交给调用它的上层函数进行处理。如果在函数体内用throw抛出了某种异常，最好要在函数名中加throws抛异常声明，然后交给调用它的上层函数进行处理 原文链接：https://blog.csdn.net/xsj_blog/article/details/83030450 原文链接：https://blog.csdn.net/xsj_blog/article/details/83030450","categories":[],"tags":[{"name":"Java基础知识","slug":"Java基础知识","permalink":"https://liudong-code.github.io/tags/Java%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"keywords":[]},{"title":"java中的异常链","slug":"java中的异常链","date":"2020-02-29T11:07:08.000Z","updated":"2020-02-29T11:11:28.590Z","comments":true,"path":"2020/02/29/java中的异常链/","link":"","permalink":"https://liudong-code.github.io/2020/02/29/java%E4%B8%AD%E7%9A%84%E5%BC%82%E5%B8%B8%E9%93%BE/","excerpt":"","text":"概念​ 把捕获的异常包装成一个新的异常，在新的异常中添加对新的异常的引用，再把新异常抛出，就像是链式反应一样，这种就叫异常链。 123456789101112131415161718192021222324252627282930313233343536373839public static void main(String[] args)&#123; ChainTest ct=new ChainTest();//创建chainTest实例 try&#123; ct.test2(); &#125;catch(Exception e)&#123; e.printStackTrace(); &#125;&#125;public void test1()throws DrunkException&#123; throw new DrunkException(\"喝车别开酒\");&#125;public void test2()&#123; try&#123; test1();&#125;catch (DrunkException e)&#123; RuntimeException newExc=new RuntimeException(\"司机一滴酒亲人两行泪\")；//含参构造器 newExc.initCause(e);//调用newExc的init方法，把捕获的DrunkException传进去 throw newExc;//抛出新异常 &#125;&#125;","categories":[],"tags":[{"name":"Java基础知识","slug":"Java基础知识","permalink":"https://liudong-code.github.io/tags/Java%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"keywords":[]},{"title":"Java的异常体系","slug":"Java的异常体系","date":"2020-02-29T10:54:18.000Z","updated":"2020-02-29T11:20:57.893Z","comments":true,"path":"2020/02/29/Java的异常体系/","link":"","permalink":"https://liudong-code.github.io/2020/02/29/Java%E7%9A%84%E5%BC%82%E5%B8%B8%E4%BD%93%E7%B3%BB/","excerpt":"","text":"java中异常的体系结构图解： ​ java中的Exception类的子类不仅仅只是像上图所示只包含IOException和RuntimeException这两大类，事实上Exception的子类很多很多，主要可概括为：运行时异常与非运行时异常。 java异常体系结构​ Thorwable类（表示可抛出）是所有异常和错误的超类，两个直接子类为Error和Exception，分别表示错误和异常。其中异常类Exception又分为运行时异常(RuntimeException)和非运行时异常， 这两种异常有很大的区别，也称之为不检查异常（Unchecked Exception）和检查异常（Checked Exception）。下面将详细讲述这些异常之间的区别与联系： 1、Error与Exception Error是程序无法处理的错误，它是由JVM产生和抛出的，比如OutOfMemoryError、ThreadDeath等。这些异常发生时，Java虚拟机（JVM）一般会选择线程终止。 Exception是程序本身可以处理的异常，这种异常分两大类运行时异常和非运行时异常。程序中应当尽可能去处理这些异常。2、运行时异常和非运行时异常 运行时异常都是RuntimeException类及其子类异常，如NullPointerException、IndexOutOfBoundsException等，这些异常是不检查异常，程序中可以选择捕获处理，也可以不处理。这些异常一般是由程序逻辑错误引起的，程序应该从逻辑角度尽可能避免这类异常的发生。 非运行时异常是RuntimeException以外的异常，类型上都属于Exception类及其子类。从程序语法角度讲是必须进行处理的异常，如果不处理，程序就不能编译通过。如IOException、SQLException等以及用户自定义的Exception异常，一般情况下不自定义检查异常。 异常的捕获和处理try、catch、finally 第一：try、catch、finally三个语句块均不能单独使用，三者可以组成 try…catch…finally、try…catch、try…finally三种结构，catch语句可以有一个或多个，finally语句最多一个。 第二：try、catch、finally三个代码块中变量的作用域为代码块内部，分别独立而不能相互访问。如果要在三个块中都可以访问，则需要将变量定义到这些块的外面。 第三：多个catch块时候，最多只会匹配其中一个异常类且只会执行该catch块代码，而不会再执行其它的catch块，且匹配catch语句的顺序为从上到下，也可能所有的catch都没执行。 第四：先Catch子类异常再Catch父类异常。 throw、throws关键字throw关键字是用于方法体内部，用来抛出一个Throwable类型的异常。如果抛出了检查异常，则还应该在方法头部声明方法可能抛出的异常类型。该方法的调用者也必须检查处理抛出的异常。如果所有方法都层层上抛获取 的异常，最终JVM会进行处理，处理也很简单，就是打印异常消息和堆栈信息。throw关键字用法如下： 123public static void test() throws Exception &#123; throw new Exception(\"方法test中的Exception\"); &#125;","categories":[],"tags":[{"name":"Java基础知识","slug":"Java基础知识","permalink":"https://liudong-code.github.io/tags/Java%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"keywords":[]},{"title":"如何格式化日期","slug":"如何格式化日期","date":"2020-02-29T10:42:07.000Z","updated":"2020-02-29T11:24:29.516Z","comments":true,"path":"2020/02/29/如何格式化日期/","link":"","permalink":"https://liudong-code.github.io/2020/02/29/%E5%A6%82%E4%BD%95%E6%A0%BC%E5%BC%8F%E5%8C%96%E6%97%A5%E6%9C%9F/","excerpt":"","text":"1234DateTimeFormatter dateTimeFormatter = DateTimeFormatter.ofPattern(\"yyyy-MM-dd\"); LocalDate localDate = LocalDate.now(); String date = localDate.format(dateTimeFormatter); System.out.println(\"date:\"+date)； 补充:java的时间日期API一直以来都是被诟病的东西,为了解决这一问题,java8中引入了新的时间日期API,其中包括LocalDate,LocalTime,LocalDate,LocalDateTime,Clock,Instant等类,这些的类的设计都使用了不变模式,因此是线程安全的设计. 原文链接：https://blog.csdn.net/riju4713/article/details/88220120","categories":[],"tags":[{"name":"Java基础知识","slug":"Java基础知识","permalink":"https://liudong-code.github.io/tags/Java%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"keywords":[]},{"title":"静态属性和静态方法是否可以被继承？是否可以被重写？以及原因？","slug":"静态属性和静态方法是否可以被继承？是否可以被重写？以及原因？","date":"2020-02-29T10:35:58.000Z","updated":"2020-02-29T10:38:33.433Z","comments":true,"path":"2020/02/29/静态属性和静态方法是否可以被继承？是否可以被重写？以及原因？/","link":"","permalink":"https://liudong-code.github.io/2020/02/29/%E9%9D%99%E6%80%81%E5%B1%9E%E6%80%A7%E5%92%8C%E9%9D%99%E6%80%81%E6%96%B9%E6%B3%95%E6%98%AF%E5%90%A6%E5%8F%AF%E4%BB%A5%E8%A2%AB%E7%BB%A7%E6%89%BF%EF%BC%9F%E6%98%AF%E5%90%A6%E5%8F%AF%E4%BB%A5%E8%A2%AB%E9%87%8D%E5%86%99%EF%BC%9F%E4%BB%A5%E5%8F%8A%E5%8E%9F%E5%9B%A0%EF%BC%9F/","excerpt":"","text":"可以被继承，代码如下123456789101112131415161718192021222324252627public class One &#123; //静态属性和静态方法是否可以被继承？ public static String one_1 = \"one\"; public static void oneFn() &#123; System.out.println(\"oneFn\"); &#125;&#125;public class Two extends One&#123; //空&#125;public class MyTest &#123; //静态属性和静态方法是否可以被继承？是否可以被重写？以及原因？ public static void main(String[] args) &#123; One one = new Two(); one.oneFn(); String one_1 = One.one_1; System.out.println(\"One.one_1&gt;&gt;&gt;&gt;&gt;&gt;&gt;\"+one_1); String one_12 = one.one_1; System.out.println(\"one.one_1&gt;&gt;&gt;&gt;&gt;&gt;&gt;\"+one_12); &#125;&#125;//打印结果如下oneFnOne.one_1&gt;&gt;&gt;&gt;&gt;&gt;&gt;oneone.one_1&gt;&gt;&gt;&gt;&gt;&gt;&gt;one 不能被重写，代码如下123456789101112131415161718192021222324252627282930313233public class One &#123; //静态属性和静态方法是否可以被重写？以及原因？ public static String one_1 = \"one\"; public static void oneFn() &#123; System.out.println(\"oneFn\"); &#125;&#125;public class Two extends One &#123; public static String one_1 = \"two\"; public static void oneFn() &#123; System.out.println(\"TwoFn\"); &#125;&#125;public class MyTest &#123; //静态属性和静态方法是否可以被继承？是否可以被重写？以及原因？ public static void main(String[] args) &#123; One one = new Two(); one.oneFn(); String one_1 = One.one_1; System.out.println(\"One.one_1&gt;&gt;&gt;&gt;&gt;&gt;&gt;\"+one_1); String one_12 = one.one_1; System.out.println(\"one.one_1&gt;&gt;&gt;&gt;&gt;&gt;&gt;\"+one_12); &#125;&#125;//打印结果如下//oneFn//One.one_1&gt;&gt;&gt;&gt;&gt;&gt;&gt;one//one.one_1&gt;&gt;&gt;&gt;&gt;&gt;&gt;one 原因static修饰函数/变量时，其实是全局函数/变量，它只是因为java强调对象的要挂，它与任何类都没有关系。靠这个类的好处就是这个类的成员函数调用static方法不用带类名。 注意：static关键字可以用修饰代码块.static代码块可以置于类中的任何一个位置，并可以有多个static代码块。在类初次被加载时，会按照静态代码块的顺序来执行，并且只会执行一次。","categories":[],"tags":[{"name":"Java基础知识","slug":"Java基础知识","permalink":"https://liudong-code.github.io/tags/Java%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"keywords":[]},{"title":"string转换成integer的方式及原理","slug":"string转换成integer的方式及原理","date":"2020-02-29T10:04:02.000Z","updated":"2020-02-29T10:23:45.563Z","comments":true,"path":"2020/02/29/string转换成integer的方式及原理/","link":"","permalink":"https://liudong-code.github.io/2020/02/29/string%E8%BD%AC%E6%8D%A2%E6%88%90integer%E7%9A%84%E6%96%B9%E5%BC%8F%E5%8F%8A%E5%8E%9F%E7%90%86/","excerpt":"","text":"string转换成integer的方式及原理1. Integer.parseInt（String str）方法​ 源码如下 123public static int parseInt(String s) throws NumberFormatException &#123; return parseInt(s,10);&#125; 2. Integer.parseInt（String s, int radix）方法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879public static int parseInt(String s, int radix) throws NumberFormatException&#123; /* * WARNING: This method may be invoked early during VM initialization * before IntegerCache is initialized. Care must be taken to not use * the valueOf method. */ if (s == null) &#123; throw new NumberFormatException(\"null\"); &#125; //基数是否小于最小基数 if (radix &lt; Character.MIN_RADIX) &#123; throw new NumberFormatException(\"radix \" + radix + \" less than Character.MIN_RADIX\"); &#125; if (radix &gt; Character.MAX_RADIX) &#123; throw new NumberFormatException(\"radix \" + radix + \" greater than Character.MAX_RADIX\"); &#125; int result = 0; //是否是负数 boolean negative = false; int i = 0, len = s.length(); //最大值限制 int limit = -Integer.MAX_VALUE; int multmin; int digit; //判断字符长度是否大于0，否则抛出异常 if (len &gt; 0) &#123; //第一个字符是否是符号 char firstChar = s.charAt(0); //根据ascii码表看出加号(43)和负号(45)对应的 //十进制数小于‘0’(48)的 if (firstChar &lt; '0') &#123; // Possible leading \"+\" or \"-\" //是负号 if (firstChar == '-') &#123; //负号属性设置为true negative = true; limit = Integer.MIN_VALUE; //不是负号也不是加号则抛出异常 &#125; else if (firstChar != '+') throw NumberFormatException.forInputString(s); //如果有符号（加号或者减号）且字符串长度为1，则抛出异常 if (len == 1) // Cannot have lone \"+\" or \"-\" throw NumberFormatException.forInputString(s); i++; &#125; multmin = limit / radix; while (i &lt; len) &#123; // Accumulating negatively avoids surprises near MAX_VALUE //此方法为确定数字的的十进制值 digit = Character.digit(s.charAt(i++),radix); //小于0，则为非数值字符串 if (digit &lt; 0) &#123; throw NumberFormatException.forInputString(s); &#125; //result第一次为0，第一次肯定为true if (result &lt; multmin) &#123; throw NumberFormatException.forInputString(s); &#125; //result乘以基数（10）为得到位置 //例如第一次的result为-1，第二次乘以10后为-10 //下面再-=digit（例如：1）则得到-11 //以此类推 result *= radix; if (result &lt; limit + digit) &#123; throw NumberFormatException.forInputString(s); &#125; //第一次result为0 -=digit则为负值的该digit result -= digit; &#125; &#125; else &#123; throw NumberFormatException.forInputString(s); &#125; //根据上面得到的是否负数，返回相应的值 return negative ? result : -result; &#125; 3.Character.digit(char ch, int radix)方法12345678910111213141516171819202122232425262728public static int digit(int codePoint, int radix) &#123; //基数必须再最大和最小基数之间 if (radix &lt; MIN_RADIX || radix &gt; MAX_RADIX) &#123; return -1; &#125; if (codePoint &lt; 128) &#123; // Optimized for ASCII int result = -1; //字符在0-9字符之间 if ('0' &lt;= codePoint &amp;&amp; codePoint &lt;= '9') &#123; result = codePoint - '0'; &#125; //字符在a-z之间 else if ('a' &lt;= codePoint &amp;&amp; codePoint &lt;= 'z') &#123; result = 10 + (codePoint - 'a'); &#125; //字符在A-Z之间 else if ('A' &lt;= codePoint &amp;&amp; codePoint &lt;= 'Z') &#123; result = 10 + (codePoint - 'A'); &#125; //通过判断result和基数大小，输出对应值 //通过我们parseInt对应的基数值为10， //所以，只能在第一个判断（字符在0-9字符之间） //中得到result值 否则后续程序会抛出异常 return result &lt; radix ? result : -1; &#125; return digitImpl(codePoint, radix); &#125; 总结：integer.parseInt(string str)方法调用Integer内部的 parseInt(string str,10)方法,默认基数为10，parseInt内部首先 判断字符串是否包含符号（-或者+），则对相应的negative和limit进行 赋值，然后再循环字符串，对单个char进行数值计算Character.digit(char ch, int radix) 在这个方法中，函数肯定进入到0-9字符的判断（相对于string转换到int）， 否则会抛出异常，数字就是如上面进行拼接然后生成的int类型数值。","categories":[],"tags":[{"name":"Java基础知识","slug":"Java基础知识","permalink":"https://liudong-code.github.io/tags/Java%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"keywords":[]},{"title":"序列化的方式","slug":"序列化的方式","date":"2020-02-29T03:23:06.000Z","updated":"2020-02-29T09:53:38.621Z","comments":true,"path":"2020/02/29/序列化的方式/","link":"","permalink":"https://liudong-code.github.io/2020/02/29/%E5%BA%8F%E5%88%97%E5%8C%96%E7%9A%84%E6%96%B9%E5%BC%8F/","excerpt":"","text":"概念序列化：将对象转化成一个字节序列，便于储存。 反序列化：将字节化的字节序列还原。 优点：可以实现对象的”持久性”， 所谓持久性就是指对象的生命周期不取决于程序。 序列化的几种方式Java原生的序列化**序列化需要：** 所需类：ObjectInputStream和ObjectOutputStream 方法： readObject()和writeObject(); 隐式序列化​ 实现Serializabie接口，通过实现Serializable接口，这种是隐式序列化(不需要手动)，这种是最简单的序列化方式，会自动序列化所有非static和 transient关键字修饰的成员变量。 ​ 12345678910111213141516171819202122232425262728public class IpConfig &#123; private String name; private int age; public static int virualAge = 123; private transient String ipAdress =\"127.0.0.1\"; public IpConfig(String name, int i) &#123; this.name=name; this.age=i; &#125; public String toString() &#123; return \"name: \" + name + \"\\n\" +\"age: \" + age + \"\\n\" +\"virualAge: \" + virualAge + \"\\n\" + \"ipAdress: \" + ipAdress; &#125; public void SetAge(int age) &#123; this.age = age; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637public class test &#123; public static void main(String[] args) throws IOException, ClassNotFoundException &#123; //创建可序列化对象 System.out.println(\"原来的对象：\"); IpConfig ipConfig = new IpConfig(\"Ming\", 16); System.out.println(ipConfig); //创建序列化输出流 ByteArrayOutputStream outputStream=new ByteArrayOutputStream(); ObjectOutputStream objectOutputStream=new ObjectOutputStream(outputStream); //将序列化对象存入缓冲区 objectOutputStream.writeObject(ipConfig); //改值 ipConfig.SetAge(11); IpConfig.virualAge=5555; //从缓冲区取回被序列化的对象 ObjectInputStream in = new ObjectInputStream(new ByteArrayInputStream(outputStream.toByteArray())); IpConfig newIpconfig = (IpConfig) in.readObject(); System.out.println(\"序列化后取出的对象：\"); System.out.println(newIpconfig); &#125;&#125;原来的对象：name: Mingage: 16virualAge: 123address: 127.0.0.1序列化后取出的对象：name: Mingage: 16virualAge: 5555address: null ​ 发现ipAddress(被transient)和virualAge(被static)也没有被序列化，中途修改virualAge的值是为了以防读者误会virualAge被序列化了。因为序列化可以保存对象的状态，但是virualAge的值被改变了，说明没有被序列化。static成员不属于对象实例，可能被别的对象修改没办法序列化,序列化是序列对象。对于address被反序列化后由于没有对应的引用，所以为null。而且Serializable不会调用构造方法。 显式序列化​ 实现Externalizable接口 ​ Externalizable接口继承自Serializable, 我们在实现该接口时，必须实现writeExternal()和readExternal()方法，而且只能通过手动进行序列化，并且两个方法是自动调用的，因此，这个序列化过程是可控的，可以自己选择哪些部分序列化 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public class Blip implements Externalizable&#123; private int i ; private String s; public Blip() &#123;&#125; public Blip(String x, int a) &#123; System.out.println(\"Blip (String x, int a)\"); s = x; i = a; &#125; public String toString() &#123; return s+i; &#125; @Override public void writeExternal(ObjectOutput out) throws IOException &#123; // TODO Auto-generated method stub System.out.println(\"Blip.writeExternal\"); out.writeObject(s); out.writeInt(i); &#125; @Override public void readExternal(ObjectInput in) throws IOException, ClassNotFoundException &#123; // TODO Auto-generated method stub System.out.println(\"Blip.readExternal\"); s = (String)in.readObject(); i = in.readInt(); &#125; public static void main(String[] args) throws FileNotFoundException, IOException, ClassNotFoundException &#123; System.out.println(\"Constructing objects\"); Blip b = new Blip(\"A Stirng\", 47); System.out.println(b); ObjectOutputStream o = new ObjectOutputStream(new FileOutputStream(\"F://Demo//file1.txt\")); System.out.println(\"保存对象\"); o.writeObject(b); o.close(); //获得对象 System.out.println(\"获取对象\"); ObjectInputStream in = new ObjectInputStream(new FileInputStream(\"F://Demo//file1.txt\")); System.out.println(\"Recovering b\"); b = (Blip)in.readObject(); System.out.println(b); &#125;&#125;打印结果为：Constructing objectsBlip (String x, int a)A Stirng47保存对象Blip.writeExternal获取对象Recovering bBlip.readExternalA Stirng47当注释掉writeExternal和readExternal方法后打印结果为:Constructing objectsBlip (String x, int a)A Stirng47保存对象Blip.writeExternal获取对象Recovering bBlip.readExternalnull0 说明：Externalizable类会调用public的构造函数先初始化对象，在调用所保存的内容将对象还原。假如构造方法不是public则会出现运行时错误。 显式+隐式的实现​ 如果想将方式一和方式二的优点都用到的话，可以采用方式三， 先实现Serializable接口，并且添加writeObject()和readObject()方法。注意这里是添加，不是重写或者覆盖。但是添加的这两个方法必须有相应的格式。 1，方法必须要被private修饰 —–&gt;才能被调用2，第一行调用默认的defaultRead/WriteObject(); —–&gt;隐式序列化非static和transient3，调用read/writeObject()将获得的值赋给相应的值 —&gt;显式序列化 123456789101112131415161718192021222324252627282930313233343536373839public class SerDemo implements Serializable&#123; public transient int age = 23; public String name ; public SerDemo()&#123; System.out.println(\"默认构造器。。。\"); &#125; public SerDemo(String name) &#123; this.name = name; &#125; private void writeObject(ObjectOutputStream stream) throws IOException &#123; stream.defaultWriteObject(); stream.writeInt(age); &#125; private void readObject(ObjectInputStream stream) throws ClassNotFoundException, IOException &#123; stream.defaultReadObject(); age = stream.readInt(); &#125; public String toString() &#123; return \"年龄\" + age + \" \" + name; &#125;public static void main(String[] args) throws IOException, ClassNotFoundException &#123; SerDemo stu = new SerDemo(\"Ming\"); ByteArrayOutputStream bout = new ByteArrayOutputStream(); ObjectOutputStream out = new ObjectOutputStream(bout); out.writeObject(stu); ObjectInputStream in = new ObjectInputStream(new ByteArrayInputStream(bout.toByteArray())); SerDemo stu1 = (SerDemo) in.readObject(); System.out.println(stu1); &#125;&#125;打印结果为：年龄23 Ming注释掉stream.writeInt(age)和age= stream.readInt()后：年龄0 Ming方式三结合了显式和隐式序列化，Ming被正常序列化，由于age被trancient修饰，所以需要显式序列化。 Json序列化​ Json序列化一般会使用jackson包，通过ObjectMapper类来进行一些操作，比如将对象转化为byte数组或者将json串转化为对象。现在的大多数公司都将json作为服务器端返回的数据格式。比如调用一个服务器接口，通常的请求为xxx.json?a=xxx&amp;b=xxx的形式。Json序列化示例代码如下所示： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package serialize;import java.io.IOException;import java.util.ArrayList;import java.util.List;import com.fasterxml.jackson.databind.ObjectMapper;public class JsonSerialize &#123; public static void main(String[] args) throws IOException &#123; new JsonSerialize().start(); &#125; public void start() throws IOException &#123; User u = new User(); List&lt;User&gt; friends = new ArrayList&lt;&gt;(); u.setUserName(\"张三\"); u.setPassWord(\"123456\"); u.setUserInfo(\"张三是一个很牛逼的人\"); u.setFriends(friends); User f1 = new User(); f1.setUserName(\"李四\"); f1.setPassWord(\"123456\"); f1.setUserInfo(\"李四是一个很牛逼的人\"); User f2 = new User(); f2.setUserName(\"王五\"); f2.setPassWord(\"123456\"); f2.setUserInfo(\"王五是一个很牛逼的人\"); friends.add(f1); friends.add(f2); ObjectMapper mapper = new ObjectMapper(); Long t1 = System.currentTimeMillis(); byte[] writeValueAsBytes = null; for (int i = 0; i &lt; 10; i++) &#123; writeValueAsBytes = mapper.writeValueAsBytes(u); &#125; System.out.println(\"json serialize: \" + (System.currentTimeMillis() - t1) + \"ms; 总大小：\" + writeValueAsBytes.length); Long t2 = System.currentTimeMillis(); User user = mapper.readValue(writeValueAsBytes, User.class); System.out.println(\"json deserialize: \" + (System.currentTimeMillis() - t2) + \"ms; User: \" + user); &#125;&#125; 12json serialize: 55ms; 总大小：341json deserialize: 35ms; User: User [userId=null, userName=张三, passWord=123456, userInfo=张三是一个很牛逼的人, friends=[User [userId=null, userName=李四, passWord=123456, userInfo=李四是一个很牛逼的人, friends=null], User [userId=null, userName=王五, passWord=123456, userInfo=王五是一个很牛逼的人, friends=null]]] FastJson序列化​ fastjson 是由阿里巴巴开发的一个性能很好的Java 语言实现的 Json解析器和生成器。特点：速度快，测试表明fastjson具有极快的性能，超越任其他的java json parser。功能强大，完全支持java bean、集合、Map、日期、Enum，支持范型和自省。无依赖，能够直接运行在Java SE 5.0以上版本支持Android。使用时候需引入FastJson第三方jar包。FastJson序列化代码示例如下所示： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package serialize;import java.util.ArrayList;import java.util.List;import com.alibaba.fastjson.JSON;/** * * @author liqqc * */public class FastJsonSerialize &#123; public static void main(String[] args) &#123; new FastJsonSerialize().start(); &#125; public void start()&#123; User u = new User(); List&lt;User&gt; friends = new ArrayList&lt;&gt;(); u.setUserName(\"张三\"); u.setPassWord(\"123456\"); u.setUserInfo(\"张三是一个很牛逼的人\"); u.setFriends(friends); User f1 = new User(); f1.setUserName(\"李四\"); f1.setPassWord(\"123456\"); f1.setUserInfo(\"李四是一个很牛逼的人\"); User f2 = new User(); f2.setUserName(\"王五\"); f2.setPassWord(\"123456\"); f2.setUserInfo(\"王五是一个很牛逼的人\"); friends.add(f1); friends.add(f2); //序列化 Long t1 = System.currentTimeMillis(); String text = null; for(int i = 0; i&lt;10; i++) &#123; text = JSON.toJSONString(u); &#125; System.out.println(\"fastJson serialize: \" +(System.currentTimeMillis() - t1) + \"ms; 总大小：\" + text.getBytes().length); //反序列化 Long t2 = System.currentTimeMillis(); User user = JSON.parseObject(text, User.class); System.out.println(\"fastJson serialize: \" + (System.currentTimeMillis() -t2) + \"ms; User: \" + user); &#125;&#125; ProtoBuff序列化1234567ProtocolBuffer是一种轻便高效的结构化数据存储格式，可以用于结构化数据序列化。适合做数据存储或 RPC 数据交换格式。可用于通讯协议、数据存储等领域的语言无关、平台无关、可扩展的序列化结构数据格式。优点：跨语言；序列化后数据占用空间比JSON小，JSON有一定的格式，在数据量上还有可以压缩的空间。缺点：它以二进制的方式存储，无法直接读取编辑，除非你有 .proto 定义，否则无法直接读出 Protobuffer的任何内容。其与thrift的对比：两者语法类似，都支持版本向后兼容和向前兼容，thrift侧重点是构建跨语言的可伸缩的服务，支持的语言多，同时提供了全套RPC解决方案，可以很方便的直接构建服务，不需要做太多其他的工作。 Protobuffer主要是一种序列化机制，在数据序列化上进行性能比较，Protobuffer相对较好。 参考地址： https://github.com/google/protobuf。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172package serialize;import java.io.Serializable;import java.util.List;import com.baidu.bjf.remoting.protobuf.FieldType;import com.baidu.bjf.remoting.protobuf.annotation.Protobuf;public class User implements Serializable &#123; private static final long serialVersionUID = -7890663945232864573L; @Protobuf(fieldType = FieldType.INT32, required = false, order = 1) private Integer userId; @Protobuf(fieldType = FieldType.STRING, required = false, order = 2) private String userName; @Protobuf(fieldType = FieldType.STRING, required = false, order = 3) private String passWord; @Protobuf(fieldType = FieldType.STRING, required = false, order = 4) private String userInfo; @Protobuf(fieldType = FieldType.OBJECT, required = false, order = 5) private List&lt;User&gt; friends; public Integer getUserId() &#123; return userId; &#125; public void setUserId(Integer userId) &#123; this.userId = userId; &#125; public String getUserName() &#123; return userName; &#125; public void setUserName(String userName) &#123; this.userName = userName; &#125; public String getPassWord() &#123; return passWord; &#125; public void setPassWord(String passWord) &#123; this.passWord = passWord; &#125; public String getUserInfo() &#123; return userInfo; &#125; public void setUserInfo(String userInfo) &#123; this.userInfo = userInfo; &#125; public List&lt;User&gt; getFriends() &#123; return friends; &#125; public void setFriends(List&lt;User&gt; friends) &#123; this.friends = friends; &#125; @Override public String toString() &#123; return \"User [userId=\" + userId + \", userName=\" + userName + \", passWord=\" + passWord + \", userInfo=\" + userInfo + \", friends=\" + friends + \"]\"; &#125;&#125; jprotobuf序列化代码示例如下所示： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package serialize;import java.io.IOException;import java.util.ArrayList;import java.util.List;import com.baidu.bjf.remoting.protobuf.Codec;import com.baidu.bjf.remoting.protobuf.ProtobufProxy;/** * * @author liqqc * */public class ProtoBuffSerialize &#123; public static void main(String[] args) throws IOException &#123; new ProtoBuffSerialize().start(); &#125; public void start() throws IOException &#123; Codec&lt;User&gt; studentClassCodec = ProtobufProxy.create(User.class, false); User u2 = new User(); List&lt;User&gt; friends = new ArrayList&lt;&gt;(); u2.setUserName(\"张三\"); u2.setPassWord(\"123456\"); u2.setUserInfo(\"张三是一个很牛逼的人\"); u2.setFriends(friends); User f1 = new User(); f1.setUserName(\"李四\"); f1.setPassWord(\"123456\"); f1.setUserInfo(\"李四是一个很牛逼的人\"); User f2 = new User(); f2.setUserName(\"王五\"); f2.setPassWord(\"123456\"); f2.setUserInfo(\"王五是一个很牛逼的人\"); friends.add(f1); friends.add(f2); Long stime_jpb_encode = System.currentTimeMillis(); byte[] bytes = null; for(int i = 0; i&lt;10; i++) &#123; bytes = studentClassCodec.encode(u2); &#125; System.out.println(\"jprotobuf序列化耗时：\" + (System.currentTimeMillis() - stime_jpb_encode) + \"ms; 总大小：\" + bytes.length); Long stime_jpb_decode = System.currentTimeMillis(); User user = studentClassCodec.decode(bytes); Long etime_jpb_decode = System.currentTimeMillis(); System.out.println(\"jprotobuf反序列化耗时：\"+ (etime_jpb_decode-stime_jpb_decode) + \"ms; User: \" + user); &#125;&#125; 运行结果 12jprotobuf序列化耗时：9ms; 总大小：148jprotobuf反序列化耗时：0ms; User: User [userId=null, userName=张三, passWord=123456, userInfo=张三是一个很牛逼的人, friends=[User [userId=null, userName=李四, passWord=123456, userInfo=李四是一个很牛逼的人, friends=null], User [userId=null, userName=王五, passWord=123456, userInfo=王五是一个很牛逼的人, friends=null]]]","categories":[],"tags":[{"name":"Java基础知识","slug":"Java基础知识","permalink":"https://liudong-code.github.io/tags/Java%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"keywords":[]},{"title":"IO多路复用技术以及epoll实现原理","slug":"IO多路复用技术以及epoll实现原理","date":"2020-02-28T10:57:13.000Z","updated":"2020-02-28T11:36:18.863Z","comments":true,"path":"2020/02/28/IO多路复用技术以及epoll实现原理/","link":"","permalink":"https://liudong-code.github.io/2020/02/28/IO%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8%E6%8A%80%E6%9C%AF%E4%BB%A5%E5%8F%8Aepoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/","excerpt":"","text":"为什么Redis中要使用I/O多路复用呢？​ redis 是一个单线程却性能非常好的内存数据库， 主要用来作为缓存系统。 ​ redis 采用网络IO多路复用技来保证在多连接的时候， 系统的高吞吐量。 ​ Redis 是跑在单线程中的，所有的操作都是按照顺序线性执行的，但是由于读写操作等待用户输入或输出都是阻塞的，所以 I/O 操作在一般情况下往往不能直接返回，这会导致某一文件的 I/O 阻塞导致整个进程无法对其它客户提供服务，而 I/O 多路复用就是为了解决这个问题而出现的. ​ select，poll，epoll都是IO多路复用的机制。I/O多路复用就通过一种机制，可以监视多个描述符，一旦某个描述符就绪，能够通知程序进行相应的操作。​ redis的io模型主要是基于epoll实现的，不过它也提供了 select和kqueue的实现，默认采用epoll。 epoll实现机制123设想一下如下场景：有100万个客户端同时与一个服务器进程保持着TCP连接。而每一时刻，通常只有几百上千个TCP连接是活跃的(事实上大部分场景都是这种情况)。如何实现这样的高并发？ ​ select/poll是，服务器进程每次都把这100万个连接告诉操作系统(从用户态复制句柄数据结构到内态)，让操作系统内核去查询这些套接字上是否有事件发生，轮询完后，再将句柄数据复制到用户态，让服务器应用程序轮询处理已发生的网络事件，这一过程资源消耗较大，因此，select/poll一般只能处理几千的并发连接。 ​ 如果没有I/O事件产生，我们的程序就会阻塞在select处。但是依然有个问题，我们从select那里仅仅知道了，有I/O事件发生了，但却并不知道是那几个流（可能有一个，多个，甚至全部），我们只能无差别轮询所有流，找出能读出数据，或者写入数据的流，对他们进行操作。 总结：select和poll的缺点如下： 每次调用select/poll，都需要把fd集合从用户态拷贝到内核态，这个开销在fd很多时会很大 同时每次调用select/poll都需要在内核遍历传递进来的所有fd，这个开销在fd很多时也很大 针对select支持的文件描述符数量太小了，默认是1024 select返回的是含有整个句柄的数组，应用程序需要遍历整个数组才能发现哪些句柄发生了事件； select的触发方式是水平触发，应用程序如果没有完成对一个已经就绪的文件描述符进行IO操作，那么之后每次select调用还是会将这些文件描述符通知进程。 相比select模型，poll使用链表保存文件描述符，因此没有了监视文件数量的限制，但其他三个缺点依然存在。 ​ epoll的设计和实现与select完全不同。epoll是poll的一种优化，返回后不需要对所有的fd进行遍历，在内核中维持了fd的列表。select和poll是将这个内核列表维持在用户态，然后传递到内核中。与poll/select不同，epoll不再是一个单独的系统调用，而是由epoll_create/epoll_ctl/epoll_wait三个系统调用组成，后面将会看到这样做的好处。epoll在2.6以后的内核才支持。 ​ epoll通过在Linux内核中申请一个简易的文件系统(文件系统一般用什么数据结构实现？B+树)。把原先select/poll调用分成了3个部分： 1）调用epoll_create()建立一个epoll对象(在epoll文件系统中为这个句柄对象分配资源) 2）调用epoll_ctl向epoll对象中添加这100万个连接的套接字 3）调用epoll_wait收集发生的事件的连接 ​ 如此一来，要实现上面说是的场景，只需要在进程启动时建立一个epoll对象，然后在需要的时候向这个epoll对象中添加或者删除连接。同时，epoll_wait的效率也非常高，因为调用epoll_wait时，并没有一股脑的向操作系统复制这100万个连接的句柄数据，内核也不需要去遍历全部的连接。 总结：epoll的优点 epoll 没有最大并发连接的限制，上限是最大可以打开文件的数目，这个数字一般远大于 2048, 一般来说这个数目和系统内存关系很大 ，具体数目可以 cat /proc/sys/fs/file-max 察看。 效率提升， epoll 最大的优点就在于它只管你“活跃”的连接 ，而跟连接总数无关，因此在实际的网络环境中， epoll 的效率就会远远高于 select 和 poll 。 内存拷贝， epoll 在这点上使用了“共享内存”，这个内存拷贝也省略了 redis epoll底层实现​ 当某一进程调用epoll_create方法时，Linux内核会创建一个eventpoll结构体，这个结构体中有两个成员与epoll的使用方式密切相关 eventpoll结构体如下所示 12345678910struct eventpoll｛ ..... /*红黑树节点的根，存储着所有添加到epoll中的需要监控的事件*/ struct rb_root rbt; /*双链表中储存着将要通过epoll_wait返回给用户满足条件的事件*/ struct list_head rdlist; .....｝ ​ 每一个epoll对象都有一个独立的eventpoll结构体，用于存放通过epoll_ctl方法向epoll对象中添加进来的事件。这些事件都会挂载在红黑树中，如此，重复添加的事件就可以通过红黑树而高效的识别出来(红黑树的插入时间效率是lgn，其中n为树的高度)。 ​ 而所有添加到epoll中的事件都会与设备(网卡)驱动程序建立回调关系，也就是说，当相应的事件发生时会调用这个回调方法。这个回调方法在内核中叫ep_poll_callback,它会将发生的事件添加到rdlist双链表中。 ​ 在epoll中，对于每一个事件，都会建立一个epitem结构体，如下所示： 1234567struct epitem&#123; struct rb_node rbn;//红黑树节点 struct list_head rdllink;//双向链表节点 struct epoll_filedfd ffd;//句柄事件信息 struct eventpoll *ep ;// 指向其所属的eventpoll对象 struct epoll_event;//期待发生的事件类型&#125; ​ 当调用epoll_wait检查是否有事件发生时，只需要检查eventpoll对象中的rdlist双链表中是否有epitem元素即可。如果rdlist不为空，则把发生的事件复制到用户态，同时将事件数量返回给用户 优势： 不用重复传递。我们调用epoll_wait时就相当于以往调用select/poll，但是这时却不用传递socket句柄给内核，因为内核已经在epoll_ctl中拿到了要监控的句柄列表。 在内核里，一切皆文件。所以，epoll向内核注册了一个文件系统，用于存储上述的被监控socket。当你调用epoll_create时，就会在这个虚拟的epoll文件系统里创建一个file结点。当然这个file不是普通文件，它只服务于epoll。epoll在被内核初始化时（操作系统启动），同时会开辟出epoll自己的内核高速cache区，用于安置每一个我们想监控的socket，这些socket会以红黑树的形式保存在内核cache里，以支持快速的查找、插入、删除。这个内核高速cache区，就是建立连续的物理内存页，然后在之上建立slab层，简单的说，就是物理上分配好你想要的size的内存对象，每次使用时都是使用空闲的已分配好的对象。 极其高效的原因：这是由于我们在调用epoll_create时，内核除了帮我们在epoll文件系统里建了个file结点，在内核cache里建了个红黑树用于存储以后epoll_ctl传来的socket外，还会再建立一个list链表，用于存储准备就绪的事件，当epoll_wait调用时，仅仅观察这个list链表里有没有数据即可。有数据就返回，没有数据就sleep，等到timeout时间到后即使链表没数据也返回。所以，epoll_wait非常高效。 这个准备就绪list链表是怎么维护的呢？​ 当我们执行epoll_ctl时，除了把socket放到epoll文件系统里file对象对应的红黑树上之外，还会给内核中断处理程序注册一个回调函数，告诉内核，如果这个句柄的中断到了，就把它放到准备就绪list链表里。所以，当一个socket上有数据到了，内核在把网卡上的数据copy到内核中后就来把socket插入到准备就绪链表里了。（注：好好理解这句话！） ​ 从上面这句可以看出，epoll的基础就是回调呀！ ​ 如此，一颗红黑树，一张准备就绪句柄链表，少量的内核cache，就帮我们解决了大并发下的socket处理问题。执行epoll_create时，创建了红黑树和就绪链表，执行epoll_ctl时，如果增加socket句柄，则检查在红黑树中是否存在，存在立即返回，不存在则添加到树干上，然后向内核注册回调函数，用于当中断事件来临时向准备就绪链表中插入数据。执行epoll_wait时立刻返回准备就绪链表里的数据即可。 ​ 最后看看epoll独有的两种模式LT和ET。无论是LT和ET模式，都适用于以上所说的流程。区别是，LT模式下，只要一个句柄上的事件一次没有处理完，会在以后调用epoll_wait时次次返回这个句柄，而ET模式仅在第一次返回。关于LT，ET，有一端描述，LT和ET都是电子里面的术语，ET是边缘触发，LT是水平触发，一个表示只有在变化的边际触发，一个表示在某个阶段都会触发。 ​ LT, ET这件事怎么做到的呢？当一个socket句柄上有事件时，内核会把该句柄插入上面所说的准备就绪list链表，这时我们调用epoll_wait，会把准备就绪的socket拷贝到用户态内存，然后清空准备就绪list链表，最后，epoll_wait干了件事，就是检查这些socket，如果不是ET模式（就是LT模式的句柄了），并且这些socket上确实有未处理的事件时，又把该句柄放回到刚刚清空的准备就绪链表了。所以，非ET的句柄，只要它上面还有事件，epoll_wait每次都会返回这个句柄。（从上面这段，可以看出，LT还有个回放的过程，低效了）","categories":[],"tags":[{"name":"IO模型","slug":"IO模型","permalink":"https://liudong-code.github.io/tags/IO%E6%A8%A1%E5%9E%8B/"},{"name":"Redis相关","slug":"Redis相关","permalink":"https://liudong-code.github.io/tags/Redis%E7%9B%B8%E5%85%B3/"}],"keywords":[]},{"title":"常见的IO模型","slug":"常见的IO模型","date":"2020-02-28T09:35:29.000Z","updated":"2020-02-29T11:22:38.590Z","comments":true,"path":"2020/02/28/常见的IO模型/","link":"","permalink":"https://liudong-code.github.io/2020/02/28/%E5%B8%B8%E8%A7%81%E7%9A%84IO%E6%A8%A1%E5%9E%8B/","excerpt":"","text":"高性能IO模型，常见的有四种：（1）同步阻塞IO（Blocking IO）：即传统的IO模型。 （2）同步非阻塞IO（Non-blocking IO）：默认创建的socket都是阻塞的，非阻塞IO要求socket被设置为NONBLOCK。注意这里所说的NIO并非Java的NIO（New IO）库。 （3）IO多路复用（IO Multiplexing）：即经典的Reactor设计模式，有时也称为异步阻塞IO，Java中的Selector和Linux中的epoll都是这种模型。 （4）异步IO（Asynchronous IO）：即经典的Proactor设计模式，也称为异步非阻塞IO。 IO模型举例理解1 阻塞IO, 给女神发一条短信, 说我来找你了, 然后就默默的一直等着女神下楼, 这个期间除了等待你不 会做其他事情, 属于备胎做法. 非阻塞IO, 给女神发短信, 如果不回, 接着再发, 一直发到女神下楼, 这个期间你除了发短信等待不会 做其他事情, 属于专一做法. IO多路复用, 是找一个宿管大妈来帮你监视下楼的女生, 这个期间你可以些其他的事情. 例如可以顺便看看其他妹子,玩玩王者荣耀, 上个厕所等等. IO复用又包括 select, poll, epoll 模式. 那么它们的区别是什么? 3.1 select大妈 每一个女生下楼, select大妈都不知道这个是不是你的女神, 她需要一个一个询问, 并且select大妈能力还有限, 最多一次帮你监视1024个妹子 3.2 poll大妈不限制盯着女生的数量, 只要是经过宿舍楼门口的女生, 都会帮你去问是不是你女神 3.3 epoll大妈不限制盯着女生的数量, 并且也不需要一个一个去问. 那么如何做呢? epoll大妈会为每个进宿舍楼的女生脸上贴上一个大字条,上面写上女生自己的名字, 只要女生下楼了, epoll大妈就知道这个是不是你女神了, 然后大妈再通知你.上面这些同步IO有一个共同点就是, 当女神走出宿舍门口的时候, 你已经站在宿舍门口等着女神的, 此时你属于同步等待状态 接下来是异步IO的情况 你告诉女神我来了, 然后你就去王者荣耀了, 一直到女神下楼了, 发现找不见你了,女神再给你打电话通知你, 说我下楼了, 你在哪呢? 这时候你才来到宿舍门口. 此时属于逆袭做法 IO模型举例理解21.阻塞I/O模型 老李去火车站买票，排队三天买到一张退票。 耗费：在车站吃喝拉撒睡 3天，其他事一件没干。 2.非阻塞I/O模型 老李去火车站买票，隔12小时去火车站问有没有退票，三天后买到一张票。耗费：往返车站6次，路上6小时，其他时间做了好多事。 3.I/O复用模型 select/poll 老李去火车站买票，委托黄牛，然后每隔6小时电话黄牛询问，黄牛三天内买到票，然后老李去火车站交钱领票。 耗费：往返车站2次，路上2小时，黄牛手续费100元，打电话17次 epoll 老李去火车站买票，委托黄牛，黄牛买到后即通知老李去领，然后老李去火车站交钱领票。 耗费：往返车站2次，路上2小时，黄牛手续费100元，无需打电话 4.信号驱动I/O模型 老李去火车站买票，给售票员留下电话，有票后，售票员电话通知老李，然后老李去火车站交钱领票。 耗费：往返车站2次，路上2小时，免黄牛费100元，无需打电话 5.异步I/O模型 老李去火车站买票，给售票员留下电话，有票后，售票员电话通知老李并快递送票上门。 耗费：往返车站1次，路上1小时，免黄牛费100元，无需打电话 同步阻塞IO​ 同步阻塞IO模型是最简单的IO模型，用户线程在内核进行IO操作时被阻塞。 等待数据就绪（读就绪、写就绪）； 将数据从内核中拷贝到JVM进程中； 处理数据。 ​ 整个IO请求的过程中，用户线程是被阻塞的，这导致用户在发起IO请求时，不能做任何事情，对CPU的资源利用率不够 同步非阻塞IO ​ 由于socket是非阻塞的方式，因此用户线程发起IO请求时立即返回。但并未读取到任何数据，用户线程需要不断地发起IO请求，直到数据到达后，才真正读取到数据，继续执行。 伪代码： 1234&#123; while(read(socket, buffer) != SUCCESS); process(buffer);&#125; ​ 即用户需要不断地调用read，尝试读取socket中的数据，直到读取成功后，才继续处理接收的数据。整个IO请的过程中，虽然用户线程每次发起IO请求后可以立即返回，但是为了等到数据，仍需要不断地轮询、重复请求，消耗了大量的CPU的资源。一般很少直接使用这种模型，而是在其他IO模型中使用非阻塞IO这一特性。 IO多路复用​ IO多路复用模型是建立在内核提供的多路分离函数select基础之上的，使用select函数可以避免同步非阻塞IO模型中轮询等待的问题。 ​ 用户首先将需要进行IO操作的socket添加到select中，然后阻塞等待select系统调用返回。当数据到达时，socket被激活，select函数返回。用户线程正式发起read请求，读取数据并继续执行。 ​ 从流程上来看，使用select函数进行IO请求和同步阻塞模型没有太大的区别，甚至还多了添加监视socket，以及调用select函数的额外操作，效率更差。 ​ 但是，使用select以后最大的优势是用户可以在一个线程内同时处理多个socket的IO请求。 ​ 用户可以注册多个socket，然后不断地调用select读取被激活的socket，即可达到在同一个线程内同时处理多个IO请求的目的。而在同步阻塞模型中，必须通过多线程的方式才能达到这个目的。 用户线程使用IO多路复用模型的伪代码描述为： 123456789void UserEventHandler::handle_event() &#123; if(can_read(socket)) &#123; read(socket, buffer); process(buffer); &#125; &#125;&#123; Reactor.register(new UserEventHandler(socket));&#125; ​ 用户需要重写EventHandler的handle_event函数进行读取数据、处理数据的工作，用户线程只需要将自己的EventHandler注册到Reactor即可。Reactor中handle_events事件循环的伪代码大致如下： 12345678Reactor::handle_events() &#123; while(1) &#123; sockets = select(); for(socket in sockets) &#123; get_event_handler(socket).handle_event(); &#125; &#125;&#125; ​ 事件循环不断地调用select获取被激活的socket，然后根据获取socket对应的EventHandler，执行器handle_event函数即可。​ IO多路复用是最常使用的IO模型，但是其异步程度还不够“彻底”，因为它使用了会阻塞线程的select系统调用。因此IO多路复用只能称为异步阻塞IO，而非真正的异步IO。 异步IO​ “真正”的异步IO需要操作系统更强的支持。在IO多路复用模型中，事件循环将文件句柄的状态事件通知给用户线程，由用户线程自行读取数据、处理数据。而在异步IO模型中，当用户线程收到通知时，数据已经被内核读取完毕，并放在了用户线程指定的缓冲区内，内核在IO完成后通知用户线程直接使用即可。 异步IO模型使用了Proactor设计模式实现了这一机制。 异步IO模型中， 用户线程直接使用内核提供的异步IO API发起read请求，且发起后立即返回，继续执行用户线程代码。不过此时用户线程已经将调用的AsynchronousOperation和CompletionHandler注册到内核， 然后操作系统开启独立的内核线程去处理IO操作。当read请求的数据到达时，由内核负责读取socket中的数据，并写入用户指定的缓冲区中。 最后内核将read的数据和用户线程注册的CompletionHandler分发给内部Proactor，Proactor将IO完成的信息通知给用户线程（一般通过调用用户线程注册的完成事件处理函数），完成异步IO。 用户线程使用异步IO模型的伪代码描述为： 1234567void UserCompletionHandler::handle_event(buffer) &#123; process(buffer);&#125;&#123; aio_read(socket, new UserCompletionHandler);&#125; 用户需要重写CompletionHandler的handle_event函数进行处理数据的工作，参数buffer表示Proactor已经准备好的数据，用户线程直接调用内核提供的异步IO API，并将重写的CompletionHandler注册即可。 相比于IO多路复用模型，异步IO并不十分常用，不少高性能并发服务程序使用IO多路复用模型+多线程任务处理的架构基本可以满足需求。况且目前操作系统对异步IO的支持并非特别完善，更多的是采用IO多路复用模型模拟异步IO的方式（IO事件触发时不直接通知用户线程，而是将数据读写完毕后放到用户指定的缓冲区中）。Java7之后已经支持了异步IO，感兴趣的读者可以尝试使用。","categories":[],"tags":[{"name":"IO模型","slug":"IO模型","permalink":"https://liudong-code.github.io/tags/IO%E6%A8%A1%E5%9E%8B/"}],"keywords":[]},{"title":"IO设计模式之Reactor和Proactor","slug":"IO设计模式之Reactor和Proactor","date":"2020-02-28T08:49:31.000Z","updated":"2020-02-28T09:45:04.562Z","comments":true,"path":"2020/02/28/IO设计模式之Reactor和Proactor/","link":"","permalink":"https://liudong-code.github.io/2020/02/28/IO%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8BReactor%E5%92%8CProactor/","excerpt":"","text":"反应器Reactor概述​ 反应器设计模式(Reactor pattern)是一种为处理并发服务请求，并将请求提交到一个或者多个服务处理程序的 事件设计模式。当客户端请求抵达后，服务处理程序使用多路分配策略，由一个非阻塞的线程来接收所有的请求， 然后派发这些请求至相关的工作线程进行处理。 Reactor模式主要包含下面几部分内容： 初始事件分发器(Initialization Dispatcher)：用于管理Event Handler，定义注册、移除EventHandler等。它还作为Reactor模式的入口调用Synchronous Event Demultiplexer的select方法以阻塞等待事件返回，当阻塞等待返回时，根据事件发生的Handle将其分发给对应的Event Handler处理，即回调EventHandler中handle_event(方法 同步（多路）事件分离器(Synchronous Event Demultiplexer)：无限循环等待新事件的到来，一旦发现有新的事件到来，就会通知初始事件分发器去调取特定的事件处理器。 系统处理程序(Handles)：操作系统中的句柄，是对资源在操作系统层面上的一种抽象，它可以是打开的文件、一个连接(Socket)、Timer等。由于Reactor模式一般使用在网络编程中，因而这里一般指SocketHandle，即一个网络连接（Connection，在Java NIO中的Channel）。这个Channel注册到SynchronousEvent Demultiplexer中，以监听Handle中发生的事件，对ServerSocketChannnel可以是CONNECT事件，对SocketChannel可以是READ、WRITE、CLOSE事件等。 事件处理器(Event Handler)： 定义事件处理方法，以供Initialization Dispatcher回调使用。 为什么使用Reactor模式​ 并发系统常使用reactor模式代替常用的多线程的处理方式，节省系统的资源，提高系统的吞吐量。 例如：在高并发的情况下，既可以使用多处理处理方式，也可以使用Reactor处理方式。 多线程的处理：​ 为每个单独到来的请求，专门启动一条线程，这样的话造成系统的开销很大，并且在单核的机上，多线程并不能提高系统的性能，除非在有一些阻塞的情况发生。否则线程切换的开销会使处理的速度变慢。 Reactor模式的处理：​ 服务器端启动一条单线程，用于轮询IO操作是否就绪，当有就绪的才进行相应的读写操作，这样的话就减少了服务器产生大量的线程，也不会出现线程之间的切换产生的性能消耗。(目前JAVA的NIO就采用的此种模式，这里引申出一个问题：在多核情况下NIO的扩展问题) ​ 以上两种处理方式都是基于同步的，多线程的处理是我们传统模式下对高并发的处理方式，Reactor模式的处理是现今面对高并发和高性能一种主流的处理方式。 Reactor模式结构 Reactor包含如下角色： Handle 句柄；用来标识socket连接或是打开文件； Synchronous Event Demultiplexer：同步事件多路分解器：由操作系统内核实现的一个函数；用于阻塞等待发生在句柄集合上的一个或多个事件；（如select/epoll；） Event Handler：事件处理接口 Concrete Event HandlerA：实现应用程序所提供的特定事件处理逻辑； Reactor：反应器，定义一个接口，实现以下功能： 1）供应用程序注册和删除关注的事件句柄； 2）运行事件循环； 3）有就绪事件到来时，分发事件到之前注册的回调函数上处理； Initiation Dispatcher：用于管理Event Handler，即EventHandler的容器，用以注册、移除EventHandler等；另外，它还作为Reactor模式的入口调用Synchronous Event Demultiplexer的select方法以阻塞等待事件返回，当阻塞等待返回时，根据事件发生的Handle将其分发给对应的Event Handler处理，即回调EventHandler中的handle_event()方法 应用启动，将关注的事件handle注册到Reactor中； 调用Reactor，进入无限事件循环，等待注册的事件到来； 事件到来，select返回，Reactor将事件分发到之前注册的回调函数中处理； Proactor模式​ 运用于异步I/O操作，Proactor模式中，应用程序不需要进行实际的读写过程，它只需要从缓存区读取或者写入即可，操作系统会读取缓存区或者写入缓存区到真正的IO设备.​ Proactor中写入操作和读取操作，只不过感兴趣的事件是写入完成事件。 Proactor模式结构 Proactor主动器模式包含如下角色 Handle 句柄；用来标识socket连接或是打开文件； Asynchronous Operation Processor：异步操作处理器；负责执行异步操作，一般由操作系统内核实现； Asynchronous Operation：异步操作 Completion Event Queue：完成事件队列；异步操作完成的结果放到队列中等待后续使用 Proactor：主动器；为应用程序进程提供事件循环；从完成事件队列中取出异步操作的结果，分发调用相应的后续处理逻辑； Completion Handler：完成事件接口；一般是由回调函数组成的接口； Concrete Completion Handler：完成事件处理逻辑；实现接口定义特定的应用处理逻辑； 业务流程及时序图 应用程序启动，调用异步操作处理器提供的异步操作接口函数，调用之后应用程序和异步操作处理就独立运行；应用程序可以调用新的异步操作，而其它操作可以并发进行； 应用程序启动Proactor主动器，进行无限的事件循环，等待完成事件到来； 异步操作处理器执行异步操作，完成后将结果放入到完成事件队列； 主动器从完成事件队列中取出结果，分发到相应的完成事件回调函数处理逻辑中； 对比两者的区别主动和被动以主动写为例： Reactor将handle放到select()，等待可写就绪，然后调用write()写入数据；写完处理后续逻辑； Proactor调用aoi_write后立刻返回，由内核负责写操作，写完后调用相应的回调函数处理后续逻辑； 可以看出，Reactor被动的等待指示事件的到来并做出反应；它有一个等待的过程，做什么都要先放入到监听事件集合中等待handler可用时再进行操作；Proactor直接调用异步读写操作，调用完后立刻返回； 实现Reactor实现了一个被动的事件分离和分发模型，服务等待请求事件的到来，再通过不受间断的同步处理事件，从而做出反应； Proactor实现了一个主动的事件分离和分发模型；这种设计允许多个任务并发的执行，从而提高吞吐量；并可执行耗时长的任务（各个任务间互不影响） 优点Reactor实现相对简单，对于耗时短的处理场景处理高效；操作系统可以在多个事件源上等待，并且避免了多线程编程相关的性能开销和编程复杂性；事件的串行化对应用是透明的，可以顺序的同步执行而不需要加锁；事务分离：将与应用无关的多路分解和分配机制和与应用相关的回调函数分离开来， Proactor性能更高，能够处理耗时长的并发场景； 缺点Reactor处理耗时长的操作会造成事件分发的阻塞，影响到后续事件的处理； Proactor实现逻辑复杂；依赖操作系统对异步的支持，目前实现了纯异步操作的操作系统少，实现优秀的如windows IOCP，但由于其windows系统用于服务器的局限性，目前应用范围较小；而Unix/Linux系统对纯异步的支持有限，应用事件驱动的主流还是通过select/epoll来实现； 适用场景Reactor：同时接收多个服务请求，并且依次同步的处理它们的事件驱动程序；Proactor：异步接收和同时处理多个服务请求的事件驱动程序； 开源产品如Redis、ACE，事件模型都使用的Reactor模式","categories":[],"tags":[{"name":"IO模型","slug":"IO模型","permalink":"https://liudong-code.github.io/tags/IO%E6%A8%A1%E5%9E%8B/"}],"keywords":[]},{"title":"同步IO、异步IO、阻塞IO、非阻塞IO","slug":"同步IO、异步IO、阻塞IO、非阻塞IO","date":"2020-02-28T08:43:03.000Z","updated":"2020-02-28T08:47:20.869Z","comments":true,"path":"2020/02/28/同步IO、异步IO、阻塞IO、非阻塞IO/","link":"","permalink":"https://liudong-code.github.io/2020/02/28/%E5%90%8C%E6%AD%A5IO%E3%80%81%E5%BC%82%E6%AD%A5IO%E3%80%81%E9%98%BB%E5%A1%9EIO%E3%80%81%E9%9D%9E%E9%98%BB%E5%A1%9EIO/","excerpt":"","text":"同步IO和异步IO同步和异步是针对应用程序和内核的交互而言的， 同步指的是用户进程触发IO操作并等待或者轮询的去查看IO操作是否就绪， 异步是指用户进程触发IO操作以后便开始做自己的事情，而当IO操作已经完成的时候会得到IO完成的通知。 指的是用户空间和内核空间数据交互的方式 ​ 同步：用户空间要的数据，必须等到内核空间给它才做其他事情 ​ 异步：用户空间要的数据，不需要等到内核空间给它，才做其他事情。内核空间会异步通知用户进程，并把 数据直接给到用户空间 阻塞IO和非阻塞IO 阻塞方式下读取或者写入函数将一直等待， 非阻塞方式下，读取或者写入函数会立即返回一个状态值。 指的是用户就和内核空间IO操作的方式 ​ 堵塞：用户空间通过系统调用（systemcall）和内核空间发送IO操作时，该调用是堵塞的 ​ 非堵塞：用户空间通过系统调用（systemcall）和内核空间发送IO操作时，该调用是不堵塞的，直接返回的， 只是返回时，可能没有数据而已","categories":[],"tags":[{"name":"IO模型","slug":"IO模型","permalink":"https://liudong-code.github.io/tags/IO%E6%A8%A1%E5%9E%8B/"}],"keywords":[]},{"title":"Socket网络编程","slug":"Socket网络编程","date":"2020-02-28T08:29:54.000Z","updated":"2020-02-28T08:37:52.983Z","comments":true,"path":"2020/02/28/Socket网络编程/","link":"","permalink":"https://liudong-code.github.io/2020/02/28/Socket%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/","excerpt":"","text":"客户端123456789101112131415public class SocketClient &#123; public static void main(String args[]) throws Exception &#123; // 要连接的服务端IP地址和端口 String host = \"127.0.0.1\"; int port = 55533; // 与服务端建立连接 Socket socket = new Socket(host, port); // 建立连接后获得输出流 OutputStream outputStream = socket.getOutputStream(); String message=\"你好 yiwangzhibujian\"; socket.getOutputStream().write(message.getBytes(\"UTF-8\")); outputStream.close(); socket.close(); &#125;&#125; 服务端123456789101112131415161718192021222324public class SocketServer &#123; public static void main(String[] args) throws Exception &#123; // 监听指定的端口 int port = 55533; ServerSocket server = new ServerSocket(port); // server将一直等待连接的到来 System.out.println(\"server将一直等待连接的到来\"); Socket socket = server.accept(); // 建立好连接后，从socket中获取输入流，并建立缓冲区进行读取 InputStream inputStream = socket.getInputStream(); byte[] bytes = new byte[1024]; int len; StringBuilder sb = new StringBuilder(); while ((len = inputStream.read(bytes)) != -1) &#123; //注意指定编码格式，发送方和接收方一定要统一，建议使用UTF-8 sb.append(new String(bytes, 0, len,\"UTF-8\")); &#125; System.out.println(\"get message from client: \" + sb); inputStream.close(); socket.close(); server.close(); &#125;&#125; 123456789101112131415161718192021222324public class SocketServer &#123; public static void main(String args[]) throws IOException &#123; // 监听指定的端口 int port = 55533; ServerSocket server = new ServerSocket(port); // server将一直等待连接的到来 System.out.println(\"server将一直等待连接的到来\"); while(true)&#123; Socket socket = server.accept(); // 建立好连接后，从socket中获取输入流，并建立缓冲区进行读取 InputStream inputStream = socket.getInputStream(); byte[] bytes = new byte[1024]; int len; StringBuilder sb = new StringBuilder(); while ((len = inputStream.read(bytes)) != -1) &#123; // 注意指定编码格式，发送方和接收方一定要统一，建议使用UTF-8 sb.append(new String(bytes, 0, len, \"UTF-8\")); &#125; System.out.println(\"get message from client: \" + sb); inputStream.close(); socket.close(); &#125; &#125;&#125; 线程池的方式： 1234567891011121314151617181920212223242526272829303132public class SocketServer &#123; public static void main(String args[]) throws Exception &#123; // 监听指定的端口 int port = 55533; ServerSocket server = new ServerSocket(port); // server将一直等待连接的到来 System.out.println(\"server将一直等待连接的到来\"); //如果使用多线程，那就需要线程池，防止并发过高时创建过多线程耗尽资源 ExecutorService threadPool = Executors.newFixedThreadPool(100); while (true) &#123;Socket socket = server.accept(); Runnable runnable=()-&gt;&#123; try &#123; // 建立好连接后，从socket中获取输入流，并建立缓冲区进行读取 InputStream inputStream = socket.getInputStream(); byte[] bytes = new byte[1024]; int len; StringBuilder sb = new StringBuilder(); while ((len = inputStream.read(bytes)) != -1) &#123; // 注意指定编码格式，发送方和接收方一定要统一，建议使用UTF-8 sb.append(new String(bytes, 0, len, \"UTF-8\")); &#125; System.out.println(\"get message from client: \" + sb); inputStream.close(); socket.close(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;; threadPool.submit(runnable); &#125; &#125;&#125;","categories":[],"tags":[{"name":"IO模型","slug":"IO模型","permalink":"https://liudong-code.github.io/tags/IO%E6%A8%A1%E5%9E%8B/"}],"keywords":[]},{"title":"IO访问方式","slug":"IO访问方式","date":"2020-02-28T03:50:23.000Z","updated":"2020-02-28T08:27:25.684Z","comments":true,"path":"2020/02/28/IO访问方式/","link":"","permalink":"https://liudong-code.github.io/2020/02/28/IO%E8%AE%BF%E9%97%AE%E6%96%B9%E5%BC%8F/","excerpt":"","text":"磁盘IO​ 当应用程序调用read接口时，操作系统检查在内核的高速缓存有没有需要的数据，如果已经缓存了，那么就直接从 缓存中返回，如果没有，则从磁盘中读取，然后缓存在操作系统的缓存中。​ 当应用程序调用write接口时，将数据从用户地址空间复制到内核地址空间的缓存中，这时对用户程序来说，写操作已 经完成，至于什么时候再写到磁盘中，由操作系统决定，除非显示调用了sync同步命令。 网络IO​ 1）操作系统将数据从磁盘复制到操作系统内核的页缓存中 ​ 2）应用将数据从内核缓存复制到应用的缓存中 ​ 3）应用 将数据写回内核的Socket缓存中 ​ 4）操作系统将数据从Socket缓存区复制到网卡缓存，然后将其通过网络发出 1我是图，图待补... 1、当调用read系统调用时，通过DMA（Direct Memory Access）将数据copy到内核模式 2、然后由CPU控制将内 核模式数据copy到用户模式下的 buffer中 3、read调用完成后，write调用首先将用户模式下 buffer中的数据copy到内核模式下的socket buffer中 4、最后通过DMA copy将内核模式下的socket buffer中的数据copy到网卡设备中传送。 比较 磁盘IO主要的延时是由（以15000rpm硬盘为例）： 机械转动延时（机械磁盘的主要性能瓶颈，平均为2ms） + 寻址延时（2~3ms） + 块传输延时（一般4k每块，40m/s的传输速度，延时一般为0.1ms) 决定。（平均为5ms） 网络IO主要延是由： 服务器响应延时 + 带宽限制 + 网络延时 + 跳转路由延时 + 本地接收延时 决定。（一般为几十到几千毫秒，受环境干扰极大）所以两者一般来说网络IO延时要大于磁盘IO的延时。","categories":[],"tags":[{"name":"IO模型","slug":"IO模型","permalink":"https://liudong-code.github.io/tags/IO%E6%A8%A1%E5%9E%8B/"}],"keywords":[]},{"title":"缓存IO和直接IO","slug":"缓存IO和直接IO","date":"2020-02-28T02:21:56.000Z","updated":"2020-02-28T03:23:17.351Z","comments":true,"path":"2020/02/28/缓存IO和直接IO/","link":"","permalink":"https://liudong-code.github.io/2020/02/28/%E7%BC%93%E5%AD%98IO%E5%92%8C%E7%9B%B4%E6%8E%A5IO/","excerpt":"","text":"定义 缓存IO：数据从磁盘通过DMA copy到内核空间，再从内核空间通过CPU copy到用户空间。 直接IO：数据从磁盘通过DMA copy到用户空间。 缓存IO​ 缓存I/O又被称作标准IO，大多数文件系统的默认操作都是缓存IO。在Linux的缓存IO机制中，数据先从磁盘复制到内核空间的缓冲区，然后从内核空间缓冲区复制到应用程序的地址空间。 读操作： 操作系统检查内核的缓冲区有没有西药的数据，如果已经缓存，那么直接从缓存中返回；否则从磁盘中读取，然后缓存在操作系统的缓存中。 写操作： 数据从用户空间复制到内核空间的缓存中，这时对用户程序来说写操作就已经完后才能，至于什么时候在写到磁盘中由操作系统决定，除非显示的调用了sync同步命令。–《【珍藏】linux 同步IO: sync、fsync与fdatasync》。 缓存I/O的优点： 1）在一定程度上分离了内核空间和用户空间，保护系统本身的运行安全； 2）可以减少读盘的次数，从而提高性能。 缓存I/O的缺点： 在缓存 I/O 机制中，DMA 方式可以将数据直接从磁盘读到页缓存中，或者将数据从页缓存直接写回到磁盘 上，而不能直接在应用程序地址空间和磁盘之间进行数据传输，这样，数据在传输过程中需要在应用程序地址 空间（用户空间）和缓存（内核空间）之间进行多次数据拷贝操作，这些数据拷贝操作所带来的CPU以及内存 开销是非常大的。 直接IO​ 直接IO就是应用程序直接访问磁盘数据，而不经过内核缓冲区，也就是绕过内核缓冲区，自己管理IO缓冲区，这样做的目的是减少一次从内核缓冲区带用户程序缓存的数据复制。 直接IO的优点 ​ 应用程序直接访问磁盘数据，不经过操作系统内核数据缓冲区，这样做的目的是减少一次从内核缓冲区到用户程序 缓存的数据复制。这种方式通常是在对数据的缓存管理由应用程序实现的数据库管理系统中。 ​ 引入内核缓冲区的目的在于提高磁盘文件的访问性能，因为当进程需要读取磁盘文件时，如果文件内容已经在内核缓 冲区中，那么就不需要再次访问磁盘；而当进程需要向文件中写入数据时，实际上只是写到了内核缓冲区便告诉进程 已经写成功，而真正写入磁盘是通过一定的策略进行延迟的。 ​ 然而，对于一些较复杂的应用，比如数据库服务器，它们为了充分提高性能，希望绕过内核缓冲区，由自己在用户 态空间实现并管理I/O缓冲区，包括缓存机制和写延迟机制等，以支持独特的查询机制，比如数据库可以根据更加 合理的策略来提高查询缓存命中率。另一方面，绕过内核缓冲区也可以减少系统内存的开销，因为内核缓冲区本身就 在使用系统内存。 直接IO的缺点 ​ 如果访问的数据不在应用程序缓存中，那么每次数据都会直接从磁盘进行加载，这种直接加载 会非常缓慢。通常直接I/O跟异步I/O结合使用会得到较好的性能。 ​","categories":[],"tags":[{"name":"IO模型","slug":"IO模型","permalink":"https://liudong-code.github.io/tags/IO%E6%A8%A1%E5%9E%8B/"}],"keywords":[]},{"title":"PIO和DMA","slug":"PIO和DMA","date":"2020-02-28T02:09:06.000Z","updated":"2020-02-28T02:20:04.626Z","comments":true,"path":"2020/02/28/PIO和DMA/","link":"","permalink":"https://liudong-code.github.io/2020/02/28/PIO%E5%92%8CDMA/","excerpt":"","text":"慢速IO设备和内存之间的数据传输方式有下面两种： PIO：磁盘和内存之间的数据传输是需要CPU控制的，也就是说如果我们读取磁盘文件到内存中，数据要经过CPU储存转发，这种方式成为PIO。这种方式是不合理的，需要大量的CPU时间来读取文件，照成文件访问时系统几乎停止响应。 DMA：DMA（直接内存访问，Direct Memory Access），不经过CPU而直接进行磁盘和内存（内核空间）的数据交换。在DMA的模式下，CPU只需要向DMA控制器下达指令，让DMA控制器来处理数据和传达即可，DMA控制器通过系统总线来传输数据，传送完毕再通知CPU，这样就在很大程度上降低了CPU占有率，大大节省了系统资源，而它的传输速度与PIO的差异并不是十分明显，因为这主要取决于慢速设备的速度。","categories":[],"tags":[{"name":"IO模型","slug":"IO模型","permalink":"https://liudong-code.github.io/tags/IO%E6%A8%A1%E5%9E%8B/"}],"keywords":[]},{"title":"用户空间和内核空间","slug":"用户空间和内核空间","date":"2020-02-27T15:14:39.000Z","updated":"2020-02-27T15:56:55.859Z","comments":true,"path":"2020/02/27/用户空间和内核空间/","link":"","permalink":"https://liudong-code.github.io/2020/02/27/%E7%94%A8%E6%88%B7%E7%A9%BA%E9%97%B4%E5%92%8C%E5%86%85%E6%A0%B8%E7%A9%BA%E9%97%B4/","excerpt":"","text":"定义虚拟内存被操作系统划分成两块：内核空间和用户空间， 内核空间是内核代码运行的地方， 用户空间是用户程序代码运行的地方 当进程运行在内核空间时就处于内核态，当进程运行在用户空间时就处于用户态。 简单说， Kernel space 是 Linux 内核的运行空间， User space 是用户程序的运行空间。 为了安全，它们是隔离的，即使用户的程序崩溃了，内核也不受影响。 Kernel space 可以执行任意命令，调用系统的一切资源； User space 只能执行简单的运算，不能直接调用系统资源，必须通过系统接口（又称 system call），才能向内核发出指令。 1234str = \"my string\" // 用户空间x = x + 2file.write(str) // 切换到内核空间y = x + 4 // 切换回用户空间 第一行和第二行都是简单的赋值运算，在 User space 执行。 第三行需要写入文件，就要切换到Kernel space，因为用户不能直接写文件，必须通过内核安排。 第四行又是赋值运算，就切换回 User space。 查看 CPU 时间在 User space 与 Kernel Space 之间的分配情况，可以使用top命令。它的第三行输出就是 CPU 时间分配统计。 其中，第一项24.8 us（user 的缩写）就是 CPU 消耗在 User space 的时间百分比，第二项0.5 sy（system 的缩写）是消耗在 Kernel space 的时间百分比。随便也说一下其他 6 个指标的含义。ni：niceness 的缩写，CPU 消耗在 nice 进程（低优先级）的时间百分比id：idle 的缩写，CPU 消耗在闲置进程的时间百分比，这个值越低，表示 CPU 越忙wa：wait 的缩写，CPU 等待外部 I/O 的时间百分比，这段时间 CPU 不能干其他事，但是也没有执行运算，这个值太高就说明外部设备有问题hi：hardware interrupt 的缩写，CPU 响应硬件中断请求的时间百分比si：software interrupt 的缩写，CPU 响应软件中断请求的时间百分比st：stole time 的缩写，该项指标只对虚拟机有效，表示分配给当前虚拟机的 CPU 时间之中，被同一台物理机上的其他虚拟机偷走的时间百分比","categories":[],"tags":[{"name":"IO模型","slug":"IO模型","permalink":"https://liudong-code.github.io/tags/IO%E6%A8%A1%E5%9E%8B/"}],"keywords":[]},{"title":"ArrayList和LinkedList有什么区别","slug":"ArrayList和LinkedList有什么区别","date":"2020-02-27T09:00:12.000Z","updated":"2020-02-27T09:03:51.250Z","comments":true,"path":"2020/02/27/ArrayList和LinkedList有什么区别/","link":"","permalink":"https://liudong-code.github.io/2020/02/27/ArrayList%E5%92%8CLinkedList%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB/","excerpt":"","text":"区别 LinkedList和ArrayList的差别主要来自于Array和LinkedList数据结构的不同。ArrayList是基于数组实现的，LinkedList是基于双链表实现的。另外LinkedList类不仅是List接口的实现类，可以根据索引来随机访问集合中的元素，除此之外，LinkedList还实现了Deque接口，Deque接口是Queue接口的子接口，它代表一个双向队列，因此LinkedList可以作为双向对列，栈（可以参见Deque提供的接口方法）和List集合使用，功能强大。 因为Array是基于索引(index)的数据结构，它使用索引在数组中搜索和读取数据是很快的，可以直接返回数组中index位置的元素，因此在随机访问集合元素上有较好的性能。Array获取数据的时间复杂度是O(1),但是要插入、删除数据却是开销很大的，因为这需要移动数组中插入位置之后的的所有元素。 相对于ArrayList，LinkedList的随机访问集合元素时性能较差，因为需要在双向列表中找到要index的位置，再返回；但在插入，删除操作是更快的。因为LinkedList不像ArrayList一样，不需要改变数组的大小，也不需要在数组装满的时候要将所有的数据重新装入一个新的数组，这是ArrayList最坏的一种情况，时间复杂度是O(n)，而LinkedList中插入或删除的时间复杂度仅为O(1)。ArrayList在插入数据时还需要更新索引（除了插入数组的尾部）。 LinkedList需要更多的内存，因为ArrayList的每个索引的位置是实际的数据，而LinkedList中的每个节点中存储的是实际的数据和前后节点的位置。 使用场景 如果应用程序对数据有较多的随机访问，ArrayList对象要优于LinkedList对象； 如果应用程序有更多的插入或者删除操作，较少的数据读取，LinkedList对象要优于ArrayList对象； 不过ArrayList的插入，删除操作也不一定比LinkedList慢，如果在List靠近末尾的地方插入，那么ArrayList只需要移动较少的数据，而LinkedList则需要一直查找到列表尾部，反而耗费较多时间，这时ArrayList就比LinkedList要快","categories":[],"tags":[{"name":"Java基础知识","slug":"Java基础知识","permalink":"https://liudong-code.github.io/tags/Java%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"keywords":[]},{"title":"final，finally，finalize的区别","slug":"final，finally，finalize的区别","date":"2020-02-27T08:31:46.000Z","updated":"2020-02-27T08:57:27.919Z","comments":true,"path":"2020/02/27/final，finally，finalize的区别/","link":"","permalink":"https://liudong-code.github.io/2020/02/27/final%EF%BC%8Cfinally%EF%BC%8Cfinalize%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"1.简单区别： final用于声明属性，方法和类，分别表示属性不可交变，方法不可覆盖，类不可继承。 finally是异常处理语句结构的一部分，表示总是执行。 finalize是Object类的一个方法，在垃圾收集器执行的时候会调用被回收对象的此方法，供垃圾收集时的其他资源回收，例如关闭文件等。 2.中等区别： final：java中的关键字，修饰符。 如果一个类被声明为final，就意味着它不能再派生出新的子类，不能作为父类被继承。因此，一个类不能同时被声明为abstract抽象类的和final的类。 如果将变量或者方法声明为final，可以保证它们在使用中不被改变. 1)被声明为final的变量必须在声明时给定初值，而在以后的引用中只能读取，不可修改。 2)被声明final的方法只能使用，不能重载。 finally：java的一种异常处理机制。 finally是对Java异常处理模型的最佳补充。finally结构使代码总会执行，而不管无异常发生。使用finally可以维护对象的内部状态，并可以清理非内存资源。特别是在关闭数据库连接这方面，如果程序员把数据库连接的close()方法放到finally中，就会大大降低程序出错的几率。 finalize：Java中的一个方法名。Java技术使用finalize()方法在垃圾收集器将对象从内存中清除出去前，做必要的清理工作。这个方法是由垃圾收集器在确定这个对象没被引用时对这个对象调用的。它是在Object类中定义的，因此所的类都继承了它。子类覆盖finalize()方法以整理系统资源或者执行其他清理工作。finalize()方法是在垃圾收集器删除对象之前对这个对象调用的。 3.详细区别：final定义变量 第一种情况：如果final修饰的是一个基本类型，就表示这个变量被赋予的值是不可变的，即它是个常量；如果final修饰的是一个对象，就表示这个变量被赋予的引用是不可变的这里需要提醒大家注意的是，不可改变的只是这个变量所保存的引用，并不是这个引用所指向的对象。 第二种情况：final的含义与第一种情况相同。实际上对于前两种情况，一种更贴切的表述final的含义的描述，那就是，如果一个变量或方法参数被final修饰，就表示它只能被赋值一次，但是JAVA虚拟机为变量设定的默认值不记作一次赋值。被final修饰的变量必须被初始化。初始化的方式以下几种：1.在定义的时候初始化。2.final变量可以在初始化块中初始化，不可以在静态初始化块中初始化。3.静态final变量可以在定义时初始化，也可以在静态初始化块中初始化，不可以在初始化块中初始化。4.final变量还可以在类的构造器中初始化，但是静态final变量不可以。 定义方法当final用来定义一个方法时，它表示这个方法不可以被子类重写，但是并不影响它被子类继承。定义类​ final类不允许被继承，编译器在处理时把它的所方法都当作final的，因此final类比普通类拥更高的效率。而由关键字abstract定义的抽象类含必须由继承自它的子类重载实现的抽象方法，因此无法同时用final和abstract来修饰同一个类。同样的道理，final也不能用来修饰接口。 final的类的所方法都不能被重写，但这并不表示final的类的属性（变量值也是不可改变的，要想做到final类的属性值不可改变，必须给它增加final修饰 finally语句​ finally只能用在try/catch语句中并且附带着一个语句块，表示这段语句最终总是被执行 。 finalize方法​ 它是一个方法，属于java.lang.Object类，它的定义如下： 1protected void finalize()throws Throwable&#123;&#125; ​ 众所周知，finalize()方法是GC （garbagecollector）运行机制的一部分,在此我们只说说finalize()方法的作用是什么呢？finalize()方法是在GC清理它所从属的对象时被调用的，如果执行它的过程中抛出了无法捕获的异常（uncaughtexception），GC将终止对改对象的清理，并且该异常会被忽略；直到下一次GC开始清理这个对象时，它的finalize()会被再次调用。","categories":[],"tags":[{"name":"Java基础知识","slug":"Java基础知识","permalink":"https://liudong-code.github.io/tags/Java%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"keywords":[]},{"title":"进程和线程的区别","slug":"进程和线程的区别","date":"2020-02-27T08:04:55.000Z","updated":"2020-02-27T08:22:29.252Z","comments":true,"path":"2020/02/27/进程和线程的区别/","link":"","permalink":"https://liudong-code.github.io/2020/02/27/%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"概念《系统架构师》一书中给出的定义是： 进程（process）是资源分配和独立运行的基本单位。研究操作系统的进程，实质是研究系统中诸多进程之间的并发特性以及进程之间的相互制约性。 线程是进程中的一个实体，是被系统独立分配和调度的基本操作单位。具有就绪、运行、阻塞三种状态。 一个进程有若干个线程，线程只拥有一些运行中必不可少的资源，它可与同一个进程的其他线程共享进程所拥有的的全部资源。线程可创建另外一个线程，同一个进程中的多个线程可并发执行。 Linux中的概念 进程是资源（CPU、内存等）分配的基本单位，它是程序执行时的一个实例。程序运行时系统就会创建一个进程，并为它分配资源，然后把该进程放入进程就绪队列，进程调度器选中它的时候就会为它分配CPU时间，程序开始真正运行。 Linux系统函数fork()可以在父进程中创建一个子进程，这样的话，在一个进程接到来自客户端新的请求时就可以复制出一个子进程让其来处理，父进程只需负责监控请求的到来，然后创建子进程让其去处理，这样就能做到并发处理。 123456789101112131415# -*- coding:utf-8 -*-import osprint('当前进程:%s 启动中 ....' % os.getpid())pid = os.fork()if pid == 0: print('子进程:%s,父进程是:%s' % (os.getpid(), os.getppid()))else: print('进程:%s 创建了子进程:%s' % (os.getpid(),pid )) 输出结果：当前进程:27223 启动中 ....进程:27223 创建了子进程:27224子进程:27224,父进程是:27223 线程是程序执行时的最小单位，它是进程的一个执行流，是CPU调度和分派的基本单位，一个进程可以由很多个线程组成，线程间共享进程的所有资源，每个线程有自己的堆栈和局部变量。线程由CPU独立调度执行，在多CPU环境下就允许多个线程同时运行。同样多线程也可以实现并发操作，每个请求分配一个线程来处理。 区别和优劣 进程是资源分配的最小单位，线程是程序执行的最小单位。 进程有自己的独立地址空间，每启动一个进程，系统就会为它分配地址空间，建立数据表来维护代码段、堆栈段和数据段，这种操作非常昂贵。而线程是共享进程中的数据的，使用相同的地址空间，因此CPU切换一个线程的花费远比进程要小很多，同时创建一个线程的开销也比进程要小很多。 线程之间的通信更方便，同一进程下的线程共享全局变量、静态变量等数据，而进程之间的通信需要以通信的方式（IPC)进行。不过如何处理好同步与互斥是编写多线程程序的难点。 但是多进程程序更健壮，多线程程序只要有一个线程死掉，整个进程也死掉了，而一个进程死掉并不会对另外一个进程造成影响，因为进程有自己独立的地址空间。 参考书籍《Unix网络编程》","categories":[],"tags":[{"name":"Java基础知识","slug":"Java基础知识","permalink":"https://liudong-code.github.io/tags/Java%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"keywords":[]},{"title":"泛型中extends和super的区别","slug":"泛型中extends和super的区别","date":"2020-02-27T07:35:59.000Z","updated":"2020-02-27T07:58:34.700Z","comments":true,"path":"2020/02/27/泛型中extends和super的区别/","link":"","permalink":"https://liudong-code.github.io/2020/02/27/%E6%B3%9B%E5%9E%8B%E4%B8%ADextends%E5%92%8Csuper%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"释义：在非泛型中： extends：让一个类继承另外一个类； super：指向父类对象的引用； 在泛型中： extends也成为上界通配符，就是指定上边界。即泛型中的类必须为当前类的子类或当前类。 super也称为下届通配符，就是指定下边界。即泛型中的类必须为当前类或者其父类。 看如下代码： 123456789101112131415161718192021222324252627public class Food &#123;&#125;public class Fruit extends Food &#123;&#125;public class Apple extends Fruit &#123;&#125;public class Banana extends Fruit&#123;&#125;public class GenericTest &#123;public void testExtends(List&lt;? extends Fruit&gt; list)&#123; //报错,extends为上界通配符,只能取值,不能放. //因为Fruit的子类不只有Apple还有Banana,这里不能确定具体的泛型到底是Apple还是Banana，所以放入任何一种类型都会报错 //list.add(new Apple()); //可以正常获取 Fruit fruit = list.get(1);&#125;public void testSuper(List&lt;? super Fruit&gt; list)&#123; //super为下界通配符，可以存放元素，但是也只能存放当前类或者子类的实例，以当前的例子来讲， //无法确定Fruit的父类是否只有Food一个(Object是超级父类) //因此放入Food的实例编译不通过 list.add(new Apple()); // list.add(new Food()); Object object = list.get(1); &#125;&#125; 在testExtends方法中，因为泛型中用的是extends，在向list中存放元素的时候，我们并不能确定List中的元素的具体类型，即可能是Apple也可能是Banana。因此调用add方法时，不论传入new Apple()还是new Banana()，都会出现编译错误。 理解了extends之后，再看super就很容易理解了，即我们不能确定testSuper方法的参数中的泛型是Fruit的哪个父类，因此在调用get方法时只能返回Object类型。结合extends可见，在获取泛型元素时，使用extends获取到的是泛型中的上边界的类型(本例子中为Fruit),范围更小。 总结：在使用泛型时，存取元素时用super,获取元素时，用extends。 扩展： 通配符 ? 使用 在泛型中的通配符就是一个问号，标准叫法是无界通配符，它一般使用在参数或变量的声明上 1234567891011121314 // 在参数中使用无界通配符 public static void test(List&lt;?&gt; list) &#123; Object o = list.get(1);&#125;public static void main(String[] args) &#123; List&lt;Integer&gt; list1 = new ArrayList&lt;Integer&gt;(); // 在变量声明中使用无界通配符 List&lt;?&gt; list2 = list1; test(list1); test(list2);&#125; 泛型中使用无界通配符，表示泛型可以是任意具体的类型，没有限制（基本数据类型除外，基本数据类型不能用作泛型，可以使用基本数据类型的包装类）； 所以无界通配符给人的感觉就和原生的类型没什么区别，比如就上面这段代码，使用List&lt;?&gt;，和直接使用List，好像是一样的；但是实际上还是有一些区别的，比如看下面这段代码: 1234567891011121314151617181920212223 // 在参数中使用无界通配符 public static void test1(List&lt;?&gt; list) &#123; // 均编译错误，因为使用了无界通配符，编译器无法确定具体是什么类型 // list.add(1111); // list.add(\"aaa\"); // list.add(new Object());&#125;// 在参数中使用原生Listpublic static void test2(List list) &#123; // 编译通过，不加泛型时，编译器默认为Object类型 list.add(1111); list.add(\"aaa\"); list.add(new Object());&#125;public static void main(String[] args) &#123; // 声明两个泛型明确的list集合 List&lt;Integer&gt; list1 = new ArrayList&lt;&gt;(); List&lt;String&gt; list2 = new ArrayList&lt;&gt;(); // 调用使用了&lt;？&gt;的方法 test1(list1); test2(list2);&#125; List：表示可以存储任意Object类型的集合； List：表示一个存储某种特定类型的List集合，但是不知道这种特定类型是什么；","categories":[],"tags":[{"name":"Java基础知识","slug":"Java基础知识","permalink":"https://liudong-code.github.io/tags/Java%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"keywords":[]},{"title":"String，Stringbuffer，StringBuilder的区别","slug":"String，Stringbuffer，StringBuilder的区别","date":"2020-02-27T07:16:10.000Z","updated":"2020-02-27T07:19:07.092Z","comments":true,"path":"2020/02/27/String，Stringbuffer，StringBuilder的区别/","link":"","permalink":"https://liudong-code.github.io/2020/02/27/String%EF%BC%8CStringbuffer%EF%BC%8CStringBuilder%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"String string类是不可变的：创建一个String对象之后，任何对这个对象的改变都会引起一个新的String对象生成。 123String str =\"abc\";str = str +\"def\";//这一步jvm会再次创建一个String对象 第二次其实jvm又生成了一个String类，而不是直接覆盖原来的”abc”，因此我们说String类是不可改变类。这一种特性会带来一个问题，每次拼接都要创建都要创建一次对象，当我们要拼接大量字符串的时候，效率会变得非常非常慢。 StringBuffer StringBuffer 不同于String的是，stringbuffer 是可变的， 123StringBuffer sb =new StringBuffer(\"abc\"); sb.append(\"efg\");//并没有创建一个新的对象 这里第二步并没有产生一个新的对象，而是在原来的基础上追加字符串，这种方式在拼接字符串的时候效率肯定比String要高得多。 StringBuilderStringBuffer和StringBuilder类的区别也是如此，他们的原理和操作基本相同，区别在于StringBuffer支持并发操作，线性安全的，适 合多线程中使用。StringBuilder不支持并发操作，线性不安全的，不适合多线程中使用。新引入的StringBuilder类不是线程安全的，但其在单线程中的性能比StringBuffer高。 total:1.如果要操作少量的数据用 String 2.单线程操作字符串缓冲区下操作大量数据用StringBuilder 3.多线程操作字符串缓冲区下操作大量数据用StringBuffer","categories":[],"tags":[{"name":"Java基础知识","slug":"Java基础知识","permalink":"https://liudong-code.github.io/tags/Java%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"keywords":[]},{"title":"Java中的可变对象与不可变对象","slug":"Java中的可变对象与不可变对象","date":"2020-02-27T06:59:24.000Z","updated":"2020-02-27T07:09:51.291Z","comments":true,"path":"2020/02/27/Java中的可变对象与不可变对象/","link":"","permalink":"https://liudong-code.github.io/2020/02/27/Java%E4%B8%AD%E7%9A%84%E5%8F%AF%E5%8F%98%E5%AF%B9%E8%B1%A1%E4%B8%8E%E4%B8%8D%E5%8F%AF%E5%8F%98%E5%AF%B9%E8%B1%A1/","excerpt":"","text":"不可变对象 （Immutable Objects）​ 对象一旦被创建它的状态（对象的数据，也即对象属性值）就不能改变，任何对它的改变都应该产生一个新的对象。 ​ 不可变对象的类即为不可变类(Immutable Class)。JAVA平台类库中包含许多不可变类，如String、基本类型的包装类、BigInteger和BigDecimal等 . 可变对象(Mutable Objects)​ 相对于不可变类，可变类创建实例后可以改变其成员变量值，开发中创建的大部分类都属于可变类。 编写不可变类 1.确保类不能被继承：将类声明为final, 或者使用静态工厂并声明构造器为private 使用private和final修饰符来修饰该类的属性 如果成员属性为可变对象属性，不要使这些对象改变： 1）不要提供更改可变对象的方法 2）不要共享对可变对象的引用，不要存储传给构造器的外部可变对象的引用。因为引用可变对象的成员变量和外部可变对象的引用指向同一块内存地址，用户可以在不可变类之外通过修改可变对象的值 为了保证内部的值不被修改，可以采用深度拷贝的方法来复制一个对象并传入副本的引用来确保类的不可变 1234567891011public final class MyImmutableDemo &#123; private final int[] myArray; public MyImmutableDemo(int[] array) &#123; this.myArray = array.clone(); &#125; &#125; C. 不要提供任何可以修改对象状态的方法（不仅仅是set方法, 还有任何其它可以改变状态的方法） 不可变对象的优缺点优点* 构造、测试和使用都很简单 * 不可变对象是线程安全的，在线程之间可以相互共享，不需要利用特殊机制来保证同步问题，因为对象的值无法改变。可以降低并发错误的可能性，因为不需要用一些锁机制等保证内存一致性问题也减少了同步开销。 * 不可变对象可以被重复使用，可以将它们缓存起来重复使用，就像字符串字面量和整型数字一样。可以使用静态工厂方法来提供类似于valueOf()这样的方法，它可以从缓存中返回一个已经存在的Immutable对象，而不是重新创建一个。 缺点 * 不可变对象最大的缺点就是创建对象的开销，因为每一步操作都会产生一个新的对象,制造大量垃圾，由于他们不能被重用而且对于它们的使用就是”用“然后”扔“，会创造很多的垃圾，给垃圾收集带来很大的麻烦 参考：https://blog.csdn.net/bupa900318/article/details/80696785","categories":[],"tags":[{"name":"Java基础知识","slug":"Java基础知识","permalink":"https://liudong-code.github.io/tags/Java%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"keywords":[]},{"title":"接口和抽象类的区别","slug":"接口和抽象类的区别","date":"2020-02-27T06:06:01.000Z","updated":"2020-02-27T07:43:41.584Z","comments":true,"path":"2020/02/27/接口和抽象类的区别/","link":"","permalink":"https://liudong-code.github.io/2020/02/27/%E6%8E%A5%E5%8F%A3%E5%92%8C%E6%8A%BD%E8%B1%A1%E7%B1%BB%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"抽象类1abstract ​ 抽象类必须用abstract 修饰，子类必须实现抽象类中的抽象方法，如果有方法未实现，那么子类的该方法也要用abstract 修饰。 ​ 默认的权限修饰符是：public，也可以是procted.如果是private的话，子类无法继承。 ​ 抽象类无法创建对象。 接口1interface ​ 接口中的变量隐式的使用Public static final 修饰，也要给出初始值 ​ 接口中的方法隐式的使用public abstract 修饰，只能是public 修饰。 ​ 接口中的方法默认的不能有实现（JDK1.8可以有默认实现）。 抽象类和接口的区别 抽象类只能继承一次，但是可以有多个接口。 继承于抽象类或者接口的类，必须实现其中的所有方法，抽象类中的为实现的抽象方法，子类也需要定义为抽象类。 抽象类可以有非抽象方法。 接口的变量必须由public static final 修饰，并给出初始值，所以其实现类不能重新定义，也不能改变其值。 接口的方法默认是public abstract ，不能是static，抽象类中可以有static方法。 接口的方法也不允许子类覆写。","categories":[],"tags":[{"name":"Java基础知识","slug":"Java基础知识","permalink":"https://liudong-code.github.io/tags/Java%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"keywords":[]},{"title":"抽象类的意义","slug":"抽象类的意义","date":"2020-02-27T05:53:43.000Z","updated":"2020-02-27T06:07:43.236Z","comments":true,"path":"2020/02/27/抽象类的意义/","link":"","permalink":"https://liudong-code.github.io/2020/02/27/%E6%8A%BD%E8%B1%A1%E7%B1%BB%E7%9A%84%E6%84%8F%E4%B9%89/","excerpt":"","text":"抽象类的意义最主要的：对代码的维护和重用。 关键字 1abstract 1.因为抽象类不能实例化对象，所以必须要有子类来实现它之后才能使用。这样就可以把一些具有相同属性和方法的组件进行抽象，这样更有利于代码和程序的维护 2.当又有一个具有相似的组件产生时，只需要实现该抽象类就可以获得该抽象类的那些属性和方法。 ​ 实际写代码的过程中，如果想要对某一个接口进行默认实现，并且不希望在该实现类 上创建实例，那么抽象类是一个不错地选择。","categories":[],"tags":[{"name":"Java基础知识","slug":"Java基础知识","permalink":"https://liudong-code.github.io/tags/Java%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"keywords":[]},{"title":"int与Integer的区别","slug":"int与Integer的区别","date":"2020-02-27T05:13:32.000Z","updated":"2020-02-27T05:34:38.327Z","comments":true,"path":"2020/02/27/int与Integer的区别/","link":"","permalink":"https://liudong-code.github.io/2020/02/27/int%E4%B8%8EInteger%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"int1int 是java的基本数据类型。 Integer1Integer 继承了Object类，是对象类型，是 int 的包装类。 int 与 Integer 的区别1,值的储存​ int储存在栈中 ​ Integer对象的引用储存在栈中，对象的数据储存在堆中。 2,初始化​ int初始值是0 ​ Integer的初始值是null 3,传参​ int是值传递 ​ Integer是引用传递，引用不可改变但是引用指向堆空间地址的值是可以改变的。 4,泛型支持​ 泛型不支持int，但是支持Integer。 5,运算​ int 可以直接做运算，是类的特性。​ Integer 的对象可以调用该类的方法，但是在拆箱之前不能进行运算，需要转化为基本类型int。 相同值下的 int 和 Integer 的比较结果​ 1,两个通过new生成的变量，结果为false。​ 2,int 和 Integer 的值比较，若两者的值相等，则为true。​ （注意：在比较时，Integer会自动拆箱为int类型，然后再做比较。）​ 3,new 生成的Integer变量 和 非new 生成的Integer变量比较，，结果为false。​ （注意：new 生成的Integer变量的值在堆空间中，非new 生成的Integer变量的值在在常量池中。）​ （注意：非new生成的Integer变量，会先判断常量池中是否有该对象，若有则共享，若无则在常量池中放入 该对象；也叫享元模式，后面再说。）​ 4,两个非new 生成的Integer对象比较，则结果为true。​ （注意：此处需要一个前提：值的范围在 -128 ~ 127 之间。​ 涉及到java对 int 与 Integer 的自动装箱和拆箱的一种模式：享元模式—flyweight，为了加强对简单数字的重 复利用。​ 在赋值时，其实是执行了Integer的valueOf()方法。​ 当值在 -128 ~ 127之间时，java会进行自动装箱，然后会对值进行缓存，如果下次再有相同的值，会直接在 缓存中取出使用。缓存是通过Integer的内部类IntegerCache来完成的。​ 当值超出此范围，会在堆中new出一个对象来存储。​ PS：自动装箱与拆箱是在JDK1.5中出现的。 12345678910111213141516public static void main(String[] args) &#123; int a =12; Integer b =12; Integer c = new Integer(12); Integer d=128; Integer e=128; System.out.println(a==b); System.out.println(b==c); System.out.println(d==e); &#125;truefalsefalse ​ 5,内部类IntegerCache​ 通过此类可以缓存简单数字。​ 缓存的数大小可以由 -XX：AutoBoxCacheMax = 控制。​ jvm初始化时，java.lang.Integer.Integ","categories":[],"tags":[{"name":"Java基础知识","slug":"Java基础知识","permalink":"https://liudong-code.github.io/tags/Java%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"keywords":[]},{"title":"Java==和equals和hashCode的区别","slug":"Java中==和equals和hashCode的区别","date":"2020-02-27T02:00:49.000Z","updated":"2020-02-27T08:54:31.546Z","comments":true,"path":"2020/02/27/Java中==和equals和hashCode的区别/","link":"","permalink":"https://liudong-code.github.io/2020/02/27/Java%E4%B8%AD==%E5%92%8Cequals%E5%92%8ChashCode%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"Java中==和equals和hashCode的区别1，==1, 基础类型：byte,short,char,int,double,boolean,float ​ 他们之间用“==”，比较的是他们的值 2, 引用类型（类，接口，数组） ​ 使用“==”进行比较的时候，比较的是内存地址 12345678910111213141516public static void main(String[] args) &#123; String str1 = new String(\"str\"); String str2 = new String(\"str\"); System.out.println(\"str1==str2:\" + (str1 == str2)); String str3=\"str\"; String str4=\"str\"; System.out.println(str3 == str4); System.out.println(System.identityHashCode(str1)); System.out.println(System.identityHashCode(str2)); System.out.println(System.identityHashCode(str3)); System.out.println(System.identityHashCode(str4)); &#125; 结果为 str1==str2:falsetrue323247493111501236338603633860 对象是放在堆中的，栈中存放的是对象的引用（地址）。由此可见’==’是对栈中的值进行比较的。如果要比较堆 中对象的内容是否相同，那么就要重写equals方法了。 但是这行代码，128！=128，a!=b 123456Integer a1 = 127;Integer b1 = 127;Integer a = 128;Integer b = 128;System.out.println(a1==b1);System.out.println(a==b); 结果 true false 去看Integer的源码可以明白，是这段源码 12345678910111213141516171819202122232425262728293031323334private static class IntegerCache &#123; static final int low = -128; static final int high; static final Integer cache[]; static &#123; // high value may be configured by property int h = 127; String integerCacheHighPropValue = sun.misc.VM.getSavedProperty(\"java.lang.Integer.IntegerCache.high\"); if (integerCacheHighPropValue != null) &#123; try &#123; int i = parseInt(integerCacheHighPropValue); i = Math.max(i, 127); // Maximum array size is Integer.MAX_VALUE h = Math.min(i, Integer.MAX_VALUE - (-low) -1); &#125; catch( NumberFormatException nfe) &#123; // If the property cannot be parsed into an int, ignore it. &#125; &#125; high = h; cache = new Integer[(high - low) + 1]; int j = low; for(int k = 0; k &lt; cache.length; k++) cache[k] = new Integer(j++); // range [-128, 127] must be interned (JLS7 5.1.7) assert IntegerCache.high &gt;= 127; &#125; private IntegerCache() &#123;&#125; &#125; Integer对于 对于-128到127之间的数，会进行缓存 。 2，equals() 1,默认没有重新的情况下，都调用的是Objects 的equals（）的方法，源码如下： 1public boolean equals(Object obj) &#123; return (this == obj);&#125; ​ 可以看到是Objects中的equal（）方法和== 是等同的。 2，日常的代码中会覆盖equal() 方法，下面是string 类 重写的equal（）方法 123456789101112131415161718192021public boolean equals(Object anObject) &#123; if (this == anObject) &#123; return true; &#125; if (anObject instanceof String) &#123; String anotherString = (String)anObject; int n = value.length; if (n == anotherString.value.length) &#123; char v1[] = value; char v2[] = anotherString.value; int i = 0; while (n-- != 0) &#123; if (v1[i] != v2[i]) return false; i++; &#125; return true; &#125; &#125; return false; &#125; ​ 逻辑很明显： ​ 内存地址相同，直接返回true ​ 比较对象是String类型，直接返回false ​ a和b长度不同，直接返回false ​ 逐个字符比较，有不同就返回false equal重写注意的五个原则​ 1 自反性：对任意引用值X，x.equals(x)的返回值一定为true.​ 2 对称性：对于任何引用值x,y,当且仅当y.equals(x)返回值为true时，x.equals(y)的返回值一定为true;​ 3 传递性：如果x.equals(y)=true, y.equals(z)=true,则x.equals(z)=true​ 4 一致性：如果参与比较的对象没任何改变，则对象比较的结果也不应该有任何改变​ 5 非空性：任何非空的引用值X，x.equals(null)的返回值一定为false 3，hashCodeObjects给出的hashCode（）的原生代码是： 1public native int hashCode(); 返回时的一个int 类型的数值。 下面是引用摘抄的 ======================================================================================= hashCode()方法返回的就是一个数值，从方法的名称上就可以看出，其目的是生成一个hash码。hash码的主要用途就是在对对象进行散列的时候作为key输入，据此很容易推断出，我们需要每个对象的hash码尽可能不同，这样才能保证散列的存取性能。事实上，Object类提供的默认实现确实保证每个对象的hash码不同（在对象的内存地址基础上经过特定算法返回一个hash码）。Java采用了哈希表的原理。哈希（Hash）实际上是个人名，由于他提出一哈希算法的概念，所以就以他的名字命名了。 哈希算法也称为散列算法，是将数据依特定算法直接指定到一个地址上。 散列函数,散列算法,哈希函数。是一种从任何一种数据中创建小的数字“指纹”的方法。散列函数将任意长度的二进制值映射为较短的固定长度的二进制值，这个小的二进制值称为哈希值。好的散列函数在输入域中很少出现散列冲突。 ====================================================================================== hashCode的作用想要明白，必须要先知道Java中的集合。 总的来说，Java中的集合（Collection）有两类，一类是List，再有一类是Set。前者集合内的元素是有序的，元素可以重复；后者元素无序，但元素不可重复。 那么这里就有一个比较严重的问题了：要想保证元素不重复，可两个元素是否重复应该依据什么来判断呢？ 这就是Object.equals方法了。但是，如果每增加一个元素就检查一次，那么当元素很多时，后添加到集合中的元素比较的次数就非常多了。也就是说，如果集合中现在已经有1000个元素，那么第1001个元素加入集合时，它就要调用1000次equals方法。这显然会大大降低效率。于是，Java采用了哈希表的原理。 这样一来，当集合要添加新的元素时， 先调用这个元素的hashCode方法，就一下子能定位到它应该放置的物理位置上。 如果这个位置上没有元素，它就可以直接存储在这个位置上，不用再进行任何比较了； 如果这个位置上已经有元素了，就调用它的equals方法与新元素进行比较，相同的话就不存，不相同就散列其它的地址。所以这里存在一个冲突解决的问题。这样一来实际调用equals方法的次数就大大降低了，几乎只需要一两次。 4、eqauls方法和hashCode方法关系Java对于eqauls方法和hashCode方法是这样规定的： (1)同一对象上多次调用hashCode()方法，总是返回相同的整型值。 (2)如果a.equals(b)，则一定有a.hashCode() 一定等于 b.hashCode()。(3)如果!a.equals(b)，则a.hashCode() 不一定等于 b.hashCode()。此时如果a.hashCode() 总是不等于 b.hashCode()，会提高hashtables的性能。 (4)a.hashCode()==b.hashCode() 则 a.equals(b)可真可假 (5)a.hashCode()！= b.hashCode() 则 a.equals(b)为假。 上面结论简记： 1、如果两个对象equals，Java运行时环境会认为他们的hashcode一定相等。2、如果两个对象不equals，他们的hashcode有可能相等。3、如果两个对象hashcode相等，他们不一定equals。4、如果两个对象hashcode不相等，他们一定不equals。 关于这两个方法的重要规范：规范1：若重写equals(Object obj)方法，有必要重写hashcode()方法，确保通过equals(Object obj)方法判断结果为true的两个对象具备相等的hashcode()返回值。说得简单点就是：“如果两个对象相同，那么他们的hashcode应该相等”。不过请注意：这个只是规范，如果你非要写一个类让equals(Object obj)返回true而hashcode()返回两个不相等的值，编译和运行都是不会报错的。不过这样违反了Java规范，程序也就埋下了BUG。 规范2：如果equals(Object obj)返回false，即两个对象“不相同”，并不要求对这两个对象调用hashcode()方法得到两个不相同的数。说的简单点就是：“如果两个对象不相同，他们的hashcode可能相同”。 5、为什么覆盖equals时总要覆盖hashCode 一个很常见的错误根源在于没有覆盖hashCode方法。在每个覆盖了equals方法的类中，也必须覆盖hashCode方法。如果不这样做的话，就会违反Object.hashCode的通用约定，从而导致该类无法结合所有基于散列的集合一起正常运作，这样的集合包括HashMap、HashSet和Hashtable。 1.在应用程序的执行期间，只要对象的equals方法的比较操作所用到的信息没有被修改，那么对这同一个对象调用多次，hashCode方法都必须始终如一地返回同一个整数。在同一个应用程序的多次执行过程中，每次执行所返回的整数可以不一致。 2.如果两个对象根据equals()方法比较是相等的，那么调用这两个对象中任意一个对象的hashCode方法都必须产生同样的整数结果。 3.如果两个对象根据equals()方法比较是不相等的，那么调用这两个对象中任意一个对象的hashCode方法，则不一定要产生相同的整数结果。但是程序员应该知道，给不相等的对象产生截然不同的整数结果，有可能提高散列表的性能。 6、总结：1、equals方法用于比较对象的内容是否相等（覆盖以后） 2、hashcode方法只有在集合中用到 3、当覆盖了equals方法时，比较对象是否相等将通过覆盖后的equals方法进行比较（判断对象的内容是否相等）。 4、将对象放入到集合中时，首先判断要放入对象的hashcode值与集合中的任意一个元素的hashcode值是否相等，如果不相等直接将该对象放入集合中。如果hashcode值相等，然后再通过equals方法判断要放入对象与集合中的任意一个对象是否相等，如果equals判断不相等，直接将该元素放入到集合中，否则不放入。 以上从第四点以后转载至博客：https://blog.csdn.net/hla199106/article/details/46907725","categories":[],"tags":[{"name":"Java基础知识","slug":"Java基础知识","permalink":"https://liudong-code.github.io/tags/Java%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"keywords":[]},{"title":"写于博客之始","slug":"写于博客之始","date":"2020-02-26T10:00:08.000Z","updated":"2020-02-27T07:36:23.335Z","comments":true,"path":"2020/02/26/写于博客之始/","link":"","permalink":"https://liudong-code.github.io/2020/02/26/%E5%86%99%E4%BA%8E%E5%8D%9A%E5%AE%A2%E4%B9%8B%E5%A7%8B/","excerpt":"","text":"​ 重新整理了之前的旧的博客，发现有部分博客有老东家的Code，想起来之前有同事因无意中泄露了公司的淘宝三段码，而受到处罚，索性就新搞了一个新的博客 地址，这几天正好因为疫情在家里面，找工作也不是很好找，就把之前的整理的东西全部搞过来。新的博客只弄了主题，其他的细节都还没整，想的是尽快的把日常干货先给弄上来。嗯，就这些吧！ ​","categories":[],"tags":[{"name":"日常随笔","slug":"日常随笔","permalink":"https://liudong-code.github.io/tags/%E6%97%A5%E5%B8%B8%E9%9A%8F%E7%AC%94/"}],"keywords":[]}]}